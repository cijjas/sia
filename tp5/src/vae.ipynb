{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def prepare_mnist_data_local():\n",
    "    \"\"\"\n",
    "    Prepares the MNIST dataset for training and visualization using a local .npz file.\n",
    "    \"\"\"\n",
    "    # Load MNIST dataset from local file\n",
    "    data = np.load(\"../data/mnist.npz\")\n",
    "    x_train, y_train = data[\"x_train\"], data[\"y_train\"]\n",
    "    x_test, y_test = data[\"x_test\"], data[\"y_test\"]\n",
    "\n",
    "    \n",
    "    # Normalize the images between 0 and 1\n",
    "    x_visual = x_train.astype(\"float32\") / 255.0  # For visualization purposes\n",
    "    x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255\n",
    "\n",
    "    # Create training and test pairs without labels (unsupervised learning)\n",
    "    training_data = [(x.reshape(784, 1),x.reshape(784, 1) ) for x in x_train]\n",
    "    test_data = [(x.reshape(784, 1),x.reshape(784, 1) ) for x in x_test]\n",
    "\n",
    "    return training_data, test_data, x_visual\n",
    "\n",
    "# Load the training, test data, and visual data\n",
    "training_data, test_data, x_visual = prepare_mnist_data_local()\n",
    "\n",
    "# Pick 3 random indices\n",
    "random_indices = np.random.choice(len(x_visual), 3, replace=False)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(x_visual[idx], cmap=\"gray\")\n",
    "    plt.title(f\"Sample {idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 779.3677833636103, KL divergence: 4.066704134292779\n",
      "Reconstruction loss: 673.3058891117648, KL divergence: 9.694795664214347\n",
      "Reconstruction loss: 758.3893084286584, KL divergence: 6.292549303489278\n",
      "Reconstruction loss: 696.7748425579375, KL divergence: 5.552816871761366\n",
      "Reconstruction loss: 684.7514482933491, KL divergence: 8.957795524475998\n",
      "Reconstruction loss: 693.7951488356711, KL divergence: 4.9995881161687965\n",
      "Reconstruction loss: 767.7940774589342, KL divergence: 3.7069249751976687\n",
      "Reconstruction loss: 709.1064649482156, KL divergence: 6.456033098305855\n",
      "Reconstruction loss: 694.3030251029263, KL divergence: 5.004763076344109\n",
      "Reconstruction loss: 712.7338346297279, KL divergence: 7.889682124668474\n",
      "Reconstruction loss: 807.2600609640756, KL divergence: 4.31892674577336\n",
      "Reconstruction loss: 788.9971007807251, KL divergence: 3.4724156554600993\n",
      "Reconstruction loss: 720.4598932936633, KL divergence: 3.2240935218037934\n",
      "Reconstruction loss: 746.7685895893909, KL divergence: 2.861599775436935\n",
      "Reconstruction loss: 784.9713531900234, KL divergence: 11.543615970935065\n",
      "Reconstruction loss: 823.3382445938549, KL divergence: 6.474055025976737\n",
      "Reconstruction loss: 735.6790096933013, KL divergence: 3.257679537189567\n",
      "Reconstruction loss: 714.7232062158607, KL divergence: 3.51998196725063\n",
      "Reconstruction loss: 673.1805644638453, KL divergence: 2.8712167393772905\n",
      "Reconstruction loss: 741.546102170835, KL divergence: 5.871297681107983\n",
      "Reconstruction loss: 865.1009458724355, KL divergence: 8.590354027273804\n",
      "Reconstruction loss: 661.3481482103307, KL divergence: 3.6996827461501174\n",
      "Reconstruction loss: 704.6126654208568, KL divergence: 2.050195651862255\n",
      "Reconstruction loss: 737.5520894174506, KL divergence: 3.69631059703228\n",
      "Reconstruction loss: 816.5534512313001, KL divergence: 3.304810315606407\n",
      "Reconstruction loss: 727.7905188904115, KL divergence: 6.706916016095817\n",
      "Reconstruction loss: 735.2737888432046, KL divergence: 4.279323875022538\n",
      "Reconstruction loss: 828.6500591369768, KL divergence: 1.6949415422107867\n",
      "Reconstruction loss: 675.2086826443588, KL divergence: 3.7698017088166575\n",
      "Reconstruction loss: 809.9896196444618, KL divergence: 7.495840624753644\n",
      "Reconstruction loss: 804.1484956646065, KL divergence: 4.506375787811864\n",
      "Reconstruction loss: 697.9241771898121, KL divergence: 3.0753167449443186\n",
      "Reconstruction loss: 544.3913652338745, KL divergence: 6.107354305538481\n",
      "Reconstruction loss: 566.8078425796723, KL divergence: 2.115336725378385\n",
      "Reconstruction loss: 530.2289610932901, KL divergence: 1.6413128490484405\n",
      "Reconstruction loss: 546.6419835844533, KL divergence: 3.0038084053626806\n",
      "Reconstruction loss: 579.8521826581668, KL divergence: 1.8040824032641427\n",
      "Reconstruction loss: 549.0014542685815, KL divergence: 2.980459491530047\n",
      "Reconstruction loss: 555.6217027675855, KL divergence: 3.779634472922755\n",
      "Reconstruction loss: 538.1820217467565, KL divergence: 2.4559992840708036\n",
      "Reconstruction loss: 544.3304641404713, KL divergence: 3.6524938769293005\n",
      "Reconstruction loss: 544.0022334984269, KL divergence: 3.540333331052514\n",
      "Reconstruction loss: 581.7395855415383, KL divergence: 2.6304736770650474\n",
      "Reconstruction loss: 564.2670181614773, KL divergence: 2.32347679930376\n",
      "Reconstruction loss: 540.1903878354267, KL divergence: 1.5435295306742647\n",
      "Reconstruction loss: 589.6753517731846, KL divergence: 3.532211242322672\n",
      "Reconstruction loss: 526.9404387744992, KL divergence: 2.784763049206318\n",
      "Reconstruction loss: 551.953492425574, KL divergence: 3.2815474928045356\n",
      "Reconstruction loss: 550.9443250962274, KL divergence: 2.7259594138394716\n",
      "Reconstruction loss: 537.6916181004535, KL divergence: 3.2745520382590456\n",
      "Reconstruction loss: 570.9067809305004, KL divergence: 1.591270203788203\n",
      "Reconstruction loss: 548.6255603637256, KL divergence: 3.9201395207494225\n",
      "Reconstruction loss: 570.4748852598568, KL divergence: 6.878164679635637\n",
      "Reconstruction loss: 603.0274627223173, KL divergence: 5.239539890369045\n",
      "Reconstruction loss: 594.5948598695502, KL divergence: 1.5983012057316008\n",
      "Reconstruction loss: 585.4630862956473, KL divergence: 3.964600150231321\n",
      "Reconstruction loss: 577.4131761216826, KL divergence: 4.346390347911196\n",
      "Reconstruction loss: 550.6919294991189, KL divergence: 6.250960978858322\n",
      "Reconstruction loss: 569.223017014274, KL divergence: 2.6890344560721173\n",
      "Reconstruction loss: 542.1275139032847, KL divergence: 3.1393189757561664\n",
      "Reconstruction loss: 580.1865758011998, KL divergence: 8.42054855001773\n",
      "Reconstruction loss: 547.7112070209328, KL divergence: 4.660885024732946\n",
      "Reconstruction loss: 551.6822376567314, KL divergence: 5.272980443683565\n",
      "Reconstruction loss: 566.6493384548389, KL divergence: 6.507294816205132\n",
      "Reconstruction loss: 481.35259564926025, KL divergence: 2.5712310204301136\n",
      "Reconstruction loss: 529.9286295763396, KL divergence: 3.608113439033725\n",
      "Reconstruction loss: 524.8548928036902, KL divergence: 2.1071192194082733\n",
      "Reconstruction loss: 476.2800497753956, KL divergence: 3.918338614842974\n",
      "Reconstruction loss: 502.5783066320177, KL divergence: 2.216885261036049\n",
      "Reconstruction loss: 464.67483962151505, KL divergence: 1.7895433515756238\n",
      "Reconstruction loss: 517.666425899687, KL divergence: 5.529018221265249\n",
      "Reconstruction loss: 472.80976392363937, KL divergence: 2.949149039764844\n",
      "Reconstruction loss: 475.92159936449235, KL divergence: 2.514977343533065\n",
      "Reconstruction loss: 518.3058915264161, KL divergence: 2.3086321911536043\n",
      "Reconstruction loss: 507.86699846037584, KL divergence: 1.2929035547337528\n",
      "Reconstruction loss: 462.66581480315745, KL divergence: 1.814628087673321\n",
      "Reconstruction loss: 467.2894317164661, KL divergence: 2.1276770885276086\n",
      "Reconstruction loss: 432.52197033352286, KL divergence: 0.9511480115855386\n",
      "Reconstruction loss: 446.01434841038395, KL divergence: 1.6682654098035519\n",
      "Reconstruction loss: 479.89058113841156, KL divergence: 2.3654931550418548\n",
      "Reconstruction loss: 486.5643493730166, KL divergence: 1.8991222006948827\n",
      "Reconstruction loss: 463.29636798277966, KL divergence: 1.729987095750929\n",
      "Reconstruction loss: 475.21138914449074, KL divergence: 2.9017046853485424\n",
      "Reconstruction loss: 485.3596146431016, KL divergence: 1.4426216574517077\n",
      "Reconstruction loss: 474.71497415906674, KL divergence: 2.211035621715392\n",
      "Reconstruction loss: 537.1495009640529, KL divergence: 3.491181958722996\n",
      "Reconstruction loss: 448.52023744966647, KL divergence: 2.7995696735858893\n",
      "Reconstruction loss: 479.46150002163324, KL divergence: 2.8934545837313954\n",
      "Reconstruction loss: 466.80802718877396, KL divergence: 5.285072355931376\n",
      "Reconstruction loss: 507.4325771098877, KL divergence: 3.5626500399868166\n",
      "Reconstruction loss: 449.43680447050093, KL divergence: 2.2637136657150974\n",
      "Reconstruction loss: 454.66068351259787, KL divergence: 0.6412171969621288\n",
      "Reconstruction loss: 478.1817233534414, KL divergence: 4.217077482822851\n",
      "Reconstruction loss: 473.91850326607783, KL divergence: 2.541388441332112\n",
      "Reconstruction loss: 490.1047008014158, KL divergence: 2.204105473638373\n",
      "Reconstruction loss: 471.4713534131421, KL divergence: 2.3012323772695633\n",
      "Reconstruction loss: 397.87966616169393, KL divergence: 1.2213098642513505\n",
      "Reconstruction loss: 384.5069288170184, KL divergence: 0.6213274150336371\n",
      "Reconstruction loss: 412.88208629350993, KL divergence: 1.2171939044232263\n",
      "Reconstruction loss: 397.980880089926, KL divergence: 1.3761081295486348\n",
      "Reconstruction loss: 403.1363675150999, KL divergence: 1.2072137432045502\n",
      "Reconstruction loss: 380.2043201304266, KL divergence: 1.019773265405086\n",
      "Reconstruction loss: 376.88595445525425, KL divergence: 1.0794171395431453\n",
      "Reconstruction loss: 433.2567601239581, KL divergence: 2.8921635840307536\n",
      "Reconstruction loss: 399.9088617418034, KL divergence: 2.0855227431747543\n",
      "Reconstruction loss: 370.9933370018149, KL divergence: 0.7504793438321506\n",
      "Reconstruction loss: 389.37868273707966, KL divergence: 1.9454172643191208\n",
      "Reconstruction loss: 386.75895156513377, KL divergence: 1.1096227419077271\n",
      "Reconstruction loss: 419.20946901350595, KL divergence: 1.5483768573924295\n",
      "Reconstruction loss: 384.895998379567, KL divergence: 0.9002011014284426\n",
      "Reconstruction loss: 461.69319454621944, KL divergence: 2.780945627254392\n",
      "Reconstruction loss: 396.74420049212875, KL divergence: 2.2109001204537133\n",
      "Reconstruction loss: 400.88283520520235, KL divergence: 1.8440668942924612\n",
      "Reconstruction loss: 409.0921270053563, KL divergence: 0.8223020504618223\n",
      "Reconstruction loss: 414.0589645735316, KL divergence: 2.583076132466343\n",
      "Reconstruction loss: 427.84924734416643, KL divergence: 1.0734464802882817\n",
      "Reconstruction loss: 428.88081768253005, KL divergence: 2.3735700941127043\n",
      "Reconstruction loss: 368.1184433456689, KL divergence: 1.4671748695085243\n",
      "Reconstruction loss: 415.1721976902688, KL divergence: 1.937353140313411\n",
      "Reconstruction loss: 424.8458244803078, KL divergence: 1.2285399593555528\n",
      "Reconstruction loss: 436.79256587599826, KL divergence: 3.3145241775110774\n",
      "Reconstruction loss: 415.16052637638376, KL divergence: 0.7112730884287539\n",
      "Reconstruction loss: 411.47866715218595, KL divergence: 1.8550055945661312\n",
      "Reconstruction loss: 390.2669903212756, KL divergence: 0.8494199202069062\n",
      "Reconstruction loss: 438.2084523025731, KL divergence: 1.7254871082741932\n",
      "Reconstruction loss: 381.485030340549, KL divergence: 0.4416298771132653\n",
      "Reconstruction loss: 394.9424016505087, KL divergence: 2.833559495077574\n",
      "Reconstruction loss: 380.0408034115254, KL divergence: 0.7443195013778703\n",
      "Reconstruction loss: 381.7501576489728, KL divergence: 1.4815347503602176\n",
      "Reconstruction loss: 335.72308845890996, KL divergence: 0.4959850713715312\n",
      "Reconstruction loss: 361.58535406968485, KL divergence: 1.4590788358110927\n",
      "Reconstruction loss: 364.4149071426698, KL divergence: 1.6982259293945607\n",
      "Reconstruction loss: 395.4426588434809, KL divergence: 1.4216921483359446\n",
      "Reconstruction loss: 371.58078897150864, KL divergence: 1.5517911544417564\n",
      "Reconstruction loss: 327.7779687240946, KL divergence: 1.1960926404879135\n",
      "Reconstruction loss: 369.68961055574977, KL divergence: 1.7437059543809927\n",
      "Reconstruction loss: 319.61626901713703, KL divergence: 1.258365509768246\n",
      "Reconstruction loss: 335.0327201781573, KL divergence: 1.1963959821345442\n",
      "Reconstruction loss: 393.8959342434727, KL divergence: 2.6328405242133375\n",
      "Reconstruction loss: 394.9179282550272, KL divergence: 2.6228855617516693\n",
      "Reconstruction loss: 353.8949444996548, KL divergence: 2.000527580602879\n",
      "Reconstruction loss: 368.35125175627036, KL divergence: 1.129201605608273\n",
      "Reconstruction loss: 387.950988752859, KL divergence: 2.188936731511907\n",
      "Reconstruction loss: 346.78973565103894, KL divergence: 2.1628291224998817\n",
      "Reconstruction loss: 372.0574563098022, KL divergence: 1.5961874055579373\n",
      "Reconstruction loss: 287.18871835583343, KL divergence: 0.7059528661338558\n",
      "Reconstruction loss: 397.084540578476, KL divergence: 1.8411981623280935\n",
      "Reconstruction loss: 303.59509908177336, KL divergence: 1.4063572077541737\n",
      "Reconstruction loss: 317.80614823093595, KL divergence: 0.8613848148758733\n",
      "Reconstruction loss: 393.5498108594486, KL divergence: 2.063962055266173\n",
      "Reconstruction loss: 370.11894220287826, KL divergence: 0.8814898543684475\n",
      "Reconstruction loss: 371.7604548999565, KL divergence: 0.9749348394944231\n",
      "Reconstruction loss: 359.4167411278552, KL divergence: 1.8826171483051288\n",
      "Reconstruction loss: 340.9382466151243, KL divergence: 0.9247147529436529\n",
      "Reconstruction loss: 397.6443792806584, KL divergence: 1.5656674088360736\n",
      "Reconstruction loss: 378.41289264718836, KL divergence: 1.7929169534146805\n",
      "Reconstruction loss: 310.6848389312134, KL divergence: 0.6315948409516556\n",
      "Reconstruction loss: 388.81318247476804, KL divergence: 2.3375532493212616\n",
      "Reconstruction loss: 389.68502119263667, KL divergence: 2.011040851558695\n",
      "Reconstruction loss: 331.0822217417692, KL divergence: 1.3744586945122537\n",
      "Reconstruction loss: 287.7120904722273, KL divergence: 0.9463365065898839\n",
      "Reconstruction loss: 360.0014379616937, KL divergence: 1.4734406629888626\n",
      "Reconstruction loss: 387.25623156383693, KL divergence: 2.248649126045949\n",
      "Reconstruction loss: 298.65827933386095, KL divergence: 0.7982384823905191\n",
      "Reconstruction loss: 289.71380560837304, KL divergence: 1.278761308203306\n",
      "Reconstruction loss: 303.9571049772371, KL divergence: 1.519261649712047\n",
      "Reconstruction loss: 288.36441272402044, KL divergence: 1.463263227040596\n",
      "Reconstruction loss: 260.10198723807, KL divergence: 1.2194397614605548\n",
      "Reconstruction loss: 241.98485966135482, KL divergence: 0.5716933469935019\n",
      "Reconstruction loss: 302.3097505222551, KL divergence: 1.0954827827189586\n",
      "Reconstruction loss: 367.4916170159848, KL divergence: 1.266406208827187\n",
      "Reconstruction loss: 263.2178175906833, KL divergence: 0.9149708165383637\n",
      "Reconstruction loss: 332.36977086645913, KL divergence: 1.4993946321412168\n",
      "Reconstruction loss: 322.92306626564186, KL divergence: 1.1728445268653114\n",
      "Reconstruction loss: 272.718021216426, KL divergence: 0.9086518232494487\n",
      "Reconstruction loss: 295.89914066059134, KL divergence: 0.9257819633693263\n",
      "Reconstruction loss: 305.4427599281473, KL divergence: 0.5941672924796526\n",
      "Reconstruction loss: 255.2340013877285, KL divergence: 0.9034498508580621\n",
      "Reconstruction loss: 347.9243589941364, KL divergence: 1.9951704024029\n",
      "Reconstruction loss: 291.7516653966157, KL divergence: 0.9644125843091599\n",
      "Reconstruction loss: 303.9578043864375, KL divergence: 0.7879576352846851\n",
      "Reconstruction loss: 370.3349185443519, KL divergence: 2.3097146527476986\n",
      "Reconstruction loss: 413.41813052982434, KL divergence: 1.9661694962969303\n",
      "Reconstruction loss: 254.20875371040728, KL divergence: 1.2260027891727479\n",
      "Reconstruction loss: 251.6829240410465, KL divergence: 0.7148525956727197\n",
      "Reconstruction loss: 322.9264539066178, KL divergence: 1.5309265999432253\n",
      "Reconstruction loss: 401.9629412729438, KL divergence: 2.314201333326462\n",
      "Reconstruction loss: 266.4461435551873, KL divergence: 0.8254827130956028\n",
      "Reconstruction loss: 377.11775856442614, KL divergence: 1.958639697908502\n",
      "Reconstruction loss: 345.8722497755174, KL divergence: 1.7597869601920062\n",
      "Reconstruction loss: 371.01358167402157, KL divergence: 1.980920331780442\n",
      "Reconstruction loss: 275.037111201131, KL divergence: 0.7993688853229135\n",
      "Reconstruction loss: 257.1983348575651, KL divergence: 0.9012016494644035\n",
      "Reconstruction loss: 327.9153787772751, KL divergence: 0.9503638501572605\n",
      "Reconstruction loss: 274.10579682433126, KL divergence: 0.9709641370500228\n",
      "Reconstruction loss: 375.6473506278701, KL divergence: 1.8994943127926909\n",
      "Reconstruction loss: 261.7912695409645, KL divergence: 0.9335675987585306\n",
      "Reconstruction loss: 273.4947540968049, KL divergence: 1.1500025149289255\n",
      "Reconstruction loss: 261.5585738121357, KL divergence: 1.0979222097543118\n",
      "Reconstruction loss: 219.92034237998104, KL divergence: 0.9144295368827374\n",
      "Reconstruction loss: 297.15096898874236, KL divergence: 1.0237987356472653\n",
      "Reconstruction loss: 232.43698798447028, KL divergence: 1.454529892827546\n",
      "Reconstruction loss: 281.97199604136114, KL divergence: 1.0145354134592517\n",
      "Reconstruction loss: 460.12977099360774, KL divergence: 1.856747324196391\n",
      "Reconstruction loss: 293.4966473088429, KL divergence: 0.8241696088013116\n",
      "Reconstruction loss: 297.7974820429575, KL divergence: 0.3927850212166939\n",
      "Reconstruction loss: 221.21624178147442, KL divergence: 0.9838498378620273\n",
      "Reconstruction loss: 219.85858005998165, KL divergence: 0.9980332220810169\n",
      "Reconstruction loss: 159.9706212693055, KL divergence: 0.3659180878748328\n",
      "Reconstruction loss: 201.5802139471109, KL divergence: 0.810225963382902\n",
      "Reconstruction loss: 354.3615652719982, KL divergence: 1.0757634804666143\n",
      "Reconstruction loss: 293.3204629061498, KL divergence: 1.4995105206738593\n",
      "Reconstruction loss: 365.9063005856856, KL divergence: 1.3923532145437605\n",
      "Reconstruction loss: 323.3241136642026, KL divergence: 0.7033090332865236\n",
      "Reconstruction loss: 189.4903897541996, KL divergence: 0.8246463795381099\n",
      "Reconstruction loss: 289.69978885464997, KL divergence: 1.27418563119052\n",
      "Reconstruction loss: 250.57312158946405, KL divergence: 0.41564792961719954\n",
      "Reconstruction loss: 325.9310924160893, KL divergence: 2.128780785110127\n",
      "Reconstruction loss: 190.09012131581346, KL divergence: 0.5149587287832578\n",
      "Reconstruction loss: 255.30531960764824, KL divergence: 0.9984760698407066\n",
      "Reconstruction loss: 316.06137845926133, KL divergence: 0.6309275116885824\n",
      "Reconstruction loss: 250.93906094920254, KL divergence: 0.6475676336290557\n",
      "Reconstruction loss: 354.8201973478499, KL divergence: 0.9137053056628965\n",
      "Reconstruction loss: 243.27188852355317, KL divergence: 0.7608432574548368\n",
      "Reconstruction loss: 267.4371286783504, KL divergence: 1.2915032832831996\n",
      "Reconstruction loss: 227.2108781439086, KL divergence: 0.34510297905921317\n",
      "Reconstruction loss: 448.43334682910995, KL divergence: 2.2265628490269833\n",
      "Reconstruction loss: 293.723934096622, KL divergence: 1.350520595667237\n",
      "Reconstruction loss: 360.0056252990559, KL divergence: 0.9782475118565197\n",
      "Reconstruction loss: 257.6901534679333, KL divergence: 0.6009715946450964\n",
      "Reconstruction loss: 231.17222483001538, KL divergence: 0.6760685499684711\n",
      "Reconstruction loss: 233.07601029031835, KL divergence: 0.9173040780925514\n",
      "Reconstruction loss: 222.69753138968315, KL divergence: 1.0703072236142042\n",
      "Reconstruction loss: 474.569491623935, KL divergence: 2.0995230955380473\n",
      "Reconstruction loss: 289.33450289258656, KL divergence: 0.3450900062184667\n",
      "Reconstruction loss: 286.0325212356646, KL divergence: 1.0160915698523125\n",
      "Reconstruction loss: 243.77449122544678, KL divergence: 1.1670267178301805\n",
      "Reconstruction loss: 355.19934068607455, KL divergence: 0.6442577944878034\n",
      "Reconstruction loss: 451.8985051947243, KL divergence: 0.9028719085181647\n",
      "Reconstruction loss: 203.87859775694616, KL divergence: 0.7868312823525043\n",
      "Reconstruction loss: 254.62046409546014, KL divergence: 0.6358766238607858\n",
      "Reconstruction loss: 312.13007390487144, KL divergence: 1.0283643695183233\n",
      "Reconstruction loss: 408.4017817280365, KL divergence: 1.0742866018352792\n",
      "Reconstruction loss: 189.32027524082798, KL divergence: 0.4414513842146696\n",
      "Reconstruction loss: 318.9995661186395, KL divergence: 0.872784558528213\n",
      "Reconstruction loss: 235.12387581279413, KL divergence: 0.7808986901691419\n",
      "Reconstruction loss: 288.9906491922084, KL divergence: 1.679959085942043\n",
      "Reconstruction loss: 304.28239055872035, KL divergence: 0.6921569936348178\n",
      "Reconstruction loss: 322.33012519041176, KL divergence: 0.41695518046306834\n",
      "Reconstruction loss: 209.91567649495568, KL divergence: 1.1224503790947047\n",
      "Reconstruction loss: 339.02068368681125, KL divergence: 0.8874588276328078\n",
      "Reconstruction loss: 265.99206086765344, KL divergence: 0.896969189361108\n",
      "Reconstruction loss: 382.7439112465833, KL divergence: 1.3582465178249135\n",
      "Reconstruction loss: 178.85785012381695, KL divergence: 0.41227340333878193\n",
      "Reconstruction loss: 289.0586066499134, KL divergence: 0.9032499389935736\n",
      "Reconstruction loss: 330.24540630012564, KL divergence: 1.52542878479039\n",
      "Reconstruction loss: 320.6095110232577, KL divergence: 0.6911402068316457\n",
      "Reconstruction loss: 285.37071500797435, KL divergence: 0.6257104911626676\n",
      "Reconstruction loss: 302.24846229535535, KL divergence: 0.2871746224389186\n",
      "Reconstruction loss: 168.79009781803427, KL divergence: 0.42162535682233765\n",
      "Reconstruction loss: 313.45681956039806, KL divergence: 0.30442993869911233\n",
      "Reconstruction loss: 425.969652547355, KL divergence: 1.2851412686088295\n",
      "Reconstruction loss: 246.5792011511336, KL divergence: 1.3800770850153108\n",
      "Reconstruction loss: 446.4254500589896, KL divergence: 0.6934028916316646\n",
      "Reconstruction loss: 231.51503927817072, KL divergence: 0.469458180473196\n",
      "Reconstruction loss: 219.17377806606942, KL divergence: 0.5141091699107333\n",
      "Reconstruction loss: 160.9409496976062, KL divergence: 0.542832180225653\n",
      "Reconstruction loss: 439.8867836216592, KL divergence: 0.9618828049184888\n",
      "Reconstruction loss: 330.19728965008335, KL divergence: 1.063075670203598\n",
      "Reconstruction loss: 346.67335068856806, KL divergence: 0.8233012625167515\n",
      "Reconstruction loss: 258.05034448453927, KL divergence: 0.7329503389098408\n",
      "Reconstruction loss: 364.65564624503213, KL divergence: 0.2563732221008054\n",
      "Reconstruction loss: 426.67357888853866, KL divergence: 0.7890830751914617\n",
      "Reconstruction loss: 269.9972694515859, KL divergence: 0.9247345814002899\n",
      "Reconstruction loss: 245.13288199650611, KL divergence: 0.6364077662236967\n",
      "Reconstruction loss: 192.59546529543633, KL divergence: 0.2385875289872399\n",
      "Reconstruction loss: 269.08251063070486, KL divergence: 0.37993707235764645\n",
      "Reconstruction loss: 194.9474475174652, KL divergence: 0.4182594450512335\n",
      "Reconstruction loss: 288.1892135200187, KL divergence: 1.095856126463381\n",
      "Reconstruction loss: 254.8646822453892, KL divergence: 0.6163608389795434\n",
      "Reconstruction loss: 246.6754734272545, KL divergence: 0.5601094892141749\n",
      "Reconstruction loss: 262.6219367697224, KL divergence: 0.6462216334741093\n",
      "Reconstruction loss: 177.87032524723872, KL divergence: 0.8178834739235222\n",
      "Reconstruction loss: 180.40921863170317, KL divergence: 0.6982982986761608\n",
      "Reconstruction loss: 310.0474615753345, KL divergence: 0.3771385756479863\n",
      "Reconstruction loss: 337.8841612857609, KL divergence: 0.37855739114114434\n",
      "Reconstruction loss: 152.41567136771644, KL divergence: 0.40828207645440506\n",
      "Reconstruction loss: 373.7178994165679, KL divergence: 1.5658575266410957\n",
      "Reconstruction loss: 282.4665483035553, KL divergence: 0.4349379576236975\n",
      "Reconstruction loss: 282.5254266409215, KL divergence: 0.41981640555862676\n",
      "Reconstruction loss: 210.4843711281715, KL divergence: 0.2509510601628974\n",
      "Reconstruction loss: 241.5639970936465, KL divergence: 0.25551928301945975\n",
      "Reconstruction loss: 390.3235651654969, KL divergence: 0.3970034933356194\n",
      "Reconstruction loss: 347.4510665967932, KL divergence: 0.18472684452143373\n",
      "Reconstruction loss: 154.133285048827, KL divergence: 0.4274703149822386\n",
      "Reconstruction loss: 214.23443032533487, KL divergence: 0.37262699120417697\n",
      "Reconstruction loss: 299.3971849040627, KL divergence: 0.3976121701676758\n",
      "Reconstruction loss: 280.52439213111455, KL divergence: 0.5783813697746398\n",
      "Reconstruction loss: 365.5183273439098, KL divergence: 0.7237275545034066\n",
      "Reconstruction loss: 231.06375347720842, KL divergence: 0.5522490487288754\n",
      "Reconstruction loss: 306.58002032981136, KL divergence: 0.4169950899907386\n",
      "Reconstruction loss: 205.70070610865184, KL divergence: 0.594032514639345\n",
      "Reconstruction loss: 185.30501151609238, KL divergence: 0.2741768711789072\n",
      "Reconstruction loss: 381.9020815559353, KL divergence: 0.3560122718295953\n",
      "Reconstruction loss: 327.0005800239809, KL divergence: 0.6015566808733604\n",
      "Reconstruction loss: 245.61086664681545, KL divergence: 0.22037783787467635\n",
      "Reconstruction loss: 267.5347126139652, KL divergence: 0.32020443781753444\n",
      "Reconstruction loss: 147.98762430501523, KL divergence: 0.33359871429907295\n",
      "Reconstruction loss: 251.6184136557313, KL divergence: 0.7466904426867279\n",
      "Reconstruction loss: 127.68913846325275, KL divergence: 0.12416213081941418\n",
      "Reconstruction loss: 316.0752244572709, KL divergence: 0.2797759841052756\n",
      "Reconstruction loss: 207.3722014660548, KL divergence: 0.25434660231588024\n",
      "Reconstruction loss: 249.88797177174428, KL divergence: 0.23191397805289748\n",
      "Reconstruction loss: 352.4228038742501, KL divergence: 0.6872183365683673\n",
      "Reconstruction loss: 205.82300328783077, KL divergence: 0.37421442521908665\n",
      "Reconstruction loss: 272.84021898829525, KL divergence: 0.3297509308460334\n",
      "Reconstruction loss: 161.32642678909622, KL divergence: 0.3696267352718932\n",
      "Reconstruction loss: 450.0252941417999, KL divergence: 0.8309816534743898\n",
      "Reconstruction loss: 262.30650172946656, KL divergence: 0.20493267738959636\n",
      "Reconstruction loss: 230.25452620853085, KL divergence: 0.45449432155264047\n",
      "Reconstruction loss: 294.9133273500071, KL divergence: 0.31244942380404567\n",
      "Reconstruction loss: 192.16686986648324, KL divergence: 0.34018396764431724\n",
      "Reconstruction loss: 253.76892209095996, KL divergence: 0.08621962185818194\n",
      "Reconstruction loss: 268.92795247982644, KL divergence: 0.08620164239515465\n",
      "Reconstruction loss: 216.60750109670352, KL divergence: 0.342799026095404\n",
      "Reconstruction loss: 332.7648382279116, KL divergence: 0.43168074400735523\n",
      "Reconstruction loss: 258.180776658074, KL divergence: 0.10108673819585134\n",
      "Reconstruction loss: 229.20869476534398, KL divergence: 0.2556700480737472\n",
      "Reconstruction loss: 443.3398104949703, KL divergence: 0.2668194678769147\n",
      "Reconstruction loss: 249.77624743682193, KL divergence: 0.4358839782459692\n",
      "Reconstruction loss: 130.80678179280162, KL divergence: 0.13211807474626724\n",
      "Reconstruction loss: 414.8683953773759, KL divergence: 0.3688390512927405\n",
      "Reconstruction loss: 272.92609126238335, KL divergence: 0.263474485727388\n",
      "Reconstruction loss: 186.5993967382811, KL divergence: 0.32475374234086524\n",
      "Reconstruction loss: 307.37337244517715, KL divergence: 0.4728272839977094\n",
      "Reconstruction loss: 289.39848186126807, KL divergence: 0.05610828088116809\n",
      "Reconstruction loss: 300.8571100631632, KL divergence: 0.21669516368646158\n",
      "Reconstruction loss: 152.1551648216828, KL divergence: 0.23157055836314278\n",
      "Reconstruction loss: 215.57029008341084, KL divergence: 0.23885829077927806\n",
      "Reconstruction loss: 384.66004271785715, KL divergence: 0.3106636858786741\n",
      "Reconstruction loss: 270.5603522224286, KL divergence: 0.24738656397950803\n",
      "Reconstruction loss: 278.63357738017197, KL divergence: 0.5009661083242195\n",
      "Reconstruction loss: 239.1211585403498, KL divergence: 0.17084981518759307\n",
      "Reconstruction loss: 418.4066577787815, KL divergence: 0.17660867964057875\n",
      "Reconstruction loss: 352.1485580998937, KL divergence: 0.17784459092049187\n",
      "Reconstruction loss: 319.8650954537888, KL divergence: 0.2609379917366778\n",
      "Reconstruction loss: 197.37773936533978, KL divergence: 0.1010761747131339\n",
      "Reconstruction loss: 221.29761204357, KL divergence: 0.15404919950783807\n",
      "Reconstruction loss: 204.95270498729684, KL divergence: 0.09855953389070299\n",
      "Reconstruction loss: 242.20197452477873, KL divergence: 0.3101510607982633\n",
      "Reconstruction loss: 398.21999777909707, KL divergence: 0.35420329047933785\n",
      "Reconstruction loss: 229.87830414853647, KL divergence: 0.1708465623793477\n",
      "Reconstruction loss: 121.3795375889944, KL divergence: 0.11558651680150933\n",
      "Reconstruction loss: 227.42193116969003, KL divergence: 0.1275272948315741\n",
      "Reconstruction loss: 254.27476484248763, KL divergence: 0.1773411543027268\n",
      "Reconstruction loss: 189.58644468441094, KL divergence: 0.10642655334486373\n",
      "Reconstruction loss: 161.22271503221117, KL divergence: 0.18666858811822074\n",
      "Reconstruction loss: 133.23883243815072, KL divergence: 0.06817068795294301\n",
      "Reconstruction loss: 157.83201469158718, KL divergence: 0.1034800334751591\n",
      "Reconstruction loss: 203.5888989431301, KL divergence: 0.03345661128109545\n",
      "Reconstruction loss: 166.8838650930527, KL divergence: 0.03857851537730611\n",
      "Reconstruction loss: 234.61472457778382, KL divergence: 0.08341178994608034\n",
      "Reconstruction loss: 302.7806911290268, KL divergence: 0.14675099688334514\n",
      "Reconstruction loss: 268.8481969037744, KL divergence: 0.04837701153410762\n",
      "Reconstruction loss: 156.4147527431753, KL divergence: 0.12979452946887488\n",
      "Reconstruction loss: 559.6519860081985, KL divergence: 0.06153674501059664\n",
      "Reconstruction loss: 207.1054146225799, KL divergence: 0.10767402011473443\n",
      "Reconstruction loss: 178.23961800471625, KL divergence: 0.09517206953803392\n",
      "Reconstruction loss: 338.41967347979426, KL divergence: 0.08060803681666301\n",
      "Reconstruction loss: 240.4781073996313, KL divergence: 0.09339401635725764\n",
      "Reconstruction loss: 134.82436549582772, KL divergence: 0.07802478714330302\n",
      "Reconstruction loss: 247.25210779043084, KL divergence: 0.22455595032201703\n",
      "Reconstruction loss: 330.0069159589631, KL divergence: 0.15487628249107116\n",
      "Reconstruction loss: 332.88468399869674, KL divergence: 0.28061798304569496\n",
      "Reconstruction loss: 281.66274580853, KL divergence: 0.26498209750167623\n",
      "Reconstruction loss: 258.6471838087449, KL divergence: 0.08655046914586417\n",
      "Reconstruction loss: 554.9658341622329, KL divergence: 0.18405551851005736\n",
      "Reconstruction loss: 358.35653360189633, KL divergence: 0.23964899126597417\n",
      "Reconstruction loss: 420.862370325431, KL divergence: 0.19139972503871422\n",
      "Reconstruction loss: 370.822265707681, KL divergence: 0.15668172876356767\n",
      "Reconstruction loss: 554.3508539315873, KL divergence: 0.029053759789424205\n",
      "Reconstruction loss: 391.2014391116457, KL divergence: 0.11875379101394762\n",
      "Reconstruction loss: 257.6840150636406, KL divergence: 0.0901303401882031\n",
      "Reconstruction loss: 347.3138865504661, KL divergence: 0.08255802066851592\n",
      "Reconstruction loss: 243.59870620658862, KL divergence: 0.07099610337808626\n",
      "Reconstruction loss: 254.76648200904543, KL divergence: 0.015060248934038678\n",
      "Reconstruction loss: 367.7546698316312, KL divergence: 0.09160453934109308\n",
      "Reconstruction loss: 219.22688780328275, KL divergence: 0.09574434896014028\n",
      "Reconstruction loss: 257.47359718636426, KL divergence: 0.07774888025989646\n",
      "Reconstruction loss: 167.15735412656272, KL divergence: 0.05940309533694893\n",
      "Reconstruction loss: 127.31377723739999, KL divergence: 0.026450734678197918\n",
      "Reconstruction loss: 323.66028358644826, KL divergence: 0.07365938235217018\n",
      "Reconstruction loss: 369.463349855488, KL divergence: 0.11438863294290846\n",
      "Reconstruction loss: 237.03977319951844, KL divergence: 0.0643422739988857\n",
      "Reconstruction loss: 222.09011884869022, KL divergence: 0.10967880730341639\n",
      "Reconstruction loss: 198.7366792454983, KL divergence: 0.04468864018278795\n",
      "Reconstruction loss: 431.05973216856603, KL divergence: 0.04165692428431439\n",
      "Reconstruction loss: 486.0841096583601, KL divergence: 0.4088873183997185\n",
      "Reconstruction loss: 291.07996583619547, KL divergence: 0.021469782788793468\n",
      "Reconstruction loss: 341.6523016858613, KL divergence: 0.03881353702660223\n",
      "Reconstruction loss: 260.0384190426677, KL divergence: 0.0792578557848298\n",
      "Reconstruction loss: 343.1353601854954, KL divergence: 0.10569064669643952\n",
      "Reconstruction loss: 131.2351075609797, KL divergence: 0.06189749153237262\n",
      "Reconstruction loss: 161.7977639665215, KL divergence: 0.1964814211004648\n",
      "Reconstruction loss: 237.48362437175808, KL divergence: 0.08616981618174657\n",
      "Reconstruction loss: 325.86059077100015, KL divergence: 0.1246649105244747\n",
      "Reconstruction loss: 261.4608068853528, KL divergence: 0.04328682048791338\n",
      "Reconstruction loss: 266.54028053995467, KL divergence: 0.0642705365657531\n",
      "Reconstruction loss: 478.2378197670881, KL divergence: 0.056561040846070554\n",
      "Reconstruction loss: 151.29351126966986, KL divergence: 0.07135346253744895\n",
      "Reconstruction loss: 486.65496388154156, KL divergence: 0.07814834918467639\n",
      "Reconstruction loss: 213.63089809420586, KL divergence: 0.11546249818975146\n",
      "Reconstruction loss: 252.24954549980822, KL divergence: 0.32632169618304574\n",
      "Reconstruction loss: 252.11676284380252, KL divergence: 0.03715149625994213\n",
      "Reconstruction loss: 203.2940230378747, KL divergence: 0.021669857137449844\n",
      "Reconstruction loss: 478.29532879646933, KL divergence: 0.07489195387634895\n",
      "Reconstruction loss: 495.19667722799585, KL divergence: 0.34061750575539146\n",
      "Reconstruction loss: 192.18971998461066, KL divergence: 0.052129739198221725\n",
      "Reconstruction loss: 422.71336919488357, KL divergence: 0.1326980980038004\n",
      "Reconstruction loss: 244.49110001805755, KL divergence: 0.13867695924831974\n",
      "Reconstruction loss: 143.22801907075717, KL divergence: 0.06588303986980676\n",
      "Reconstruction loss: 429.8420072282442, KL divergence: 0.11031225353186996\n",
      "Reconstruction loss: 173.95395709194185, KL divergence: 0.07963765847490606\n",
      "Reconstruction loss: 201.55320212068992, KL divergence: 0.054293271849948255\n",
      "Reconstruction loss: 216.8129430763036, KL divergence: 0.0515856785890807\n",
      "Reconstruction loss: 313.63768759046604, KL divergence: 0.10866617621727037\n",
      "Reconstruction loss: 301.4580684321443, KL divergence: 0.011225594111423487\n",
      "Reconstruction loss: 326.8773164511141, KL divergence: 0.03339099949368357\n",
      "Reconstruction loss: 280.71557968798584, KL divergence: 0.11897897346849606\n",
      "Reconstruction loss: 215.7796159161961, KL divergence: 0.12621764083200532\n",
      "Reconstruction loss: 218.19208315512205, KL divergence: 0.10872252201534927\n",
      "Reconstruction loss: 261.69426300170664, KL divergence: 0.10510129440925109\n",
      "Reconstruction loss: 363.789637683266, KL divergence: 0.19459126704547408\n",
      "Reconstruction loss: 223.75862304386192, KL divergence: 0.14345946011588956\n",
      "Reconstruction loss: 350.9963589237595, KL divergence: 0.07600648333452759\n",
      "Reconstruction loss: 229.80392890936614, KL divergence: 0.025031110041704274\n",
      "Reconstruction loss: 294.1111322158257, KL divergence: 0.1688232359482636\n",
      "Reconstruction loss: 227.51796510151576, KL divergence: 0.08401639200923883\n",
      "Reconstruction loss: 146.2176327289102, KL divergence: 0.03464349575109249\n",
      "Reconstruction loss: 229.16243309495331, KL divergence: 0.13170075418183008\n",
      "Reconstruction loss: 224.18693799093927, KL divergence: 0.07663802305615297\n",
      "Reconstruction loss: 206.6621021924387, KL divergence: 0.05778998686311526\n",
      "Reconstruction loss: 282.1800683163428, KL divergence: 0.052065679340103366\n",
      "Reconstruction loss: 233.99891122174625, KL divergence: 0.02906073408732779\n",
      "Reconstruction loss: 557.2595325298375, KL divergence: 0.20342823356808903\n",
      "Reconstruction loss: 406.51299083662246, KL divergence: 0.06325590173388046\n",
      "Reconstruction loss: 175.47548951055518, KL divergence: 0.04148099514016401\n",
      "Reconstruction loss: 387.08367526673226, KL divergence: 0.07160716399970124\n",
      "Reconstruction loss: 328.9048323705617, KL divergence: 0.06445467731168192\n",
      "Reconstruction loss: 361.3523300128281, KL divergence: 0.07324986645980325\n",
      "Reconstruction loss: 271.30322044309986, KL divergence: 0.10167743573418908\n",
      "Reconstruction loss: 219.96415447595928, KL divergence: 0.03695341496584198\n",
      "Reconstruction loss: 332.67467568197856, KL divergence: 0.16442746672119146\n",
      "Reconstruction loss: 189.7250661709067, KL divergence: 0.07497622268616438\n",
      "Reconstruction loss: 330.3589117332018, KL divergence: 0.17044827370607823\n",
      "Reconstruction loss: 304.6534858513028, KL divergence: 0.08445527231253636\n",
      "Reconstruction loss: 265.6505443324469, KL divergence: 0.0783055856298625\n",
      "Reconstruction loss: 391.7567257639985, KL divergence: 0.02087835606789229\n",
      "Reconstruction loss: 161.57028572177003, KL divergence: 0.03766206580610704\n",
      "Reconstruction loss: 375.97631352203996, KL divergence: 0.059185520434939576\n",
      "Reconstruction loss: 190.88121542524118, KL divergence: 0.005488255374571593\n",
      "Reconstruction loss: 171.22178616439413, KL divergence: 0.06817430658496093\n",
      "Reconstruction loss: 335.4116474512077, KL divergence: 0.13561531485499617\n",
      "Reconstruction loss: 176.77818073207067, KL divergence: 0.024993417380283378\n",
      "Reconstruction loss: 281.90972746658764, KL divergence: 0.07336462860718751\n",
      "Reconstruction loss: 378.2141326737982, KL divergence: 0.1492488925437393\n",
      "Reconstruction loss: 227.89886365008851, KL divergence: 0.12371269264245488\n",
      "Reconstruction loss: 398.4824765769089, KL divergence: 0.1998995354914755\n",
      "Reconstruction loss: 321.3031654807007, KL divergence: 0.09591770696416657\n",
      "Reconstruction loss: 350.16690479212775, KL divergence: 0.24331962649481237\n",
      "Reconstruction loss: 183.02451566951203, KL divergence: 0.05976609874954453\n",
      "Reconstruction loss: 217.93942880769296, KL divergence: 0.04270184071607763\n",
      "Reconstruction loss: 203.45736462443966, KL divergence: 0.03653199327890244\n",
      "Reconstruction loss: 473.86686679311185, KL divergence: 0.2929896731336544\n",
      "Reconstruction loss: 162.2130837807164, KL divergence: 0.022972068395277045\n",
      "Reconstruction loss: 276.32881620507226, KL divergence: 0.12097117674040975\n",
      "Reconstruction loss: 323.27640006487957, KL divergence: 0.10024114527940348\n",
      "Reconstruction loss: 309.65638204850666, KL divergence: 0.11259078382010396\n",
      "Reconstruction loss: 185.52777914837753, KL divergence: 0.04303364734898707\n",
      "Reconstruction loss: 293.1675758187456, KL divergence: 0.2485162498770193\n",
      "Reconstruction loss: 249.276573062272, KL divergence: 0.051735743454978944\n",
      "Reconstruction loss: 200.8676456605698, KL divergence: 0.10658755399906034\n",
      "Reconstruction loss: 226.13687663765512, KL divergence: 0.06315653850404473\n",
      "Reconstruction loss: 375.8023645238378, KL divergence: 0.19307353230150748\n",
      "Reconstruction loss: 167.64486297902144, KL divergence: 0.04185671831249971\n",
      "Reconstruction loss: 207.48070829753405, KL divergence: 0.025400395404740206\n",
      "Reconstruction loss: 174.3065426984563, KL divergence: 0.04054814042064431\n",
      "Reconstruction loss: 635.0947622517336, KL divergence: 0.37982483851588905\n",
      "Reconstruction loss: 254.00633171571627, KL divergence: 0.18021347907210616\n",
      "Reconstruction loss: 164.8710443426902, KL divergence: 0.11415133272853001\n",
      "Reconstruction loss: 339.92419890963214, KL divergence: 0.10580383591039488\n",
      "Reconstruction loss: 162.18312623955455, KL divergence: 0.037696876955336744\n",
      "Reconstruction loss: 194.03201688901652, KL divergence: 0.10868145959298997\n",
      "Reconstruction loss: 270.71037944307136, KL divergence: 0.05852808106891627\n",
      "Reconstruction loss: 467.5518842501327, KL divergence: 0.09856754699864412\n",
      "Reconstruction loss: 158.28735321457444, KL divergence: 0.07079556238148493\n",
      "Reconstruction loss: 201.07317812633403, KL divergence: 0.07120122975299603\n",
      "Reconstruction loss: 192.12780228409855, KL divergence: 0.028609741213519946\n",
      "Reconstruction loss: 405.82756262180607, KL divergence: 0.27128178094299743\n",
      "Reconstruction loss: 190.24102100511413, KL divergence: 0.17573542104588108\n",
      "Reconstruction loss: 274.57310569965114, KL divergence: 0.07489028257420505\n",
      "Reconstruction loss: 190.73496387386052, KL divergence: 0.01830676528260955\n",
      "Reconstruction loss: 320.68475604760795, KL divergence: 0.2901890842222023\n",
      "Reconstruction loss: 306.49577218029276, KL divergence: 0.14366787997092306\n",
      "Reconstruction loss: 342.78202933926525, KL divergence: 0.1472181217564007\n",
      "Reconstruction loss: 389.73316601722047, KL divergence: 0.2568664670877422\n",
      "Reconstruction loss: 212.42400340953157, KL divergence: 0.08149051430670551\n",
      "Reconstruction loss: 213.8450061547527, KL divergence: 0.18088019841973102\n",
      "Reconstruction loss: 309.75173103672626, KL divergence: 0.12164648698377994\n",
      "Reconstruction loss: 284.378362836287, KL divergence: 0.05153984009658036\n",
      "Reconstruction loss: 554.849937516931, KL divergence: 0.06848461147066143\n",
      "Reconstruction loss: 346.0954679034978, KL divergence: 0.3585559663263939\n",
      "Reconstruction loss: 211.05941227094377, KL divergence: 0.2542792812845116\n",
      "Reconstruction loss: 275.42512124112034, KL divergence: 0.04700363438207905\n",
      "Reconstruction loss: 157.2968263275388, KL divergence: 0.06628222974491599\n",
      "Reconstruction loss: 202.9514452140983, KL divergence: 0.17275003701036085\n",
      "Reconstruction loss: 275.80072983699324, KL divergence: 0.06217484915197341\n",
      "Reconstruction loss: 168.16795433522464, KL divergence: 0.0263836724585918\n",
      "Reconstruction loss: 235.7279963686654, KL divergence: 0.14690914987147763\n",
      "Reconstruction loss: 222.23542535475585, KL divergence: 0.4472821701242107\n",
      "Reconstruction loss: 161.71173838641562, KL divergence: 0.020330097871048225\n",
      "Reconstruction loss: 368.38753182042024, KL divergence: 0.10667231067440985\n",
      "Reconstruction loss: 240.0089060872479, KL divergence: 0.0919520689462115\n",
      "Reconstruction loss: 224.71403678050882, KL divergence: 0.1577872422597622\n",
      "Reconstruction loss: 276.12032347658294, KL divergence: 0.38265796158289356\n",
      "Reconstruction loss: 169.86483230394072, KL divergence: 0.032550060592954344\n",
      "Reconstruction loss: 458.808167525129, KL divergence: 0.1803329735149467\n",
      "Reconstruction loss: 230.70701812290747, KL divergence: 0.10963964329528958\n",
      "Reconstruction loss: 259.55156972814314, KL divergence: 0.0641817803548056\n",
      "Reconstruction loss: 444.4221027675717, KL divergence: 0.4362316579348704\n",
      "Reconstruction loss: 169.74017597200162, KL divergence: 0.08966252340366948\n",
      "Reconstruction loss: 498.48686815528896, KL divergence: 0.2838664887397413\n",
      "Reconstruction loss: 349.5603874727335, KL divergence: 0.2634291447508884\n",
      "Reconstruction loss: 237.94544244318425, KL divergence: 0.040212407559123975\n",
      "Reconstruction loss: 370.51880586604716, KL divergence: 0.3142928430258893\n",
      "Reconstruction loss: 270.4390685684473, KL divergence: 0.05605558288184043\n",
      "Reconstruction loss: 187.4064632501494, KL divergence: 0.06215968956994805\n",
      "Reconstruction loss: 416.8345999153173, KL divergence: 0.16509568205057823\n",
      "Reconstruction loss: 205.04672029466263, KL divergence: 0.02195469403433048\n",
      "Reconstruction loss: 192.81687377210665, KL divergence: 0.018075178049660023\n",
      "Reconstruction loss: 594.9208552521607, KL divergence: 0.15530713287964532\n",
      "Reconstruction loss: 174.6680144970503, KL divergence: 0.07578831220894289\n",
      "Reconstruction loss: 304.60943623463487, KL divergence: 0.14113544272938205\n",
      "Reconstruction loss: 261.1884107306396, KL divergence: 0.03245503483603529\n",
      "Reconstruction loss: 457.43738984673115, KL divergence: 0.2887418391723084\n",
      "Reconstruction loss: 410.10762977120703, KL divergence: 0.1528134068270845\n",
      "Reconstruction loss: 261.07148250872103, KL divergence: 0.38620288080600607\n",
      "Reconstruction loss: 409.00434937298644, KL divergence: 0.47682232416445314\n",
      "Reconstruction loss: 135.90640005046305, KL divergence: 0.020051715552962313\n",
      "Reconstruction loss: 162.21409695252652, KL divergence: 0.026746193136630625\n",
      "Reconstruction loss: 280.781136720091, KL divergence: 0.10634583029631056\n",
      "Reconstruction loss: 160.10177769567275, KL divergence: 0.13340443387947232\n",
      "Reconstruction loss: 272.4312768709146, KL divergence: 0.18738962931704084\n",
      "Reconstruction loss: 202.38256448135547, KL divergence: 0.109091363495328\n",
      "Reconstruction loss: 155.20516329351244, KL divergence: 0.025874187373516322\n",
      "Reconstruction loss: 169.16068191816672, KL divergence: 0.08601982183175211\n",
      "Reconstruction loss: 423.99438857795474, KL divergence: 0.1485806643645594\n",
      "Reconstruction loss: 193.86711620648492, KL divergence: 0.17477569450284802\n",
      "Reconstruction loss: 158.80261042554974, KL divergence: 0.015766560893261283\n",
      "Reconstruction loss: 301.8362394959338, KL divergence: 0.2548779775734449\n",
      "Reconstruction loss: 454.9110023415893, KL divergence: 0.3767254635912158\n",
      "Reconstruction loss: 177.13035222624785, KL divergence: 0.019601392144827257\n",
      "Reconstruction loss: 304.7750918250929, KL divergence: 0.36274717264690837\n",
      "Reconstruction loss: 281.6567722231343, KL divergence: 0.10223892174126098\n",
      "Reconstruction loss: 184.13460575408322, KL divergence: 0.055154967748442474\n",
      "Reconstruction loss: 166.54705212292427, KL divergence: 0.05563140262494165\n",
      "Reconstruction loss: 293.41261946696034, KL divergence: 0.05241047276561284\n",
      "Reconstruction loss: 593.0771279997417, KL divergence: 0.26942971259187953\n",
      "Reconstruction loss: 376.5794368646759, KL divergence: 0.223394510787556\n",
      "Reconstruction loss: 237.67540126977386, KL divergence: 0.22072939083708815\n",
      "Reconstruction loss: 254.7969079071281, KL divergence: 0.14703222327165894\n",
      "Reconstruction loss: 177.71231857535622, KL divergence: 0.11139400718861492\n",
      "Reconstruction loss: 242.22071357284224, KL divergence: 0.1677884574418006\n",
      "Reconstruction loss: 292.5515393152916, KL divergence: 0.24936467269770218\n",
      "Reconstruction loss: 203.38593078336788, KL divergence: 0.09840203184795548\n",
      "Reconstruction loss: 255.5693935404737, KL divergence: 0.049058734255372494\n",
      "Reconstruction loss: 159.54894054251298, KL divergence: 0.04672486787680086\n",
      "Reconstruction loss: 258.4200030904736, KL divergence: 0.10349185263657751\n",
      "Reconstruction loss: 426.8782244578428, KL divergence: 0.3904600926361541\n",
      "Reconstruction loss: 188.53407401220375, KL divergence: 0.06401409681975201\n",
      "Reconstruction loss: 252.85243775966399, KL divergence: 0.14709359251876586\n",
      "Reconstruction loss: 230.43372130471886, KL divergence: 0.13466793317233577\n",
      "Reconstruction loss: 238.8217822678829, KL divergence: 0.12907587321181951\n",
      "Reconstruction loss: 230.2371483688258, KL divergence: 0.03908254867274402\n",
      "Reconstruction loss: 146.12311165579902, KL divergence: 0.008540111636298364\n",
      "Reconstruction loss: 139.17479477902734, KL divergence: 0.02362971361579591\n",
      "Reconstruction loss: 287.55629362605833, KL divergence: 0.09680114115110172\n",
      "Reconstruction loss: 294.0100730335062, KL divergence: 0.07517211796930973\n",
      "Reconstruction loss: 195.14633301158176, KL divergence: 0.07789678694551838\n",
      "Reconstruction loss: 592.2237370543065, KL divergence: 0.5217056447650914\n",
      "Reconstruction loss: 265.2408190637452, KL divergence: 0.10707213952167344\n",
      "Reconstruction loss: 164.97895234515386, KL divergence: 0.019296769448732765\n",
      "Reconstruction loss: 259.8145540422967, KL divergence: 0.2941953285711016\n",
      "Reconstruction loss: 249.13756336603336, KL divergence: 0.15871896865533008\n",
      "Reconstruction loss: 298.5076723663024, KL divergence: 0.2968116487066939\n",
      "Reconstruction loss: 316.7745913619375, KL divergence: 0.17077070854822318\n",
      "Reconstruction loss: 185.6795234192553, KL divergence: 0.0350111487594823\n",
      "Reconstruction loss: 187.99198415516753, KL divergence: 0.13117560051674504\n",
      "Reconstruction loss: 287.80432232177236, KL divergence: 0.057447512416104185\n",
      "Reconstruction loss: 237.73072156115114, KL divergence: 0.049906250613061476\n",
      "Reconstruction loss: 182.93237645589159, KL divergence: 0.09874714957980713\n",
      "Reconstruction loss: 295.3059451928861, KL divergence: 0.1358174344876794\n",
      "Reconstruction loss: 477.2941598077119, KL divergence: 0.23990159368228747\n",
      "Reconstruction loss: 317.4481034338578, KL divergence: 0.32282904351355113\n",
      "Reconstruction loss: 192.4726682870636, KL divergence: 0.019651334083188754\n",
      "Reconstruction loss: 273.2673165436788, KL divergence: 0.22258489505921886\n",
      "Reconstruction loss: 197.15706890898682, KL divergence: 0.027739539028122484\n",
      "Reconstruction loss: 195.1383280520073, KL divergence: 0.08346318735445207\n",
      "Reconstruction loss: 325.3551111578133, KL divergence: 0.26436015274675145\n",
      "Reconstruction loss: 292.85007687155405, KL divergence: 0.14668330714186162\n",
      "Reconstruction loss: 336.1372546173758, KL divergence: 0.047508345510686445\n",
      "Reconstruction loss: 373.1765728974643, KL divergence: 0.2600952439047687\n",
      "Reconstruction loss: 189.40882603847268, KL divergence: 0.03956846065935993\n",
      "Reconstruction loss: 159.20222332855926, KL divergence: 0.03308957452763234\n",
      "Reconstruction loss: 156.21190610959712, KL divergence: 0.041645468772457395\n",
      "Reconstruction loss: 224.21788961635406, KL divergence: 0.05595298304860791\n",
      "Reconstruction loss: 197.99584605948877, KL divergence: 0.11226820110594082\n",
      "Reconstruction loss: 229.204132280053, KL divergence: 0.21215470522324825\n",
      "Reconstruction loss: 210.1483771579504, KL divergence: 0.0578199914826365\n",
      "Reconstruction loss: 180.611242931613, KL divergence: 0.014438924828328481\n",
      "Reconstruction loss: 249.83016453058747, KL divergence: 0.15380954846321154\n",
      "Reconstruction loss: 294.77735756364615, KL divergence: 0.1626417898375761\n",
      "Reconstruction loss: 241.38401893189234, KL divergence: 0.09773037926692985\n",
      "Reconstruction loss: 326.13433258039674, KL divergence: 0.12683318504445396\n",
      "Reconstruction loss: 245.75686142232692, KL divergence: 0.12072986267466362\n",
      "Reconstruction loss: 360.5738202907595, KL divergence: 0.15977882852959646\n",
      "Reconstruction loss: 307.7465952649299, KL divergence: 0.13321520932870368\n",
      "Reconstruction loss: 209.0540018227414, KL divergence: 0.056699259653764655\n",
      "Reconstruction loss: 452.7598023923497, KL divergence: 0.11186274700565885\n",
      "Reconstruction loss: 289.1770947727894, KL divergence: 0.12193327472739385\n",
      "Reconstruction loss: 473.20311547037295, KL divergence: 0.3590551360822968\n",
      "Reconstruction loss: 197.18883268634505, KL divergence: 0.04361155934413985\n",
      "Reconstruction loss: 223.87829946177078, KL divergence: 0.26380042586891983\n",
      "Reconstruction loss: 279.5101911362236, KL divergence: 0.05889592875341293\n",
      "Reconstruction loss: 188.55997070535628, KL divergence: 0.08601389635702616\n",
      "Reconstruction loss: 277.11727138844265, KL divergence: 0.025431462413363137\n",
      "Reconstruction loss: 419.11754736672026, KL divergence: 0.4047017808135434\n",
      "Reconstruction loss: 643.1143674329737, KL divergence: 0.48021385437277203\n",
      "Reconstruction loss: 421.50025951206567, KL divergence: 0.32688926320690326\n",
      "Reconstruction loss: 170.7547179459871, KL divergence: 0.010013860540552921\n",
      "Reconstruction loss: 173.3532334846114, KL divergence: 0.052649979738089436\n",
      "Reconstruction loss: 282.62320263490153, KL divergence: 0.23764144417291566\n",
      "Reconstruction loss: 143.18829419443267, KL divergence: 0.029343659576462455\n",
      "Reconstruction loss: 183.13987562821694, KL divergence: 0.12498206075124191\n",
      "Reconstruction loss: 592.7380898453616, KL divergence: 0.29371634637542143\n",
      "Reconstruction loss: 333.6171916449001, KL divergence: 0.18433710193720054\n",
      "Reconstruction loss: 424.50149372727964, KL divergence: 0.21021540514300735\n",
      "Reconstruction loss: 304.5741917143036, KL divergence: 0.20484623349940567\n",
      "Reconstruction loss: 158.70027898297525, KL divergence: 0.02874792056202985\n",
      "Reconstruction loss: 458.7234541611076, KL divergence: 0.5971796564445233\n",
      "Reconstruction loss: 348.6561797552986, KL divergence: 0.13605459035415113\n",
      "Reconstruction loss: 292.22102708043474, KL divergence: 0.1259781976012906\n",
      "Reconstruction loss: 327.2564738091819, KL divergence: 0.07590269851485038\n",
      "Reconstruction loss: 302.0173282486359, KL divergence: 0.08911680714115816\n",
      "Reconstruction loss: 340.16358877094615, KL divergence: 0.10421990668207165\n",
      "Reconstruction loss: 206.6323455719397, KL divergence: 0.08357408954755063\n",
      "Reconstruction loss: 429.8481150582266, KL divergence: 0.1464037941738921\n",
      "Reconstruction loss: 199.67867186055182, KL divergence: 0.1312674796410126\n",
      "Reconstruction loss: 288.94783874215, KL divergence: 0.17072081336835182\n",
      "Reconstruction loss: 191.05827585768674, KL divergence: 0.037575019885482785\n",
      "Reconstruction loss: 159.8770501408232, KL divergence: 0.03665087207160228\n",
      "Reconstruction loss: 421.89448585059716, KL divergence: 0.4585203546517296\n",
      "Reconstruction loss: 271.71846764224506, KL divergence: 0.05643463937858817\n",
      "Reconstruction loss: 181.7702914653892, KL divergence: 0.01733984586376175\n",
      "Reconstruction loss: 384.54261289055063, KL divergence: 0.32474837695055153\n",
      "Reconstruction loss: 352.0468234838455, KL divergence: 0.17723771894500862\n",
      "Reconstruction loss: 381.98824700829067, KL divergence: 0.09902141644725027\n",
      "Reconstruction loss: 225.02828393776431, KL divergence: 0.08389017131878052\n",
      "Reconstruction loss: 310.92548140015845, KL divergence: 0.015597653328000338\n",
      "Reconstruction loss: 246.29408431711786, KL divergence: 0.11391505839467203\n",
      "Reconstruction loss: 289.8517736941054, KL divergence: 0.30464332600562505\n",
      "Reconstruction loss: 271.1097152597959, KL divergence: 0.15848073682038422\n",
      "Reconstruction loss: 356.60772799915947, KL divergence: 0.2043462000484037\n",
      "Reconstruction loss: 159.97659123025142, KL divergence: 0.04330981716436083\n",
      "Reconstruction loss: 151.61606764611508, KL divergence: 0.023974574481809008\n",
      "Reconstruction loss: 201.21959103823707, KL divergence: 0.023012730035035844\n",
      "Reconstruction loss: 238.75517602699205, KL divergence: 0.05876582768273497\n",
      "Reconstruction loss: 325.32258882353915, KL divergence: 0.06413045103515091\n",
      "Reconstruction loss: 386.48635370772763, KL divergence: 0.2566152877366912\n",
      "Reconstruction loss: 359.44724252449436, KL divergence: 0.23439837331255842\n",
      "Reconstruction loss: 426.4773622209501, KL divergence: 0.1152149101756425\n",
      "Reconstruction loss: 238.22316498895503, KL divergence: 0.0947645458441041\n",
      "Reconstruction loss: 237.31157053498504, KL divergence: 0.3242096733104649\n",
      "Reconstruction loss: 580.9193590376527, KL divergence: 0.18232171327147478\n",
      "Reconstruction loss: 202.11720048407386, KL divergence: 0.044916956784206985\n",
      "Reconstruction loss: 267.94573246637094, KL divergence: 0.06823651948134951\n",
      "Reconstruction loss: 174.22847169509942, KL divergence: 0.0760961955834184\n",
      "Reconstruction loss: 195.43003516916008, KL divergence: 0.09036320834298617\n",
      "Reconstruction loss: 544.0610244993093, KL divergence: 0.21033105119041223\n",
      "Reconstruction loss: 248.57685961509804, KL divergence: 0.4892894078789734\n",
      "Reconstruction loss: 212.24525611920177, KL divergence: 0.06036320305777232\n",
      "Reconstruction loss: 427.01843228014116, KL divergence: 0.20357525893399686\n",
      "Reconstruction loss: 223.12940741880604, KL divergence: 0.10517292163442471\n",
      "Reconstruction loss: 282.23437513727913, KL divergence: 0.03794006861216054\n",
      "Reconstruction loss: 144.83108815006054, KL divergence: 0.025132459632688897\n",
      "Reconstruction loss: 165.30119197840116, KL divergence: 0.012601138465028638\n",
      "Reconstruction loss: 338.37565358696736, KL divergence: 0.31131378952574407\n",
      "Reconstruction loss: 287.9447268524307, KL divergence: 0.1424591181898881\n",
      "Reconstruction loss: 351.00464594279583, KL divergence: 0.12799413175616137\n",
      "Reconstruction loss: 166.873288186793, KL divergence: 0.014001149005247793\n",
      "Reconstruction loss: 217.93910507726662, KL divergence: 0.017132007906976165\n",
      "Reconstruction loss: 148.20269697020143, KL divergence: 0.0309965914469722\n",
      "Reconstruction loss: 320.8129386541828, KL divergence: 0.13663831361802686\n",
      "Reconstruction loss: 161.17310716216957, KL divergence: 0.1494238921585973\n",
      "Reconstruction loss: 148.43261422146713, KL divergence: 0.032083315143084934\n",
      "Reconstruction loss: 522.172119586342, KL divergence: 0.25288880227435656\n",
      "Reconstruction loss: 411.5496989866225, KL divergence: 0.5171612923036333\n",
      "Reconstruction loss: 313.37703808036224, KL divergence: 0.06333559082953949\n",
      "Reconstruction loss: 257.84970515301706, KL divergence: 0.042885199884908465\n",
      "Reconstruction loss: 191.65087087274128, KL divergence: 0.049989282627376286\n",
      "Reconstruction loss: 625.1285641210225, KL divergence: 0.5841183501601095\n",
      "Reconstruction loss: 196.50646386202098, KL divergence: 0.06421361815238946\n",
      "Reconstruction loss: 138.12539817669395, KL divergence: 0.044580068884482515\n",
      "Reconstruction loss: 573.5047685930851, KL divergence: 0.09405760003057512\n",
      "Reconstruction loss: 358.21605885795776, KL divergence: 0.22859444572141885\n",
      "Reconstruction loss: 301.80186475485317, KL divergence: 0.1843891606783178\n",
      "Reconstruction loss: 356.8799083233423, KL divergence: 0.09179720582260659\n",
      "Reconstruction loss: 190.00530352710513, KL divergence: 0.03141579891374485\n",
      "Reconstruction loss: 450.90214048541776, KL divergence: 0.08192081500691473\n",
      "Reconstruction loss: 521.5382164243335, KL divergence: 0.2284446940149119\n",
      "Reconstruction loss: 408.804062274299, KL divergence: 0.04831400134751068\n",
      "Reconstruction loss: 183.59546451325556, KL divergence: 0.059909591786337535\n",
      "Reconstruction loss: 299.30124627532365, KL divergence: 0.22115640775761586\n",
      "Reconstruction loss: 187.4873554666537, KL divergence: 0.1355476073251093\n",
      "Reconstruction loss: 311.412440675149, KL divergence: 0.3123897360911345\n",
      "Reconstruction loss: 376.4772204062765, KL divergence: 0.20637673226393388\n",
      "Reconstruction loss: 191.7733560085325, KL divergence: 0.081644270973599\n",
      "Reconstruction loss: 189.415499666426, KL divergence: 0.02633878050821742\n",
      "Reconstruction loss: 215.8402049426278, KL divergence: 0.05614985403758066\n",
      "Reconstruction loss: 134.79386976411982, KL divergence: 0.045428518284898134\n",
      "Reconstruction loss: 432.89441777745196, KL divergence: 0.06087277217868364\n",
      "Reconstruction loss: 424.1555975311472, KL divergence: 0.4034421264302089\n",
      "Reconstruction loss: 361.3651432815673, KL divergence: 0.1900466825718976\n",
      "Reconstruction loss: 199.94119804804296, KL divergence: 0.03644147575500212\n",
      "Reconstruction loss: 197.886973085286, KL divergence: 0.28155590683843496\n",
      "Reconstruction loss: 411.62283331042073, KL divergence: 0.15317127627102078\n",
      "Reconstruction loss: 349.8542018116058, KL divergence: 0.283352110863557\n",
      "Reconstruction loss: 513.7378789130967, KL divergence: 0.3734520952694404\n",
      "Reconstruction loss: 130.49522033422218, KL divergence: 0.015239526327740449\n",
      "Reconstruction loss: 152.85095764602931, KL divergence: 0.15479206255600314\n",
      "Reconstruction loss: 484.85604412310624, KL divergence: 0.5092976198783451\n",
      "Reconstruction loss: 243.37859143693652, KL divergence: 0.008855832152007193\n",
      "Reconstruction loss: 455.7713765893768, KL divergence: 0.2126131392153014\n",
      "Reconstruction loss: 325.3263361930088, KL divergence: 0.20618725079593025\n",
      "Reconstruction loss: 218.2615427086364, KL divergence: 0.13033425797285786\n",
      "Reconstruction loss: 317.09052026349923, KL divergence: 0.06695011080419949\n",
      "Reconstruction loss: 277.86501795680675, KL divergence: 0.19330340138840507\n",
      "Reconstruction loss: 344.03378357017505, KL divergence: 0.39018209406776333\n",
      "Reconstruction loss: 312.2996180987715, KL divergence: 0.10449371664002144\n",
      "Reconstruction loss: 250.47274206890623, KL divergence: 0.12437143017982866\n",
      "Reconstruction loss: 152.4782806167046, KL divergence: 0.06598192262219005\n",
      "Reconstruction loss: 349.5484120028096, KL divergence: 0.42321144221038226\n",
      "Reconstruction loss: 155.69058157111002, KL divergence: 0.026894762923283355\n",
      "Reconstruction loss: 288.7887726128505, KL divergence: 0.03599897115648182\n",
      "Reconstruction loss: 225.6972902473723, KL divergence: 0.05632127510992574\n",
      "Reconstruction loss: 146.17556743340668, KL divergence: 0.04400568487589818\n",
      "Reconstruction loss: 206.1708411091328, KL divergence: 0.06045691431921835\n",
      "Reconstruction loss: 314.2040215188505, KL divergence: 0.2665354052706353\n",
      "Reconstruction loss: 242.45668510317356, KL divergence: 0.031376542530972495\n",
      "Reconstruction loss: 302.78306605364486, KL divergence: 0.12230618410466781\n",
      "Reconstruction loss: 344.6971791109802, KL divergence: 0.1542748446651423\n",
      "Reconstruction loss: 238.73391203796143, KL divergence: 0.04681244314871991\n",
      "Reconstruction loss: 256.5793708138254, KL divergence: 0.43826736147451006\n",
      "Reconstruction loss: 226.99847336410286, KL divergence: 0.197213258037227\n",
      "Reconstruction loss: 413.42747026961234, KL divergence: 0.09500354320689758\n",
      "Reconstruction loss: 188.56001852146022, KL divergence: 0.1418776667685373\n",
      "Reconstruction loss: 156.75739390654942, KL divergence: 0.06951325602807507\n",
      "Reconstruction loss: 215.46653643396627, KL divergence: 0.027783898595388656\n",
      "Reconstruction loss: 351.0832620764416, KL divergence: 0.28891265932096766\n",
      "Reconstruction loss: 296.52045574361694, KL divergence: 0.17926952831812792\n",
      "Reconstruction loss: 185.1269857585139, KL divergence: 0.06182439550765645\n",
      "Reconstruction loss: 200.20923584284276, KL divergence: 0.11699164261327805\n",
      "Reconstruction loss: 163.74222321602653, KL divergence: 0.07100778000810143\n",
      "Reconstruction loss: 211.99534271096812, KL divergence: 0.33732941096574\n",
      "Reconstruction loss: 234.26138238387995, KL divergence: 0.3919365535294535\n",
      "Reconstruction loss: 309.49624928930996, KL divergence: 0.19642104602012145\n",
      "Reconstruction loss: 159.94574002393722, KL divergence: 0.035586917457137934\n",
      "Reconstruction loss: 253.84547810180746, KL divergence: 0.029445509510193923\n",
      "Reconstruction loss: 298.944585187961, KL divergence: 0.593231156985186\n",
      "Reconstruction loss: 503.660048942871, KL divergence: 0.5715118918421155\n",
      "Reconstruction loss: 303.4753883672988, KL divergence: 0.24392839004901257\n",
      "Reconstruction loss: 210.00020942153338, KL divergence: 0.17102870195507025\n",
      "Reconstruction loss: 200.20182671579886, KL divergence: 0.014681602997386412\n",
      "Reconstruction loss: 159.00667961462386, KL divergence: 0.08939057103730264\n",
      "Reconstruction loss: 191.7140101109224, KL divergence: 0.029510432459774805\n",
      "Reconstruction loss: 352.94245574597255, KL divergence: 0.02320554609865394\n",
      "Reconstruction loss: 507.5312982215458, KL divergence: 0.4889676255538009\n",
      "Reconstruction loss: 162.59518592028653, KL divergence: 0.202639748553273\n",
      "Reconstruction loss: 187.97095518293958, KL divergence: 0.24521910628996435\n",
      "Reconstruction loss: 194.25933429350508, KL divergence: 0.20256517320178824\n",
      "Reconstruction loss: 221.87629763094415, KL divergence: 0.1752419238189018\n",
      "Reconstruction loss: 167.9475871452467, KL divergence: 0.019436748504304435\n",
      "Reconstruction loss: 221.26074944525664, KL divergence: 0.030628022070099514\n",
      "Reconstruction loss: 229.0266590181977, KL divergence: 0.21448686081771456\n",
      "Reconstruction loss: 356.5532140732911, KL divergence: 0.23422918390633024\n",
      "Reconstruction loss: 243.93594066301117, KL divergence: 0.07164210259130138\n",
      "Reconstruction loss: 492.360721674122, KL divergence: 0.9766148380324033\n",
      "Reconstruction loss: 171.61870965882014, KL divergence: 0.09478850797331223\n",
      "Reconstruction loss: 293.06987020572274, KL divergence: 0.04907908296218705\n",
      "Reconstruction loss: 292.90623435276586, KL divergence: 0.10938766600934818\n",
      "Reconstruction loss: 439.2664288840105, KL divergence: 0.03659700088621093\n",
      "Reconstruction loss: 502.35265465169795, KL divergence: 0.4097127454785617\n",
      "Reconstruction loss: 260.07500218414594, KL divergence: 0.37310673752982604\n",
      "Reconstruction loss: 242.89756050675578, KL divergence: 0.03384730029889621\n",
      "Reconstruction loss: 183.07726083033907, KL divergence: 0.0761476091708716\n",
      "Reconstruction loss: 247.9452651084703, KL divergence: 0.14074156214782313\n",
      "Reconstruction loss: 262.0233425874424, KL divergence: 0.028818990810250134\n",
      "Reconstruction loss: 261.55587741459993, KL divergence: 0.08911179207740161\n",
      "Reconstruction loss: 254.09058519456423, KL divergence: 0.03836720891784151\n",
      "Reconstruction loss: 319.86807302223974, KL divergence: 0.03656620919177167\n",
      "Reconstruction loss: 240.24131235085298, KL divergence: 0.042350165319135546\n",
      "Reconstruction loss: 183.81576527153632, KL divergence: 0.1956393657124001\n",
      "Reconstruction loss: 223.9049833152958, KL divergence: 0.23211510078699948\n",
      "Reconstruction loss: 367.02655750308054, KL divergence: 0.3514494395710293\n",
      "Reconstruction loss: 174.4487544595988, KL divergence: 0.09557758283421058\n",
      "Reconstruction loss: 220.4667867744235, KL divergence: 0.09690564836322085\n",
      "Reconstruction loss: 263.17530764882224, KL divergence: 0.4620006742107685\n",
      "Reconstruction loss: 331.05286768215745, KL divergence: 0.645940559065845\n",
      "Reconstruction loss: 215.3504551528447, KL divergence: 0.07076141470803854\n",
      "Reconstruction loss: 412.4057914452254, KL divergence: 0.12574862273773396\n",
      "Reconstruction loss: 170.7424363504649, KL divergence: 0.045456784065540545\n",
      "Reconstruction loss: 142.01773572034296, KL divergence: 0.02177739034403947\n",
      "Reconstruction loss: 165.7402016061538, KL divergence: 0.08621486755397378\n",
      "Reconstruction loss: 237.00811617509197, KL divergence: 0.15763224088520217\n",
      "Reconstruction loss: 188.18588045920825, KL divergence: 0.03198875812227431\n",
      "Reconstruction loss: 168.2745919552384, KL divergence: 0.054569507266333694\n",
      "Reconstruction loss: 231.305623718054, KL divergence: 0.1988207021171794\n",
      "Reconstruction loss: 630.8213315446787, KL divergence: 0.27302992072083704\n",
      "Reconstruction loss: 155.61239168560115, KL divergence: 0.02743172435334662\n",
      "Reconstruction loss: 189.9138045882849, KL divergence: 0.0495692043295865\n",
      "Reconstruction loss: 143.82785197643938, KL divergence: 0.05774649843158236\n",
      "Reconstruction loss: 170.28714313295706, KL divergence: 0.2123188393040839\n",
      "Reconstruction loss: 204.4365605394292, KL divergence: 0.036826320536679114\n",
      "Reconstruction loss: 180.24113372863536, KL divergence: 0.0981982126385117\n",
      "Reconstruction loss: 175.4792961716995, KL divergence: 0.1415554116835362\n",
      "Reconstruction loss: 345.8840392751169, KL divergence: 0.6683513943573047\n",
      "Reconstruction loss: 184.16729527389583, KL divergence: 0.028239307139902692\n",
      "Reconstruction loss: 212.58512528040745, KL divergence: 0.12585830105662837\n",
      "Reconstruction loss: 269.1090445223035, KL divergence: 0.31509170198674813\n",
      "Reconstruction loss: 150.36063538122633, KL divergence: 0.06619474368763434\n",
      "Reconstruction loss: 244.20655874067137, KL divergence: 0.4879835540592575\n",
      "Reconstruction loss: 287.361647446548, KL divergence: 0.13981259343311458\n",
      "Reconstruction loss: 195.50991756093416, KL divergence: 0.1064153099354106\n",
      "Reconstruction loss: 354.2827293884253, KL divergence: 0.2314804917423285\n",
      "Reconstruction loss: 427.6608686626055, KL divergence: 0.4512522558021008\n",
      "Reconstruction loss: 364.2411486996432, KL divergence: 0.9249142680274811\n",
      "Reconstruction loss: 206.5272838036109, KL divergence: 0.5955761469412469\n",
      "Reconstruction loss: 397.4490458032989, KL divergence: 0.8140358026496035\n",
      "Reconstruction loss: 496.35821986748084, KL divergence: 0.2765087401346892\n",
      "Reconstruction loss: 138.6432092828996, KL divergence: 0.024485543593348147\n",
      "Reconstruction loss: 319.2993375801369, KL divergence: 0.3467944766181806\n",
      "Reconstruction loss: 246.10839279414742, KL divergence: 0.17686764487032347\n",
      "Reconstruction loss: 148.76645199822013, KL divergence: 0.021977817758127183\n",
      "Reconstruction loss: 286.9576420359725, KL divergence: 0.7743369471242891\n",
      "Reconstruction loss: 350.61458107916326, KL divergence: 0.45263874657564046\n",
      "Reconstruction loss: 231.32572402419135, KL divergence: 0.15305062198217761\n",
      "Reconstruction loss: 183.70838249916355, KL divergence: 0.15369889326638736\n",
      "Reconstruction loss: 229.14843587507573, KL divergence: 0.07330061369418711\n",
      "Reconstruction loss: 212.8323011188544, KL divergence: 0.12459930160052618\n",
      "Reconstruction loss: 344.1151712800936, KL divergence: 0.9786483576285443\n",
      "Reconstruction loss: 164.83959879816877, KL divergence: 0.018309212166189692\n",
      "Reconstruction loss: 360.9357590438178, KL divergence: 0.3238484348898595\n",
      "Reconstruction loss: 147.14659541385797, KL divergence: 0.018193499530943957\n",
      "Reconstruction loss: 151.32917267130182, KL divergence: 0.08231981951842149\n",
      "Reconstruction loss: 281.85841620700046, KL divergence: 0.4029296768325507\n",
      "Reconstruction loss: 199.39065947393993, KL divergence: 0.02741657759012389\n",
      "Reconstruction loss: 257.5164585135691, KL divergence: 0.1771828608653132\n",
      "Reconstruction loss: 205.8831271321343, KL divergence: 0.04869065325133165\n",
      "Reconstruction loss: 173.8969038517825, KL divergence: 0.010740171366123374\n",
      "Reconstruction loss: 267.0408420382207, KL divergence: 0.9373056853045516\n",
      "Reconstruction loss: 245.3381190062757, KL divergence: 0.30592529740622737\n",
      "Reconstruction loss: 324.8361598438729, KL divergence: 0.27721901588705716\n",
      "Reconstruction loss: 378.4601860206799, KL divergence: 0.511697123804247\n",
      "Reconstruction loss: 309.90795032078506, KL divergence: 0.10377609547868288\n",
      "Reconstruction loss: 405.8148485929957, KL divergence: 0.2317109231265933\n",
      "Reconstruction loss: 271.4101257334595, KL divergence: 0.2817730525241109\n",
      "Reconstruction loss: 185.36422480699446, KL divergence: 0.16231896073970048\n",
      "Reconstruction loss: 249.1898529380693, KL divergence: 0.04811822859719894\n",
      "Reconstruction loss: 161.70023433525304, KL divergence: 0.04718235086919981\n",
      "Reconstruction loss: 236.5912814737369, KL divergence: 0.059458972625043294\n",
      "Reconstruction loss: 269.04661342254326, KL divergence: 0.057950750068848866\n",
      "Reconstruction loss: 209.3431393249984, KL divergence: 0.1388090604740062\n",
      "Reconstruction loss: 285.41438906578867, KL divergence: 0.18654135662115884\n",
      "Reconstruction loss: 192.8988914703889, KL divergence: 0.1350320479214812\n",
      "Reconstruction loss: 165.35578308354616, KL divergence: 0.036808961698556686\n",
      "Reconstruction loss: 538.3224410161424, KL divergence: 0.4015950378032403\n",
      "Reconstruction loss: 269.2391421811899, KL divergence: 1.0305185567153325\n",
      "Reconstruction loss: 232.88574430083023, KL divergence: 0.10358229547035597\n",
      "Reconstruction loss: 423.24446436256943, KL divergence: 0.31167575028140837\n",
      "Reconstruction loss: 190.70800885371688, KL divergence: 0.20947722569251254\n",
      "Reconstruction loss: 609.7311733220706, KL divergence: 1.0027237585265119\n",
      "Reconstruction loss: 168.1171324732975, KL divergence: 0.0640087320967393\n",
      "Reconstruction loss: 302.78423204208025, KL divergence: 0.27784418908231845\n",
      "Reconstruction loss: 292.3930360458024, KL divergence: 0.25968579494614896\n",
      "Reconstruction loss: 326.760851793132, KL divergence: 0.38147453342271914\n",
      "Reconstruction loss: 364.7450257006083, KL divergence: 0.20225140874477265\n",
      "Reconstruction loss: 233.76325475816986, KL divergence: 0.0313384513932935\n",
      "Reconstruction loss: 334.98570245547904, KL divergence: 0.11851326710315307\n",
      "Reconstruction loss: 174.6305732413401, KL divergence: 0.1352585706442448\n",
      "Reconstruction loss: 177.1260329243932, KL divergence: 0.031733169665537464\n",
      "Reconstruction loss: 450.240170243529, KL divergence: 0.5041700596997751\n",
      "Reconstruction loss: 212.31573512523528, KL divergence: 0.12532318885208316\n",
      "Reconstruction loss: 342.78543324309277, KL divergence: 0.04121336297690731\n",
      "Reconstruction loss: 134.37550918318385, KL divergence: 0.0876559689163528\n",
      "Reconstruction loss: 223.85298870502157, KL divergence: 0.10947539620235791\n",
      "Reconstruction loss: 225.16385161152363, KL divergence: 0.12654304159579643\n",
      "Reconstruction loss: 338.2874480228553, KL divergence: 0.47484354265462553\n",
      "Reconstruction loss: 144.85500051239023, KL divergence: 0.020934622013570825\n",
      "Reconstruction loss: 164.0918379216545, KL divergence: 0.035621443087561155\n",
      "Reconstruction loss: 272.42870262140605, KL divergence: 0.49940393972731806\n",
      "Reconstruction loss: 169.21472400741888, KL divergence: 0.04570916291721028\n",
      "Reconstruction loss: 212.22948924407658, KL divergence: 0.370339034121583\n",
      "Reconstruction loss: 562.1154006463568, KL divergence: 1.416984605774164\n",
      "Reconstruction loss: 191.447889390349, KL divergence: 0.11269123215269483\n",
      "Reconstruction loss: 145.97025736308908, KL divergence: 0.053202814870835224\n",
      "Reconstruction loss: 262.9958305339897, KL divergence: 0.2871650640033836\n",
      "Reconstruction loss: 156.87353925207316, KL divergence: 0.041624890906463874\n",
      "Reconstruction loss: 271.5786525341895, KL divergence: 0.0992213191837158\n",
      "Reconstruction loss: 259.52226813429235, KL divergence: 0.3390865131685207\n",
      "Reconstruction loss: 332.71582229886735, KL divergence: 0.08429604086916614\n",
      "Reconstruction loss: 283.68542751454044, KL divergence: 0.5396603597238325\n",
      "Reconstruction loss: 232.67823564238438, KL divergence: 0.38017224819780243\n",
      "Reconstruction loss: 155.9364349483863, KL divergence: 0.10254715995283781\n",
      "Reconstruction loss: 283.46177111901386, KL divergence: 0.22048400877289986\n",
      "Reconstruction loss: 177.6407553033134, KL divergence: 0.022664005543898136\n",
      "Reconstruction loss: 238.62076698078312, KL divergence: 0.3174125658605568\n",
      "Reconstruction loss: 275.85775747029726, KL divergence: 0.2981514784525226\n",
      "Reconstruction loss: 186.63237830278857, KL divergence: 0.12984625775205444\n",
      "Reconstruction loss: 419.8591859440387, KL divergence: 0.23892403018731978\n",
      "Reconstruction loss: 217.68662509461498, KL divergence: 0.0753909805719935\n",
      "Reconstruction loss: 231.29049900238812, KL divergence: 0.2844627043219024\n",
      "Reconstruction loss: 164.4013055535221, KL divergence: 0.07738482083776743\n",
      "Reconstruction loss: 194.8122517872854, KL divergence: 0.03225961860393084\n",
      "Reconstruction loss: 358.2135786301087, KL divergence: 0.41164344200475433\n",
      "Reconstruction loss: 185.7942972993691, KL divergence: 0.041472398435238644\n",
      "Reconstruction loss: 312.19067144206286, KL divergence: 0.35530960901591196\n",
      "Reconstruction loss: 202.5071017849151, KL divergence: 0.18357234591302374\n",
      "Reconstruction loss: 192.84092587163332, KL divergence: 0.08728006039204128\n",
      "Reconstruction loss: 238.81058213910234, KL divergence: 0.08038096992998545\n",
      "Reconstruction loss: 401.9861691816805, KL divergence: 0.24107110781006308\n",
      "Reconstruction loss: 228.5928276822668, KL divergence: 0.12561284373314918\n",
      "Reconstruction loss: 276.15411078275827, KL divergence: 0.14809478278517185\n",
      "Reconstruction loss: 147.71495417122333, KL divergence: 0.0717336230242328\n",
      "Reconstruction loss: 198.52414149210276, KL divergence: 0.05630225833264091\n",
      "Reconstruction loss: 405.27062662712717, KL divergence: 0.4110365638733478\n",
      "Reconstruction loss: 169.27039630813618, KL divergence: 0.0837097339782728\n",
      "Reconstruction loss: 195.8892234609335, KL divergence: 0.11893239107597137\n",
      "Reconstruction loss: 171.7250264380249, KL divergence: 0.014329884309221663\n",
      "Reconstruction loss: 155.7750620760434, KL divergence: 0.11451467937915244\n",
      "Reconstruction loss: 172.7086166715839, KL divergence: 0.054635124922974065\n",
      "Reconstruction loss: 543.2214375413274, KL divergence: 0.5937483004659674\n",
      "Reconstruction loss: 346.11795801256244, KL divergence: 0.10743153964783747\n",
      "Reconstruction loss: 205.32291547111208, KL divergence: 0.08515228044128781\n",
      "Reconstruction loss: 189.09103910204192, KL divergence: 0.03694991451743307\n",
      "Reconstruction loss: 279.1205800064689, KL divergence: 0.25559422961214284\n",
      "Reconstruction loss: 305.5483076012994, KL divergence: 0.16226822235513583\n",
      "Reconstruction loss: 205.21747438220262, KL divergence: 0.08402392853031398\n",
      "Reconstruction loss: 171.28839879139485, KL divergence: 0.05118124019705217\n",
      "Reconstruction loss: 133.0183466978072, KL divergence: 0.026020651409262352\n",
      "Reconstruction loss: 310.30633805740626, KL divergence: 0.20123853930342334\n",
      "Reconstruction loss: 194.45625506407887, KL divergence: 0.09905798383633574\n",
      "Reconstruction loss: 190.97161660134364, KL divergence: 0.3096557418835125\n",
      "Reconstruction loss: 235.8747218895254, KL divergence: 0.19069656175316713\n",
      "Reconstruction loss: 377.85919442742517, KL divergence: 0.3665104563193386\n",
      "Reconstruction loss: 203.3370711700298, KL divergence: 0.14422941716102383\n",
      "Reconstruction loss: 243.7602056451834, KL divergence: 0.243811799685838\n",
      "Reconstruction loss: 303.71143968250783, KL divergence: 0.224801117258774\n",
      "Reconstruction loss: 195.58123196427684, KL divergence: 0.12583575333115704\n",
      "Reconstruction loss: 353.41532090847465, KL divergence: 0.39092455518850994\n",
      "Reconstruction loss: 287.3029238122997, KL divergence: 0.10891624436554143\n",
      "Reconstruction loss: 217.17470235587075, KL divergence: 0.035430287944118566\n",
      "Reconstruction loss: 318.7048849279363, KL divergence: 0.33555994701475517\n",
      "Reconstruction loss: 181.98723937313565, KL divergence: 0.07756681948067745\n",
      "Reconstruction loss: 403.481342130234, KL divergence: 0.13980550852052193\n",
      "Reconstruction loss: 286.197507655979, KL divergence: 0.2778401050557038\n",
      "Reconstruction loss: 144.74844361933938, KL divergence: 0.0946464132383521\n",
      "Reconstruction loss: 198.3702387064925, KL divergence: 0.04707455804217192\n",
      "Reconstruction loss: 223.52508117197775, KL divergence: 0.1669078881510555\n",
      "Reconstruction loss: 284.8425624368076, KL divergence: 0.10285059505431965\n",
      "Reconstruction loss: 187.3666165698755, KL divergence: 0.04695534712228133\n",
      "Reconstruction loss: 380.09838961475407, KL divergence: 0.23949598417264034\n",
      "Reconstruction loss: 166.81268327812688, KL divergence: 0.0371101519112062\n",
      "Reconstruction loss: 328.3964883627958, KL divergence: 0.26741832582790337\n",
      "Reconstruction loss: 370.6669510615849, KL divergence: 0.09939185688721924\n",
      "Reconstruction loss: 455.8186961163042, KL divergence: 0.15924406973902278\n",
      "Reconstruction loss: 337.93536369901807, KL divergence: 0.182885006963399\n",
      "Reconstruction loss: 204.4897054640296, KL divergence: 0.15895112669084688\n",
      "Reconstruction loss: 153.04971416297192, KL divergence: 0.032200133652283525\n",
      "Reconstruction loss: 409.1443818332778, KL divergence: 0.1174735672744085\n",
      "Reconstruction loss: 151.38490421211418, KL divergence: 0.04322145781790959\n",
      "Reconstruction loss: 188.7207663794511, KL divergence: 0.04261936726212823\n",
      "Reconstruction loss: 248.58675874475986, KL divergence: 0.10218365249476108\n",
      "Reconstruction loss: 200.04264306599043, KL divergence: 0.1344638161256118\n",
      "Reconstruction loss: 298.58702130521795, KL divergence: 0.06856471088069671\n",
      "Reconstruction loss: 262.84927417925394, KL divergence: 0.0860826358698511\n",
      "Reconstruction loss: 210.48493901724333, KL divergence: 0.0954134566058028\n",
      "Reconstruction loss: 277.4931565920887, KL divergence: 0.09397959968742842\n",
      "Reconstruction loss: 228.37791777396865, KL divergence: 0.0516464687060339\n",
      "Reconstruction loss: 196.19846746668435, KL divergence: 0.1043916853964183\n",
      "Reconstruction loss: 203.79441224942548, KL divergence: 0.09900232273214954\n",
      "Reconstruction loss: 238.95532735299201, KL divergence: 0.10441907896973607\n",
      "Reconstruction loss: 221.71568742615176, KL divergence: 0.24347629563939371\n",
      "Reconstruction loss: 393.97085579261517, KL divergence: 0.15602931033649658\n",
      "Reconstruction loss: 192.26378457876433, KL divergence: 0.3758534455913694\n",
      "Reconstruction loss: 400.47333021366796, KL divergence: 0.2780406675140165\n",
      "Reconstruction loss: 184.64004940846323, KL divergence: 0.13865328371794572\n",
      "Reconstruction loss: 325.41594802820305, KL divergence: 0.1434010582916324\n",
      "Reconstruction loss: 219.53323615967804, KL divergence: 0.07478970191487994\n",
      "Reconstruction loss: 222.848611197862, KL divergence: 0.2598465542961373\n",
      "Reconstruction loss: 162.97373677450804, KL divergence: 0.014193971588844112\n",
      "Reconstruction loss: 293.19682421594325, KL divergence: 0.3398909153995728\n",
      "Reconstruction loss: 437.57994587917545, KL divergence: 0.38721912211386794\n",
      "Reconstruction loss: 247.46655905924376, KL divergence: 0.1704315680306563\n",
      "Reconstruction loss: 149.04917913835473, KL divergence: 0.0396829866022394\n",
      "Reconstruction loss: 524.9006014634293, KL divergence: 0.3295748953155141\n",
      "Reconstruction loss: 257.4088080541847, KL divergence: 0.2427559601623583\n",
      "Reconstruction loss: 213.3698871212941, KL divergence: 0.4886684304180382\n",
      "Reconstruction loss: 209.4916152076948, KL divergence: 0.19027260333904256\n",
      "Reconstruction loss: 171.382239527689, KL divergence: 0.2575366805889246\n",
      "Reconstruction loss: 179.4253594130813, KL divergence: 0.04863804074389688\n",
      "Reconstruction loss: 344.64393339184556, KL divergence: 0.28663691793793233\n",
      "Reconstruction loss: 197.32100299869387, KL divergence: 0.04242717782325084\n",
      "Reconstruction loss: 237.2264834387426, KL divergence: 0.030816949214953093\n",
      "Reconstruction loss: 187.28896837904693, KL divergence: 0.4457080427817461\n",
      "Reconstruction loss: 274.30185505436907, KL divergence: 0.5170270179838966\n",
      "Reconstruction loss: 227.23921652728785, KL divergence: 0.29891555536242365\n",
      "Reconstruction loss: 226.95270010724272, KL divergence: 0.02304754841990303\n",
      "Reconstruction loss: 276.1642253317151, KL divergence: 0.05145714588509648\n",
      "Reconstruction loss: 460.6046229551073, KL divergence: 0.34281515791801226\n",
      "Reconstruction loss: 283.91952098356177, KL divergence: 0.06907065356508846\n",
      "Reconstruction loss: 243.8556560230296, KL divergence: 0.11266659524915257\n",
      "Reconstruction loss: 283.0833264110828, KL divergence: 0.21984490468508117\n",
      "Reconstruction loss: 200.9431638495509, KL divergence: 0.30197094455181683\n",
      "Reconstruction loss: 134.3612452455298, KL divergence: 0.12062492601937713\n",
      "Reconstruction loss: 168.06070331858095, KL divergence: 0.06740813611353064\n",
      "Reconstruction loss: 242.1526516728913, KL divergence: 0.5981894138914549\n",
      "Reconstruction loss: 154.64259444888754, KL divergence: 0.029330646630279733\n",
      "Reconstruction loss: 191.92805927518808, KL divergence: 0.04578425399458913\n",
      "Reconstruction loss: 159.10689387743736, KL divergence: 0.02488235226780311\n",
      "Reconstruction loss: 262.06129667366577, KL divergence: 0.4040020885820879\n",
      "Reconstruction loss: 315.4778323116415, KL divergence: 0.5089846063770589\n",
      "Reconstruction loss: 392.30685108266096, KL divergence: 0.06399330760038374\n",
      "Reconstruction loss: 422.2214786325895, KL divergence: 0.3505181429516665\n",
      "Reconstruction loss: 183.6258434479023, KL divergence: 0.06411608563864352\n",
      "Reconstruction loss: 308.438392800075, KL divergence: 0.6450936864206201\n",
      "Reconstruction loss: 196.092637445518, KL divergence: 0.08509482519583778\n",
      "Reconstruction loss: 240.16725065496763, KL divergence: 0.10813901473404375\n",
      "Reconstruction loss: 146.67610980388292, KL divergence: 0.03755844488669824\n",
      "Reconstruction loss: 175.90467177124583, KL divergence: 0.046098447932623854\n",
      "Reconstruction loss: 168.2855193653278, KL divergence: 0.16822013164658617\n",
      "Reconstruction loss: 177.53722179083667, KL divergence: 0.044527884112733174\n",
      "Reconstruction loss: 220.83695307275838, KL divergence: 0.21474433768104623\n",
      "Reconstruction loss: 189.15555544360458, KL divergence: 0.19379656008682633\n",
      "Reconstruction loss: 338.56145512010073, KL divergence: 0.02389672288682293\n",
      "Reconstruction loss: 290.8133529382743, KL divergence: 0.06206551356726947\n",
      "Reconstruction loss: 164.6221306822798, KL divergence: 0.12954546652170124\n",
      "Reconstruction loss: 256.08427919560523, KL divergence: 0.26035412419947196\n",
      "Reconstruction loss: 291.98034780497653, KL divergence: 0.40440049972713954\n",
      "Reconstruction loss: 235.50750593207675, KL divergence: 0.03329428358018954\n",
      "Reconstruction loss: 198.46701007640348, KL divergence: 0.03001452201869631\n",
      "Reconstruction loss: 154.18863480670385, KL divergence: 0.1054033310834484\n",
      "Reconstruction loss: 238.5737639154516, KL divergence: 0.05700083736637018\n",
      "Reconstruction loss: 229.66695875161642, KL divergence: 0.04847139660558103\n",
      "Reconstruction loss: 314.962817253624, KL divergence: 0.05302296200060541\n",
      "Reconstruction loss: 186.87466513834354, KL divergence: 0.08526536640605809\n",
      "Reconstruction loss: 218.41737075042278, KL divergence: 0.12835494086561888\n",
      "Reconstruction loss: 455.9183279659669, KL divergence: 0.8484398530073389\n",
      "Reconstruction loss: 245.07369240040146, KL divergence: 0.060568586800521074\n",
      "Reconstruction loss: 222.9257848159326, KL divergence: 0.05627708713440632\n",
      "Reconstruction loss: 231.31430851425677, KL divergence: 0.06407933919527303\n",
      "Reconstruction loss: 174.06164285798377, KL divergence: 0.24178320313964607\n",
      "Reconstruction loss: 245.91353339403554, KL divergence: 0.08306455146921632\n",
      "Reconstruction loss: 224.53605103764662, KL divergence: 0.047455268573097786\n",
      "Reconstruction loss: 372.03288147817153, KL divergence: 0.3588915242241591\n",
      "Reconstruction loss: 270.56185619759765, KL divergence: 0.06627907060280697\n",
      "Reconstruction loss: 248.27513259406146, KL divergence: 0.1586354412151907\n",
      "Reconstruction loss: 197.45418276249873, KL divergence: 0.435067878946011\n",
      "Reconstruction loss: 251.401680543937, KL divergence: 0.2126389745077577\n",
      "Reconstruction loss: 267.2043339302435, KL divergence: 0.14621928728396433\n",
      "Reconstruction loss: 333.3658093314026, KL divergence: 0.5263520718079221\n",
      "Reconstruction loss: 310.9552577942671, KL divergence: 0.1132721722944286\n",
      "Reconstruction loss: 257.57436049570447, KL divergence: 0.22078553910205873\n",
      "Reconstruction loss: 242.20147866188182, KL divergence: 0.13197281169674546\n",
      "Reconstruction loss: 233.71290518759946, KL divergence: 0.23701608137531183\n",
      "Reconstruction loss: 185.4238444735459, KL divergence: 0.04942193585141025\n",
      "Reconstruction loss: 330.3008075241778, KL divergence: 0.10090228811267438\n",
      "Reconstruction loss: 257.42039649641663, KL divergence: 0.018392892453435172\n",
      "Reconstruction loss: 449.64216646057537, KL divergence: 0.047565392891298186\n",
      "Reconstruction loss: 203.3117376447691, KL divergence: 0.08634585858125832\n",
      "Reconstruction loss: 222.03903563265325, KL divergence: 0.1010196657945367\n",
      "Reconstruction loss: 275.597846159371, KL divergence: 0.11585193175307557\n",
      "Reconstruction loss: 243.92793043350736, KL divergence: 0.5208495264153767\n",
      "Reconstruction loss: 213.33545688776343, KL divergence: 0.09362193762690707\n",
      "Reconstruction loss: 319.01534031956623, KL divergence: 0.17282394060533973\n",
      "Reconstruction loss: 183.37253041599132, KL divergence: 0.01972825507838527\n",
      "Reconstruction loss: 192.28450491710993, KL divergence: 0.03370761423806362\n",
      "Reconstruction loss: 333.98689379592895, KL divergence: 0.10611243066607262\n",
      "Reconstruction loss: 383.74636994425805, KL divergence: 0.31976256818518356\n",
      "Reconstruction loss: 249.63394985963038, KL divergence: 0.08197066732382852\n",
      "Reconstruction loss: 211.15313663097135, KL divergence: 0.13730374231708614\n",
      "Reconstruction loss: 188.7069470638703, KL divergence: 0.039564296265098275\n",
      "Reconstruction loss: 266.8522892683803, KL divergence: 0.14299240010856112\n",
      "Reconstruction loss: 227.68291569393642, KL divergence: 0.09618856973440576\n",
      "Reconstruction loss: 144.11591346618536, KL divergence: 0.14550279065742272\n",
      "Reconstruction loss: 293.3982678004411, KL divergence: 0.3149576542762637\n",
      "Reconstruction loss: 292.6526426456139, KL divergence: 0.31715606461523116\n",
      "Reconstruction loss: 244.5424955470666, KL divergence: 0.09013270063791584\n",
      "Reconstruction loss: 149.4125300166666, KL divergence: 0.12202017585017738\n",
      "Reconstruction loss: 507.65487737559033, KL divergence: 0.12554088030009258\n",
      "Reconstruction loss: 298.6032366068921, KL divergence: 0.0744679273780815\n",
      "Reconstruction loss: 166.21881984521914, KL divergence: 0.09049680266480953\n",
      "Reconstruction loss: 328.5984241146644, KL divergence: 0.14117181961868464\n",
      "Reconstruction loss: 156.74465927848865, KL divergence: 0.14435192736056446\n",
      "Reconstruction loss: 194.78359397584762, KL divergence: 0.0389161585396548\n",
      "Reconstruction loss: 382.43767156157, KL divergence: 0.34096551915292084\n",
      "Reconstruction loss: 433.83847945459905, KL divergence: 0.06896092198656667\n",
      "Reconstruction loss: 188.859380686814, KL divergence: 0.10841024434525814\n",
      "Reconstruction loss: 178.6526979579679, KL divergence: 0.22058281279381997\n",
      "Reconstruction loss: 460.63217427053945, KL divergence: 0.37344671395409457\n",
      "Reconstruction loss: 403.8124958758532, KL divergence: 0.07662575696855006\n",
      "Reconstruction loss: 193.81147745148064, KL divergence: 0.08877650545688531\n",
      "Reconstruction loss: 210.50650801210784, KL divergence: 0.13042222345746352\n",
      "Reconstruction loss: 299.8932903982899, KL divergence: 0.5646211906900673\n",
      "Reconstruction loss: 225.3704292770294, KL divergence: 0.038901202062735785\n",
      "Reconstruction loss: 196.575636472481, KL divergence: 0.10789203961779376\n",
      "Reconstruction loss: 170.62218271232825, KL divergence: 0.03421527198633678\n",
      "Reconstruction loss: 243.7281533208092, KL divergence: 0.2766395129020594\n",
      "Reconstruction loss: 134.077123427086, KL divergence: 0.024727019517902127\n",
      "Reconstruction loss: 160.64018257494087, KL divergence: 0.05739215409284254\n",
      "Reconstruction loss: 222.8087432186154, KL divergence: 0.12154726260167825\n",
      "Reconstruction loss: 166.3515547555304, KL divergence: 0.05459197496483087\n",
      "Reconstruction loss: 170.6351826362628, KL divergence: 0.04198220132431563\n",
      "Reconstruction loss: 206.14699764191937, KL divergence: 0.07585287480864145\n",
      "Reconstruction loss: 230.19246255503202, KL divergence: 0.35247819387593543\n",
      "Reconstruction loss: 230.89226923112795, KL divergence: 0.20865875512336057\n",
      "Reconstruction loss: 180.44768916256893, KL divergence: 0.1446291605520416\n",
      "Reconstruction loss: 351.6417470294501, KL divergence: 0.04906322807466584\n",
      "Reconstruction loss: 393.52920296496853, KL divergence: 0.32160415113528723\n",
      "Reconstruction loss: 178.28779310840514, KL divergence: 0.06403996773479587\n",
      "Reconstruction loss: 239.59033977632873, KL divergence: 0.12104274923602676\n",
      "Reconstruction loss: 159.17012415375024, KL divergence: 0.15706194739954282\n",
      "Reconstruction loss: 319.204872661694, KL divergence: 0.10081208012176446\n",
      "Reconstruction loss: 234.32968972409486, KL divergence: 0.052647881066267666\n",
      "Reconstruction loss: 168.15984835016127, KL divergence: 0.15661265885281767\n",
      "Reconstruction loss: 206.96120608392465, KL divergence: 0.038996575613689466\n",
      "Reconstruction loss: 299.00878722119717, KL divergence: 0.42957166356772375\n",
      "Reconstruction loss: 139.26795263231176, KL divergence: 0.01625262103676295\n",
      "Reconstruction loss: 286.83005904437795, KL divergence: 0.6601170698943785\n",
      "Reconstruction loss: 303.10889836566815, KL divergence: 0.4457188822157471\n",
      "Reconstruction loss: 182.83245340819315, KL divergence: 0.023802412129839468\n",
      "Reconstruction loss: 343.8721843279855, KL divergence: 0.05620473560500028\n",
      "Reconstruction loss: 343.17924007233614, KL divergence: 0.19382374383848255\n",
      "Reconstruction loss: 243.3413696687095, KL divergence: 0.025304680957798475\n",
      "Reconstruction loss: 330.53907358917843, KL divergence: 0.6657638899084257\n",
      "Reconstruction loss: 145.59728106016513, KL divergence: 0.07602913007094231\n",
      "Reconstruction loss: 261.03916715702843, KL divergence: 0.4381486545801642\n",
      "Reconstruction loss: 207.21602733481467, KL divergence: 0.09877673995007819\n",
      "Reconstruction loss: 431.2020171128801, KL divergence: 0.4509314991784095\n",
      "Reconstruction loss: 242.54995124394944, KL divergence: 0.27307188029076257\n",
      "Reconstruction loss: 152.38308806381895, KL divergence: 0.016667615726201235\n",
      "Reconstruction loss: 155.31463231939898, KL divergence: 0.0646402411158285\n",
      "Reconstruction loss: 135.4789408431837, KL divergence: 0.08200545469456588\n",
      "Reconstruction loss: 323.98321377227745, KL divergence: 0.055206070571831134\n",
      "Reconstruction loss: 208.71974528301166, KL divergence: 0.08739853061235558\n",
      "Reconstruction loss: 205.5100897318231, KL divergence: 0.26210776372283845\n",
      "Reconstruction loss: 203.28458609274222, KL divergence: 0.07716376420598542\n",
      "Reconstruction loss: 147.87077791264457, KL divergence: 0.04671252194768133\n",
      "Reconstruction loss: 186.86919083233582, KL divergence: 0.10374778344514951\n",
      "Reconstruction loss: 141.90379291963876, KL divergence: 0.0542591694083458\n",
      "Reconstruction loss: 274.02368961920035, KL divergence: 0.5265256071627911\n",
      "Reconstruction loss: 190.4419891909087, KL divergence: 0.20573693178705565\n",
      "Reconstruction loss: 232.95221278789086, KL divergence: 0.018947342906297182\n",
      "Reconstruction loss: 194.26001517480722, KL divergence: 0.0296076271616722\n",
      "Reconstruction loss: 399.7623107935457, KL divergence: 0.15688400083312642\n",
      "Reconstruction loss: 314.64144286470315, KL divergence: 0.0758942097700836\n",
      "Reconstruction loss: 189.51415373169877, KL divergence: 0.07373101282511157\n",
      "Reconstruction loss: 197.78111151126407, KL divergence: 0.03404926604839209\n",
      "Reconstruction loss: 351.3420223844793, KL divergence: 0.21773383055404488\n",
      "Reconstruction loss: 327.267524440729, KL divergence: 0.1590670727098496\n",
      "Reconstruction loss: 208.8395458123045, KL divergence: 0.13907495534959502\n",
      "Reconstruction loss: 259.91989697746317, KL divergence: 0.3041205583353803\n",
      "Reconstruction loss: 319.02244842207074, KL divergence: 0.06947222309269246\n",
      "Reconstruction loss: 172.0417165976128, KL divergence: 0.04773848418714216\n",
      "Reconstruction loss: 146.93221477872333, KL divergence: 0.07755115687150599\n",
      "Reconstruction loss: 256.9488374179058, KL divergence: 0.684996540602442\n",
      "Reconstruction loss: 206.52521979362126, KL divergence: 0.2987143169669341\n",
      "Reconstruction loss: 301.9136138361421, KL divergence: 0.23020313569380157\n",
      "Reconstruction loss: 335.91197862120123, KL divergence: 0.2130658634478309\n",
      "Reconstruction loss: 181.465998354595, KL divergence: 0.039258641616306655\n",
      "Reconstruction loss: 411.0936003204672, KL divergence: 0.34926575483783345\n",
      "Reconstruction loss: 200.6691362170805, KL divergence: 0.12951953884254264\n",
      "Reconstruction loss: 189.87918457565485, KL divergence: 0.030394959762151452\n",
      "Reconstruction loss: 238.25198594810593, KL divergence: 0.1776932390736201\n",
      "Reconstruction loss: 407.14205371325187, KL divergence: 0.5700716905899079\n",
      "Reconstruction loss: 206.55649131896342, KL divergence: 0.40071303524845886\n",
      "Reconstruction loss: 533.0366180961772, KL divergence: 0.3694119141767577\n",
      "Reconstruction loss: 170.4959229195549, KL divergence: 0.03141662799486622\n",
      "Reconstruction loss: 223.07305837379698, KL divergence: 0.1960541271634288\n",
      "Reconstruction loss: 181.30830946537833, KL divergence: 0.040696809941755785\n",
      "Reconstruction loss: 333.35924795235724, KL divergence: 0.20016674825884873\n",
      "Reconstruction loss: 246.13068135583958, KL divergence: 0.07553705419189594\n",
      "Reconstruction loss: 330.4451218269691, KL divergence: 0.09379692978950888\n",
      "Reconstruction loss: 249.47217570594776, KL divergence: 0.045992685298442304\n",
      "Reconstruction loss: 279.014760529578, KL divergence: 0.07812846217466762\n",
      "Reconstruction loss: 343.3768478304013, KL divergence: 0.5139043426613052\n",
      "Reconstruction loss: 444.3957361760011, KL divergence: 0.6120170126949074\n",
      "Reconstruction loss: 168.32340741833025, KL divergence: 0.06435068847351244\n",
      "Reconstruction loss: 281.7674710290808, KL divergence: 0.1585725940645527\n",
      "Reconstruction loss: 127.8849002102607, KL divergence: 0.035668952756270966\n",
      "Reconstruction loss: 200.7696640937336, KL divergence: 0.1150227470667588\n",
      "Reconstruction loss: 199.8185338025752, KL divergence: 0.33794985109043835\n",
      "Reconstruction loss: 195.2561630961728, KL divergence: 0.47409662942045755\n",
      "Reconstruction loss: 233.05546275153765, KL divergence: 0.23668791516723303\n",
      "Reconstruction loss: 379.63133528492335, KL divergence: 0.22155664345341652\n",
      "Reconstruction loss: 200.02304008463378, KL divergence: 0.03592311841490953\n",
      "Reconstruction loss: 133.94542082131701, KL divergence: 0.01748235187879732\n",
      "Reconstruction loss: 174.86675658742956, KL divergence: 0.1422576369026211\n",
      "Reconstruction loss: 280.8219599453164, KL divergence: 0.2141390431469436\n",
      "Reconstruction loss: 185.05799142501328, KL divergence: 0.08792138397347832\n",
      "Reconstruction loss: 226.92789477513566, KL divergence: 0.04569898318875387\n",
      "Reconstruction loss: 200.0960300746541, KL divergence: 0.07681781728238829\n",
      "Reconstruction loss: 181.4653240294632, KL divergence: 0.2481757928997193\n",
      "Reconstruction loss: 272.1251886938615, KL divergence: 0.27587602995764976\n",
      "Reconstruction loss: 192.35451135869516, KL divergence: 0.15225851094143739\n",
      "Reconstruction loss: 291.7697566579336, KL divergence: 0.13130516677146797\n",
      "Reconstruction loss: 340.9355949566839, KL divergence: 0.9258593040258216\n",
      "Reconstruction loss: 223.45936874225362, KL divergence: 0.22956773942490427\n",
      "Reconstruction loss: 190.92271302876821, KL divergence: 0.027581194881597715\n",
      "Reconstruction loss: 193.9056821326829, KL divergence: 0.5156265901420736\n",
      "Reconstruction loss: 247.82611829152677, KL divergence: 0.5620868260578056\n",
      "Reconstruction loss: 165.1128866249418, KL divergence: 0.039476045956510664\n",
      "Reconstruction loss: 210.74085548394345, KL divergence: 0.27167838120746424\n",
      "Reconstruction loss: 166.19825046317618, KL divergence: 0.036024881153616684\n",
      "Reconstruction loss: 397.7872892518013, KL divergence: 0.537580106289068\n",
      "Reconstruction loss: 234.60416313611603, KL divergence: 0.4439406771482457\n",
      "Reconstruction loss: 474.9763110793813, KL divergence: 0.6322200092595724\n",
      "Reconstruction loss: 214.72751760117404, KL divergence: 0.5228584293910834\n",
      "Reconstruction loss: 270.6182390486183, KL divergence: 0.17409001562072618\n",
      "Reconstruction loss: 145.04673009439895, KL divergence: 0.0537804802771511\n",
      "Reconstruction loss: 242.52904153676712, KL divergence: 0.031680761687976344\n",
      "Reconstruction loss: 160.40401428580876, KL divergence: 0.12260880559426113\n",
      "Reconstruction loss: 234.43973788606615, KL divergence: 0.277833804216389\n",
      "Reconstruction loss: 124.27836232717968, KL divergence: 0.04079553068816172\n",
      "Reconstruction loss: 252.71078043882147, KL divergence: 0.252814692094846\n",
      "Reconstruction loss: 296.1993560095309, KL divergence: 0.8625781289124225\n",
      "Reconstruction loss: 211.98154108212185, KL divergence: 0.0944997028496477\n",
      "Reconstruction loss: 174.55305980063082, KL divergence: 0.1362627724125436\n",
      "Reconstruction loss: 288.13049428647577, KL divergence: 0.054485166655064976\n",
      "Reconstruction loss: 235.28206623138212, KL divergence: 0.3795238300781539\n",
      "Reconstruction loss: 312.4184052019796, KL divergence: 0.2588213835557859\n",
      "Reconstruction loss: 144.67956181375183, KL divergence: 0.0409120399551976\n",
      "Reconstruction loss: 265.95793895020205, KL divergence: 0.02085930311929385\n",
      "Reconstruction loss: 179.23084712623938, KL divergence: 0.141652900896646\n",
      "Reconstruction loss: 323.6237034787736, KL divergence: 0.5246180964482023\n",
      "Reconstruction loss: 133.4204456556817, KL divergence: 0.10813982078234274\n",
      "Reconstruction loss: 173.53235745175857, KL divergence: 0.05924729825720393\n",
      "Reconstruction loss: 205.93542795129537, KL divergence: 0.09380237149650456\n",
      "Reconstruction loss: 284.82442087509537, KL divergence: 0.13840771247075817\n",
      "Reconstruction loss: 194.58191824668515, KL divergence: 0.14450778258744007\n",
      "Reconstruction loss: 277.8674025791899, KL divergence: 0.0893281477788036\n",
      "Reconstruction loss: 211.50576612076705, KL divergence: 0.6149523438902535\n",
      "Reconstruction loss: 236.56418187815177, KL divergence: 0.08768192343231568\n",
      "Reconstruction loss: 279.71557397066005, KL divergence: 0.2586030404674594\n",
      "Reconstruction loss: 315.2202340511507, KL divergence: 0.30607012305778825\n",
      "Reconstruction loss: 151.41547768049207, KL divergence: 0.024962020076566116\n",
      "Reconstruction loss: 282.4903818913667, KL divergence: 0.06339298666075421\n",
      "Reconstruction loss: 280.31757678677195, KL divergence: 0.24080941340062245\n",
      "Reconstruction loss: 128.2963789975572, KL divergence: 0.05839776024510018\n",
      "Reconstruction loss: 177.1228410527869, KL divergence: 0.028284607543281826\n",
      "Reconstruction loss: 222.19839166444842, KL divergence: 0.07530997173175741\n",
      "Reconstruction loss: 241.04928275313495, KL divergence: 0.12203046270825679\n",
      "Reconstruction loss: 187.96265438396142, KL divergence: 0.5707156691513313\n",
      "Reconstruction loss: 285.2116190612843, KL divergence: 0.10364019070326325\n",
      "Reconstruction loss: 304.0358386526303, KL divergence: 0.5071598734868246\n",
      "Reconstruction loss: 208.4190821136866, KL divergence: 0.2340783890312897\n",
      "Reconstruction loss: 155.14545139601967, KL divergence: 0.11654435222658777\n",
      "Reconstruction loss: 170.90012576262802, KL divergence: 0.11571552625829329\n",
      "Reconstruction loss: 161.59679222871492, KL divergence: 0.0651186664880769\n",
      "Reconstruction loss: 149.53643778465232, KL divergence: 0.030687679742323504\n",
      "Reconstruction loss: 161.80261692550334, KL divergence: 0.06660074921662157\n",
      "Reconstruction loss: 174.59775387833452, KL divergence: 0.09164396287889465\n",
      "Reconstruction loss: 448.9849970083672, KL divergence: 0.5265241924933686\n",
      "Reconstruction loss: 124.64497028869467, KL divergence: 0.024868825691317376\n",
      "Reconstruction loss: 206.71109190243328, KL divergence: 0.034029496362842915\n",
      "Reconstruction loss: 182.83905731373142, KL divergence: 0.18325306699146982\n",
      "Reconstruction loss: 180.8571648627322, KL divergence: 0.04753755757172501\n",
      "Reconstruction loss: 210.63873754969143, KL divergence: 0.07250786619961824\n",
      "Reconstruction loss: 245.16215609592675, KL divergence: 0.1679127553817023\n",
      "Reconstruction loss: 160.50946498656037, KL divergence: 0.0575508591751161\n",
      "Reconstruction loss: 154.97652382706008, KL divergence: 0.027422411594926366\n",
      "Reconstruction loss: 281.2451069728619, KL divergence: 0.6798172492900516\n",
      "Reconstruction loss: 170.49573868939456, KL divergence: 0.03220354357314459\n",
      "Reconstruction loss: 152.67825454143536, KL divergence: 0.05420958918968577\n",
      "Reconstruction loss: 284.3187335132557, KL divergence: 0.707900921931895\n",
      "Reconstruction loss: 435.17785007172057, KL divergence: 0.7484106243592054\n",
      "Reconstruction loss: 278.4594851627409, KL divergence: 0.41997023354448704\n",
      "Reconstruction loss: 204.0224517730628, KL divergence: 0.4344712882191754\n",
      "Reconstruction loss: 195.67514918668428, KL divergence: 0.17004637578164794\n",
      "Reconstruction loss: 146.7226075135588, KL divergence: 0.01944908593484762\n",
      "Reconstruction loss: 142.5145525312904, KL divergence: 0.05135790979561489\n",
      "Reconstruction loss: 354.52742155116357, KL divergence: 0.4393458935570102\n",
      "Reconstruction loss: 209.55436718829986, KL divergence: 0.1458030302025402\n",
      "Reconstruction loss: 193.049163057435, KL divergence: 0.16385971842988462\n",
      "Reconstruction loss: 162.87834458913403, KL divergence: 0.045292097078412086\n",
      "Reconstruction loss: 177.850734362751, KL divergence: 0.08901362953668818\n",
      "Reconstruction loss: 273.6617997490064, KL divergence: 0.13141001370490052\n",
      "Reconstruction loss: 166.9517327282886, KL divergence: 0.1540392325181001\n",
      "Reconstruction loss: 263.8840175386669, KL divergence: 0.2967232271293118\n",
      "Reconstruction loss: 220.5826559639824, KL divergence: 0.10078917774947665\n",
      "Reconstruction loss: 211.42432103471128, KL divergence: 0.08052666238165762\n",
      "Reconstruction loss: 212.14974013046776, KL divergence: 0.3160050720136319\n",
      "Reconstruction loss: 335.44909125693675, KL divergence: 0.23875424265325812\n",
      "Reconstruction loss: 206.08116425776478, KL divergence: 0.05586867077581548\n",
      "Reconstruction loss: 210.87317188257003, KL divergence: 0.04443062027895367\n",
      "Reconstruction loss: 359.3477106165363, KL divergence: 0.22670899172667303\n",
      "Reconstruction loss: 174.1799685262986, KL divergence: 0.04095264854714381\n",
      "Reconstruction loss: 124.53479992242092, KL divergence: 0.052614845073432726\n",
      "Reconstruction loss: 207.37458020268326, KL divergence: 0.11681820292253375\n",
      "Reconstruction loss: 299.6182319797789, KL divergence: 0.48620050931259157\n",
      "Reconstruction loss: 150.63573773642833, KL divergence: 0.03447748825454233\n",
      "Reconstruction loss: 258.59154606850774, KL divergence: 0.18190653826687186\n",
      "Reconstruction loss: 282.22189375010004, KL divergence: 0.25544545888802295\n",
      "Reconstruction loss: 229.90211002368068, KL divergence: 0.37878157332641205\n",
      "Reconstruction loss: 322.32675244976735, KL divergence: 0.2789935587526127\n",
      "Reconstruction loss: 152.9234532942705, KL divergence: 0.06325555296133994\n",
      "Reconstruction loss: 192.39555197815355, KL divergence: 0.05972064608433669\n",
      "Reconstruction loss: 208.49202511834864, KL divergence: 0.06435923756385409\n",
      "Reconstruction loss: 172.1441463280698, KL divergence: 0.020043575702188954\n",
      "Reconstruction loss: 207.49339553523504, KL divergence: 0.09483059962790119\n",
      "Reconstruction loss: 177.2751536542238, KL divergence: 0.20930678970245742\n",
      "Reconstruction loss: 134.14729737920908, KL divergence: 0.0684409303337799\n",
      "Reconstruction loss: 224.74221482453044, KL divergence: 0.5534597246142101\n",
      "Reconstruction loss: 215.99683103927885, KL divergence: 0.11802488340099626\n",
      "Reconstruction loss: 227.76717844502372, KL divergence: 0.09145662158470752\n",
      "Reconstruction loss: 200.61131742063662, KL divergence: 0.024373745256396018\n",
      "Reconstruction loss: 289.6916525057935, KL divergence: 0.5170985064503337\n",
      "Reconstruction loss: 178.37870631742908, KL divergence: 0.03517117477719295\n",
      "Reconstruction loss: 260.2240880965957, KL divergence: 0.5170099581663414\n",
      "Reconstruction loss: 202.29035158372477, KL divergence: 0.05471257559992454\n",
      "Reconstruction loss: 201.96891098128583, KL divergence: 0.021575654143482725\n",
      "Reconstruction loss: 230.42253774308944, KL divergence: 0.048790030372253324\n",
      "Reconstruction loss: 169.81139660790973, KL divergence: 0.03496395537878927\n",
      "Reconstruction loss: 219.5667610434857, KL divergence: 0.22091045587721708\n",
      "Reconstruction loss: 295.06931070518044, KL divergence: 0.3907967062881117\n",
      "Reconstruction loss: 357.32314275387216, KL divergence: 0.18870739887589372\n",
      "Reconstruction loss: 215.11038210230407, KL divergence: 0.1264490865429418\n",
      "Reconstruction loss: 299.58083721912396, KL divergence: 0.38962485634791033\n",
      "Reconstruction loss: 207.73350732283814, KL divergence: 0.039938927956727766\n",
      "Reconstruction loss: 237.08875433334873, KL divergence: 0.027819850341313568\n",
      "Reconstruction loss: 128.13259923341354, KL divergence: 0.06141776332403737\n",
      "Reconstruction loss: 181.87623215221907, KL divergence: 0.045855643400191215\n",
      "Reconstruction loss: 177.1280415632922, KL divergence: 0.05835239463256536\n",
      "Reconstruction loss: 201.02493047090434, KL divergence: 0.2554531401156833\n",
      "Reconstruction loss: 181.97345043964316, KL divergence: 0.038701224309581395\n",
      "Reconstruction loss: 210.91784003648735, KL divergence: 0.040045200831848926\n",
      "Reconstruction loss: 312.9547911856913, KL divergence: 0.11606632169642273\n",
      "Reconstruction loss: 167.27759728603775, KL divergence: 0.03736005920169083\n",
      "Reconstruction loss: 183.06776095753418, KL divergence: 0.05701637595591402\n",
      "Reconstruction loss: 309.9712033088216, KL divergence: 0.2330631321820747\n",
      "Reconstruction loss: 124.93618830184104, KL divergence: 0.06136087099361204\n",
      "Reconstruction loss: 327.46662883719796, KL divergence: 0.10911871705713633\n",
      "Reconstruction loss: 180.61698830468612, KL divergence: 0.38173490163329715\n",
      "Reconstruction loss: 212.87400279000826, KL divergence: 0.10757026968619299\n",
      "Reconstruction loss: 268.7445954278886, KL divergence: 0.1613450312111661\n",
      "Reconstruction loss: 196.40980157544584, KL divergence: 0.03826777410644744\n",
      "Reconstruction loss: 228.2549411972936, KL divergence: 0.4810158602048458\n",
      "Reconstruction loss: 209.73985460986444, KL divergence: 0.11427315850515846\n",
      "Reconstruction loss: 217.32452581570493, KL divergence: 0.04102439707326233\n",
      "Reconstruction loss: 174.4278892933238, KL divergence: 0.03122957550618849\n",
      "Reconstruction loss: 128.8148162168384, KL divergence: 0.04168260518276756\n",
      "Reconstruction loss: 203.14392445682466, KL divergence: 0.02059215811987336\n",
      "Reconstruction loss: 123.65941137347689, KL divergence: 0.03435399868464978\n",
      "Reconstruction loss: 249.17261328585636, KL divergence: 0.16780876547865897\n",
      "Reconstruction loss: 412.3872767873391, KL divergence: 0.8419397017306354\n",
      "Reconstruction loss: 172.8452901782287, KL divergence: 0.033952393643947865\n",
      "Reconstruction loss: 183.81152685575304, KL divergence: 0.052412049203111266\n",
      "Reconstruction loss: 223.9677074205967, KL divergence: 0.08810812534974383\n",
      "Reconstruction loss: 256.0908036842138, KL divergence: 0.22377961651080502\n",
      "Reconstruction loss: 232.88473226713768, KL divergence: 0.07748699893096878\n",
      "Reconstruction loss: 147.103427680268, KL divergence: 0.04567082461561761\n",
      "Reconstruction loss: 178.592439306396, KL divergence: 0.05659213498296378\n",
      "Reconstruction loss: 393.3678516232089, KL divergence: 0.7038418784318925\n",
      "Reconstruction loss: 284.77062665447875, KL divergence: 0.11059157253984325\n",
      "Reconstruction loss: 189.2271344198142, KL divergence: 0.2263289112103003\n",
      "Reconstruction loss: 190.88660059280116, KL divergence: 0.16644606164805864\n",
      "Reconstruction loss: 194.6933234768547, KL divergence: 0.0228791207568928\n",
      "Reconstruction loss: 242.72919080890875, KL divergence: 0.05050644154503642\n",
      "Reconstruction loss: 266.09801973883623, KL divergence: 0.06600754030816347\n",
      "Reconstruction loss: 231.31941698265388, KL divergence: 0.09634507779973167\n",
      "Reconstruction loss: 162.9388067043945, KL divergence: 0.023278032505228752\n",
      "Reconstruction loss: 125.70799558541722, KL divergence: 0.06145407139513798\n",
      "Reconstruction loss: 185.94020217842996, KL divergence: 0.08584060951506506\n",
      "Reconstruction loss: 143.74982431871564, KL divergence: 0.04357118639538271\n",
      "Reconstruction loss: 213.40373854324412, KL divergence: 0.22938161749142572\n",
      "Reconstruction loss: 260.67667000092285, KL divergence: 0.24234946187361328\n",
      "Reconstruction loss: 218.23138687177408, KL divergence: 0.19830585337460827\n",
      "Reconstruction loss: 258.0186123751869, KL divergence: 0.021095469755670504\n",
      "Reconstruction loss: 205.8559948010561, KL divergence: 0.039734841134375976\n",
      "Reconstruction loss: 207.9403688440745, KL divergence: 0.025752691807931494\n",
      "Reconstruction loss: 222.80629545848763, KL divergence: 0.062179272895023674\n",
      "Reconstruction loss: 278.18922312381704, KL divergence: 0.3647060888583658\n",
      "Reconstruction loss: 209.1492481812839, KL divergence: 0.06748367922045456\n",
      "Reconstruction loss: 335.9820127586192, KL divergence: 0.6579979581137758\n",
      "Reconstruction loss: 359.3240769128747, KL divergence: 0.2485406475803802\n",
      "Reconstruction loss: 223.74192688263827, KL divergence: 0.1511902633520103\n",
      "Reconstruction loss: 329.47163423087653, KL divergence: 0.16828482813573048\n",
      "Reconstruction loss: 192.257690705733, KL divergence: 0.0996342269406919\n",
      "Reconstruction loss: 247.4092771077091, KL divergence: 0.17147063810301022\n",
      "Reconstruction loss: 320.3230549045139, KL divergence: 0.29839627241870764\n",
      "Reconstruction loss: 320.95039498606286, KL divergence: 0.1452125706787768\n",
      "Reconstruction loss: 276.8058651213456, KL divergence: 0.021581897291740593\n",
      "Reconstruction loss: 281.63614359396433, KL divergence: 0.05817108736962945\n",
      "Reconstruction loss: 213.62478197497515, KL divergence: 0.0275480656225634\n",
      "Reconstruction loss: 165.56854418291232, KL divergence: 0.07655775745388443\n",
      "Reconstruction loss: 240.94704721653392, KL divergence: 0.1478090360002033\n",
      "Reconstruction loss: 243.30813974324926, KL divergence: 0.05349296845141899\n",
      "Reconstruction loss: 289.25420185342807, KL divergence: 0.1183706216173277\n",
      "Reconstruction loss: 308.43870653250326, KL divergence: 0.4760486999996606\n",
      "Reconstruction loss: 250.27441283036026, KL divergence: 0.08227625308211939\n",
      "Reconstruction loss: 285.354899964722, KL divergence: 0.022862163904321553\n",
      "Reconstruction loss: 198.65934143623596, KL divergence: 0.11457328518758958\n",
      "Reconstruction loss: 311.47285080573647, KL divergence: 0.07184178099216432\n",
      "Reconstruction loss: 186.60561811407985, KL divergence: 0.022097621402038303\n",
      "Reconstruction loss: 236.54400534845112, KL divergence: 0.022972744828274105\n",
      "Reconstruction loss: 330.808045768164, KL divergence: 0.3631208869981253\n",
      "Reconstruction loss: 159.21357112278525, KL divergence: 0.021581897291740593\n",
      "Reconstruction loss: 182.93964319766337, KL divergence: 0.026531172625623656\n",
      "Reconstruction loss: 404.262731959814, KL divergence: 0.3029832375959751\n",
      "Reconstruction loss: 151.6912913825597, KL divergence: 0.09499075482101837\n",
      "Reconstruction loss: 224.46502874156118, KL divergence: 0.05740712807569437\n",
      "Reconstruction loss: 234.19322007918998, KL divergence: 0.08636291556119274\n",
      "Reconstruction loss: 293.53386877067726, KL divergence: 0.6297828889643027\n",
      "Reconstruction loss: 205.47735009784202, KL divergence: 0.10975092394653607\n",
      "Reconstruction loss: 163.53442062297978, KL divergence: 0.05599344462376288\n",
      "Reconstruction loss: 210.86386424469202, KL divergence: 0.08985675273214666\n",
      "Reconstruction loss: 255.11227035630048, KL divergence: 0.18298416080289132\n",
      "Reconstruction loss: 248.89089222214983, KL divergence: 0.2696578758546238\n",
      "Reconstruction loss: 198.47462496633113, KL divergence: 0.03028207244133374\n",
      "Reconstruction loss: 282.3375163918591, KL divergence: 0.10291948881120228\n",
      "Reconstruction loss: 133.7732868671158, KL divergence: 0.030072925494627756\n",
      "Reconstruction loss: 159.56755938118005, KL divergence: 0.022098350249171284\n",
      "Reconstruction loss: 244.76814245546205, KL divergence: 0.07866955413918031\n",
      "Reconstruction loss: 347.8854651341232, KL divergence: 0.08932054992423644\n",
      "Reconstruction loss: 169.18777335957333, KL divergence: 0.03904142392706805\n",
      "Reconstruction loss: 170.43007958126157, KL divergence: 0.059882752600066425\n",
      "Reconstruction loss: 200.0522256660494, KL divergence: 0.02864830047766398\n",
      "Reconstruction loss: 310.5021010591044, KL divergence: 0.33476304640513793\n",
      "Reconstruction loss: 215.99592879535777, KL divergence: 0.10641290898632622\n",
      "Reconstruction loss: 244.6652642213896, KL divergence: 0.06212714165252364\n",
      "Reconstruction loss: 220.02764565820922, KL divergence: 0.027407499355678044\n",
      "Reconstruction loss: 235.42640761273142, KL divergence: 0.1834043216054415\n",
      "Reconstruction loss: 247.09114680465277, KL divergence: 0.038368882241036795\n",
      "Reconstruction loss: 209.91061643158866, KL divergence: 0.022098350249171284\n",
      "Reconstruction loss: 227.79304295903876, KL divergence: 0.03589766266130906\n",
      "Reconstruction loss: 213.66748383219698, KL divergence: 0.04720807508853142\n",
      "Reconstruction loss: 156.4322310704459, KL divergence: 0.022098350249171284\n",
      "Reconstruction loss: 278.16482954112075, KL divergence: 0.27923243751657345\n",
      "Reconstruction loss: 249.56285864477104, KL divergence: 0.40737156340001723\n",
      "Reconstruction loss: 157.23740726905191, KL divergence: 0.022098350249171284\n",
      "Reconstruction loss: 241.19969297726425, KL divergence: 0.022098350249171284\n",
      "Reconstruction loss: 212.90728563041952, KL divergence: 0.13664963286535375\n",
      "Reconstruction loss: 194.5245428961286, KL divergence: 0.022098350249171284\n",
      "Reconstruction loss: 158.98204488318584, KL divergence: 0.026410021353217072\n",
      "Reconstruction loss: 192.3004543257262, KL divergence: 0.0982450983821258\n",
      "Reconstruction loss: 270.4974536077866, KL divergence: 0.05778729936732768\n",
      "Reconstruction loss: 357.83592728253006, KL divergence: 0.5304889719741321\n",
      "Reconstruction loss: 207.6120270261052, KL divergence: 0.07066627980549822\n",
      "Reconstruction loss: 190.9864939814909, KL divergence: 0.0887935814673278\n",
      "Reconstruction loss: 211.69652366734474, KL divergence: 0.23495440496985154\n",
      "Reconstruction loss: 268.9613708112115, KL divergence: 0.23187400624447818\n",
      "Reconstruction loss: 157.48242568848553, KL divergence: 0.03004820796075619\n",
      "Reconstruction loss: 126.65015465815588, KL divergence: 0.022098350249171284\n",
      "Reconstruction loss: 192.50740225356793, KL divergence: 0.1195724210079665\n",
      "Reconstruction loss: 241.78256710533958, KL divergence: 0.35186384167803625\n",
      "Reconstruction loss: 158.02602171814084, KL divergence: 0.06081886167357059\n",
      "Reconstruction loss: 196.30095446468113, KL divergence: 0.03522647117001293\n",
      "Reconstruction loss: 144.99277358315265, KL divergence: 0.04208632905245846\n",
      "Reconstruction loss: 194.5059721433243, KL divergence: 0.058142699873451775\n",
      "Reconstruction loss: 307.57557607574324, KL divergence: 0.18273150090207768\n",
      "Reconstruction loss: 235.7614548352354, KL divergence: 0.039326585164555616\n",
      "Reconstruction loss: 270.1807599745098, KL divergence: 0.027122563892175933\n",
      "Reconstruction loss: 219.21762138781583, KL divergence: 0.34449647311711123\n",
      "Reconstruction loss: 267.5431951070606, KL divergence: 0.7281936259896487\n",
      "Reconstruction loss: 291.2009316317324, KL divergence: 0.3795083947900066\n",
      "Reconstruction loss: 221.378182072923, KL divergence: 0.03441574832829408\n",
      "Reconstruction loss: 310.1433737049932, KL divergence: 0.10202849204692982\n",
      "Reconstruction loss: 344.8686494053527, KL divergence: 0.8994261630126223\n",
      "Reconstruction loss: 240.1006694162362, KL divergence: 0.08814003250164082\n",
      "Reconstruction loss: 143.30084830710786, KL divergence: 0.03636289331829118\n",
      "Reconstruction loss: 287.6889174750606, KL divergence: 0.3337910119606141\n",
      "Reconstruction loss: 203.5726371056984, KL divergence: 0.039672992097103954\n",
      "Reconstruction loss: 163.7363551088771, KL divergence: 0.06329817006621485\n",
      "Reconstruction loss: 259.764509064495, KL divergence: 0.022593421802168\n",
      "Reconstruction loss: 172.7484054473585, KL divergence: 0.061096642751993446\n",
      "Reconstruction loss: 219.94803958491855, KL divergence: 0.05401610457361\n",
      "Reconstruction loss: 219.91504170971194, KL divergence: 0.28038131788215215\n",
      "Reconstruction loss: 327.5259317052411, KL divergence: 0.07288155979221872\n",
      "Reconstruction loss: 205.25773529870224, KL divergence: 0.08423813813418807\n",
      "Reconstruction loss: 136.26902248890485, KL divergence: 0.043950883474751234\n",
      "Reconstruction loss: 218.97365775490255, KL divergence: 0.04197781900014075\n",
      "Reconstruction loss: 214.47695101567012, KL divergence: 0.1279765993262359\n",
      "Reconstruction loss: 191.2432418066851, KL divergence: 0.022586294544919394\n",
      "Reconstruction loss: 237.25078396140086, KL divergence: 0.13615394112723678\n",
      "Reconstruction loss: 298.86417949294514, KL divergence: 0.3806210116740963\n",
      "Reconstruction loss: 238.97701520110814, KL divergence: 0.11988952706604422\n",
      "Reconstruction loss: 319.22800624744343, KL divergence: 0.3818253235257486\n",
      "Reconstruction loss: 200.19736551492997, KL divergence: 0.04283511263347378\n",
      "Reconstruction loss: 200.110321211602, KL divergence: 0.1862307800170499\n",
      "Reconstruction loss: 167.432237484709, KL divergence: 0.023058978396570617\n",
      "Reconstruction loss: 289.079272086515, KL divergence: 0.5321217635652669\n",
      "Reconstruction loss: 292.14947915245716, KL divergence: 0.36538814738600145\n",
      "Reconstruction loss: 206.21285689146504, KL divergence: 0.16430598398473073\n",
      "Reconstruction loss: 197.34760075512676, KL divergence: 0.11179239286779874\n",
      "Reconstruction loss: 203.8793343902417, KL divergence: 0.06254759489229617\n",
      "Reconstruction loss: 294.71716172005966, KL divergence: 0.4063164831469358\n",
      "Reconstruction loss: 196.1249761330645, KL divergence: 0.024545312070174408\n",
      "Reconstruction loss: 185.83378098474793, KL divergence: 0.03177289619058232\n",
      "Reconstruction loss: 153.90776876189744, KL divergence: 0.04846130878442878\n",
      "Reconstruction loss: 197.36563511687572, KL divergence: 0.04837341157743502\n",
      "Reconstruction loss: 196.3094826040588, KL divergence: 0.02402002938373249\n",
      "Reconstruction loss: 166.72557129039967, KL divergence: 0.05725592546970082\n",
      "Reconstruction loss: 227.78603985244186, KL divergence: 0.07235228810683536\n",
      "Reconstruction loss: 175.96427768469195, KL divergence: 0.030552000793798384\n",
      "Reconstruction loss: 174.27552049153883, KL divergence: 0.06851371543646856\n",
      "Reconstruction loss: 122.52880929687143, KL divergence: 0.06471050228285097\n",
      "Reconstruction loss: 203.336964841299, KL divergence: 0.08676508127029564\n",
      "Reconstruction loss: 240.24102875491826, KL divergence: 0.14502912111951338\n",
      "Reconstruction loss: 233.6763168134668, KL divergence: 0.08746089766696435\n",
      "Reconstruction loss: 172.43384069303383, KL divergence: 0.05849875448815167\n",
      "Reconstruction loss: 228.55785772180297, KL divergence: 0.09432446676719464\n",
      "Reconstruction loss: 203.16933876178288, KL divergence: 0.04332458572817016\n",
      "Reconstruction loss: 214.44008557976102, KL divergence: 0.23999094283443817\n",
      "Reconstruction loss: 136.05922307996514, KL divergence: 0.023058978396570617\n",
      "Reconstruction loss: 213.02895629651982, KL divergence: 0.023058978396570617\n",
      "Reconstruction loss: 156.58145665091308, KL divergence: 0.023058978396570617\n",
      "Reconstruction loss: 209.81625106304205, KL divergence: 0.32468615543588497\n",
      "Reconstruction loss: 198.58908808445037, KL divergence: 0.13653711024161674\n",
      "Reconstruction loss: 267.509042210359, KL divergence: 0.42885491979581813\n",
      "Reconstruction loss: 174.39028444642318, KL divergence: 0.0652881109221245\n",
      "Reconstruction loss: 148.36059184032098, KL divergence: 0.04796508649304221\n",
      "Reconstruction loss: 154.643142824647, KL divergence: 0.03758900377152902\n",
      "Reconstruction loss: 190.760309220617, KL divergence: 0.10522075502161321\n",
      "Reconstruction loss: 196.4336031897208, KL divergence: 0.1655408895346322\n",
      "Reconstruction loss: 210.79530977284747, KL divergence: 0.12433480111512363\n",
      "Reconstruction loss: 302.1660697913045, KL divergence: 0.3082427594222623\n",
      "Reconstruction loss: 197.8094903459444, KL divergence: 0.02640442322507408\n",
      "Reconstruction loss: 183.42556253628868, KL divergence: 0.02347248058388235\n",
      "Reconstruction loss: 162.95466878030396, KL divergence: 0.02749495827113152\n",
      "Reconstruction loss: 214.9363010832576, KL divergence: 0.4003926250425235\n",
      "Reconstruction loss: 137.14554209488762, KL divergence: 0.03275401148725299\n",
      "Reconstruction loss: 321.4986323852463, KL divergence: 0.24654030767545565\n",
      "Reconstruction loss: 260.3180213445679, KL divergence: 0.08852161135460057\n",
      "Reconstruction loss: 314.0196611460407, KL divergence: 0.8083587477518639\n",
      "Reconstruction loss: 262.2865631472829, KL divergence: 0.4969258886786445\n",
      "Reconstruction loss: 240.13715620721717, KL divergence: 0.057835638807098355\n",
      "Reconstruction loss: 224.55391449486103, KL divergence: 0.2889659453775743\n",
      "Reconstruction loss: 235.02286461145079, KL divergence: 0.29889428632175147\n",
      "Reconstruction loss: 200.86253311156702, KL divergence: 0.047004107167351206\n",
      "Reconstruction loss: 256.8761589290675, KL divergence: 0.13046912102720204\n",
      "Reconstruction loss: 245.12236554495388, KL divergence: 0.1634108446581677\n",
      "Reconstruction loss: 204.78960514116943, KL divergence: 0.2803434068511421\n",
      "Reconstruction loss: 264.48806095341246, KL divergence: 0.07713817287413971\n",
      "Reconstruction loss: 211.59948464309278, KL divergence: 0.17746080291635824\n",
      "Reconstruction loss: 208.0076294842279, KL divergence: 0.03854491555554779\n",
      "Reconstruction loss: 209.74423027431507, KL divergence: 0.1629592547613854\n",
      "Reconstruction loss: 157.67222739560387, KL divergence: 0.028744476962655863\n",
      "Reconstruction loss: 192.22154845779855, KL divergence: 0.09812365650252214\n",
      "Reconstruction loss: 340.72273781563604, KL divergence: 0.12727005234502048\n",
      "Reconstruction loss: 157.23170111296972, KL divergence: 0.03864214668646876\n",
      "Reconstruction loss: 156.3348676859648, KL divergence: 0.03482401656817513\n",
      "Reconstruction loss: 288.0812383414994, KL divergence: 0.585866200316427\n",
      "Reconstruction loss: 221.91377027194056, KL divergence: 0.5773191255816579\n",
      "Reconstruction loss: 257.06990278819376, KL divergence: 0.46509032458384497\n",
      "Reconstruction loss: 314.39771977815474, KL divergence: 0.7696762300217282\n",
      "Reconstruction loss: 236.92298308962333, KL divergence: 0.1963508092540716\n",
      "Reconstruction loss: 155.37708124878307, KL divergence: 0.028530529799886795\n",
      "Reconstruction loss: 165.50005454516827, KL divergence: 0.03984071235630121\n",
      "Reconstruction loss: 196.49968115139376, KL divergence: 0.08599616334954274\n",
      "Reconstruction loss: 190.86174001393726, KL divergence: 0.09330079817168596\n",
      "Reconstruction loss: 235.0323799526863, KL divergence: 0.22060069158598988\n",
      "Reconstruction loss: 183.81320532315743, KL divergence: 0.023753011630217646\n",
      "Reconstruction loss: 233.76576822820124, KL divergence: 0.2161151508374582\n",
      "Reconstruction loss: 200.5439038978147, KL divergence: 0.21794999159585549\n",
      "Reconstruction loss: 269.29377773370055, KL divergence: 0.7250809420524418\n",
      "Reconstruction loss: 175.17999807728594, KL divergence: 0.07997195393858958\n",
      "Reconstruction loss: 211.52288874994173, KL divergence: 0.1515151813897282\n",
      "Reconstruction loss: 317.5190656650726, KL divergence: 0.5852506316763375\n",
      "Reconstruction loss: 325.3972558248068, KL divergence: 0.06780418460775267\n",
      "Reconstruction loss: 279.87594158467493, KL divergence: 0.6427571389302138\n",
      "Reconstruction loss: 259.322750576505, KL divergence: 0.13406312401024534\n",
      "Reconstruction loss: 161.32465900019673, KL divergence: 0.024742465957843074\n",
      "Reconstruction loss: 165.53557293799756, KL divergence: 0.06476669946267294\n",
      "Reconstruction loss: 208.86650915750297, KL divergence: 0.031166371538599247\n",
      "Reconstruction loss: 197.38738360036848, KL divergence: 0.14573677719203554\n",
      "Reconstruction loss: 221.447319719529, KL divergence: 0.05663328156868619\n",
      "Reconstruction loss: 194.59096492849113, KL divergence: 0.08789967971645574\n",
      "Reconstruction loss: 202.97895702812593, KL divergence: 0.07023039651562157\n",
      "Reconstruction loss: 214.57090333060583, KL divergence: 0.32108843999072983\n",
      "Reconstruction loss: 228.60521035226458, KL divergence: 0.06260460885632413\n",
      "Reconstruction loss: 183.74012497635198, KL divergence: 0.055089874995411736\n",
      "Reconstruction loss: 154.80679732024888, KL divergence: 0.03096072414581763\n",
      "Reconstruction loss: 339.09677144401377, KL divergence: 0.3731605871646087\n",
      "Reconstruction loss: 245.23138668188417, KL divergence: 0.15657549359842293\n",
      "Reconstruction loss: 212.59237023642243, KL divergence: 0.17589002467982828\n",
      "Reconstruction loss: 292.98974800247174, KL divergence: 0.10948537144859738\n",
      "Reconstruction loss: 201.3409468670608, KL divergence: 0.039223173400933564\n",
      "Reconstruction loss: 177.19805326983973, KL divergence: 0.053634196594708916\n",
      "Reconstruction loss: 307.0094028170496, KL divergence: 0.4033687672906769\n",
      "Reconstruction loss: 248.5916548694313, KL divergence: 0.2525908528429462\n",
      "Reconstruction loss: 215.39971994951924, KL divergence: 0.07164621407620908\n",
      "Reconstruction loss: 231.18452205465994, KL divergence: 0.023926509503236437\n",
      "Reconstruction loss: 224.87531764054162, KL divergence: 0.1899637850556377\n",
      "Reconstruction loss: 396.0007800714054, KL divergence: 0.3791346358638854\n",
      "Reconstruction loss: 217.20848234183313, KL divergence: 0.1378610011241897\n",
      "Reconstruction loss: 137.56837652208833, KL divergence: 0.023926509503236437\n",
      "Reconstruction loss: 377.01264123901717, KL divergence: 0.3256656519476479\n",
      "Reconstruction loss: 215.6341530337126, KL divergence: 0.054828298763009375\n",
      "Reconstruction loss: 214.1318547740158, KL divergence: 0.05361878024122946\n",
      "Reconstruction loss: 172.05923312066022, KL divergence: 0.024535110530428006\n",
      "Reconstruction loss: 231.8924673413922, KL divergence: 0.15028843066764586\n",
      "Reconstruction loss: 226.9846395333065, KL divergence: 0.07083787164351546\n",
      "Reconstruction loss: 197.2329254849491, KL divergence: 0.0351596042668339\n",
      "Reconstruction loss: 157.2427403555315, KL divergence: 0.03486903875739139\n",
      "Reconstruction loss: 243.23755626714936, KL divergence: 0.5769670411075216\n",
      "Reconstruction loss: 305.48141446829015, KL divergence: 0.3410992510060129\n",
      "Reconstruction loss: 299.0444298210314, KL divergence: 0.8626628967932366\n",
      "Reconstruction loss: 189.68134602291428, KL divergence: 0.03457442510880204\n",
      "Reconstruction loss: 205.51008981342352, KL divergence: 0.32788191278092077\n",
      "Reconstruction loss: 225.78043182493408, KL divergence: 0.08160273058447853\n",
      "Reconstruction loss: 278.04759291160883, KL divergence: 0.10908700276364719\n",
      "Reconstruction loss: 216.7098273499778, KL divergence: 0.136755114797399\n",
      "Reconstruction loss: 195.46154689871491, KL divergence: 0.03340977472333018\n",
      "Reconstruction loss: 422.42737507736, KL divergence: 0.22633830305813118\n",
      "Reconstruction loss: 340.64679669824426, KL divergence: 0.5550209202334699\n",
      "Reconstruction loss: 240.2089091599206, KL divergence: 0.38413408645479064\n",
      "Reconstruction loss: 178.22183963641942, KL divergence: 0.024119751734611172\n",
      "Reconstruction loss: 178.36899618085835, KL divergence: 0.18102719991115718\n",
      "Reconstruction loss: 320.25168224617903, KL divergence: 1.383062128679284\n",
      "Reconstruction loss: 179.58148308271413, KL divergence: 0.05211434777509372\n",
      "Reconstruction loss: 312.41688266628876, KL divergence: 0.7749499341316131\n",
      "Reconstruction loss: 290.6043843026266, KL divergence: 0.9594616518782417\n",
      "Reconstruction loss: 267.5613795518603, KL divergence: 0.04584048020376369\n",
      "Reconstruction loss: 206.79644793653125, KL divergence: 0.07631153579700656\n",
      "Reconstruction loss: 197.91509646408008, KL divergence: 0.04106418996053174\n",
      "Reconstruction loss: 121.44723879391634, KL divergence: 0.024119751734611172\n",
      "Reconstruction loss: 199.70218854402054, KL divergence: 0.06614867570291583\n",
      "Reconstruction loss: 202.1879940992744, KL divergence: 0.2411515940670264\n",
      "Reconstruction loss: 136.06562560391907, KL divergence: 0.028932113147287386\n",
      "Reconstruction loss: 159.60724258823012, KL divergence: 0.024119751734611172\n",
      "Reconstruction loss: 302.2457756671073, KL divergence: 0.42528964513240647\n",
      "Reconstruction loss: 216.83926207386818, KL divergence: 0.03469243311937964\n",
      "Reconstruction loss: 231.45063945864428, KL divergence: 0.39640401150050886\n",
      "Reconstruction loss: 266.19637607089606, KL divergence: 0.18168075256750071\n",
      "Reconstruction loss: 215.71400845385807, KL divergence: 0.029852885135743357\n",
      "Reconstruction loss: 240.0344462638007, KL divergence: 0.22853240333878228\n",
      "Reconstruction loss: 204.35792729251762, KL divergence: 0.39519141632489124\n",
      "Reconstruction loss: 316.9253966162278, KL divergence: 0.05707232506321458\n",
      "Reconstruction loss: 233.39347774044927, KL divergence: 0.025833780637266712\n",
      "Reconstruction loss: 303.0112112075295, KL divergence: 0.13346162450887505\n",
      "Reconstruction loss: 233.21660741039892, KL divergence: 0.7297888022705749\n",
      "Reconstruction loss: 198.2037508420242, KL divergence: 0.03999660388913928\n",
      "Reconstruction loss: 226.78351506011563, KL divergence: 0.024119751734611172\n",
      "Reconstruction loss: 204.89927566014399, KL divergence: 0.03678900930616469\n",
      "Reconstruction loss: 284.3503658095374, KL divergence: 0.2779508186471897\n",
      "Reconstruction loss: 161.7400349375242, KL divergence: 0.0243141696268257\n",
      "Reconstruction loss: 262.5310512900013, KL divergence: 0.2762952296015861\n",
      "Reconstruction loss: 316.7946503348464, KL divergence: 0.2814528966802678\n",
      "Reconstruction loss: 278.7769369632393, KL divergence: 0.06319950024397336\n",
      "Reconstruction loss: 282.30203095338595, KL divergence: 0.03501348907144264\n",
      "Reconstruction loss: 248.07942714401935, KL divergence: 0.047293077251432614\n",
      "Reconstruction loss: 242.47338098139122, KL divergence: 0.043290494213507225\n",
      "Reconstruction loss: 410.51256219406093, KL divergence: 0.028959786830348444\n",
      "Reconstruction loss: 221.3679583632359, KL divergence: 0.11841857743878909\n",
      "Reconstruction loss: 167.47954719198756, KL divergence: 0.04095829539882678\n",
      "Reconstruction loss: 246.90247402497914, KL divergence: 0.02639623623209797\n",
      "Reconstruction loss: 376.25993625612466, KL divergence: 0.20055265230625957\n",
      "Reconstruction loss: 198.45832638480812, KL divergence: 0.08510544145556415\n",
      "Reconstruction loss: 275.9459942247428, KL divergence: 0.1629608887375235\n",
      "Reconstruction loss: 247.0677399068444, KL divergence: 0.16629644074921562\n",
      "Reconstruction loss: 388.14553092481776, KL divergence: 0.1761443896399873\n",
      "Reconstruction loss: 260.3267720150106, KL divergence: 0.10722371503357414\n",
      "Reconstruction loss: 179.52773853435025, KL divergence: 0.12208325390743757\n",
      "Reconstruction loss: 248.0923115667663, KL divergence: 0.06869886009651482\n",
      "Reconstruction loss: 180.7851107937358, KL divergence: 0.042029650925401396\n",
      "Reconstruction loss: 191.93434868102224, KL divergence: 0.0243141696268257\n",
      "Reconstruction loss: 142.099350092631, KL divergence: 0.02636958670569073\n",
      "Reconstruction loss: 260.8967914644834, KL divergence: 0.13550076554349\n",
      "Reconstruction loss: 194.89692568380715, KL divergence: 0.02436171381340102\n",
      "Reconstruction loss: 221.95306475179106, KL divergence: 0.08002548888028205\n",
      "Reconstruction loss: 262.92527737920415, KL divergence: 0.05856779790816263\n",
      "Reconstruction loss: 143.4120716327511, KL divergence: 0.029136072099642862\n",
      "Reconstruction loss: 238.34616642572917, KL divergence: 0.1682364143083615\n",
      "Reconstruction loss: 200.59272906962462, KL divergence: 0.02489294436248557\n",
      "Reconstruction loss: 220.72844548295558, KL divergence: 0.025575170390668367\n",
      "Reconstruction loss: 166.49009224347137, KL divergence: 0.02711389788275076\n",
      "Reconstruction loss: 200.9101807215431, KL divergence: 0.027795816613162994\n",
      "Reconstruction loss: 177.41042839688726, KL divergence: 0.02450519053990352\n",
      "Reconstruction loss: 223.80871985391155, KL divergence: 0.05345986315822493\n",
      "Reconstruction loss: 255.16452430531538, KL divergence: 0.02729704515035286\n",
      "Reconstruction loss: 209.3435812568253, KL divergence: 0.02767959625543137\n",
      "Reconstruction loss: 153.10532272600977, KL divergence: 0.03519562722586267\n",
      "Reconstruction loss: 224.39257328326795, KL divergence: 0.18486815937063478\n",
      "Reconstruction loss: 234.01516675009378, KL divergence: 0.7236999147330623\n",
      "Reconstruction loss: 299.0273767636214, KL divergence: 1.6929223677124006\n",
      "Reconstruction loss: 127.27902377294507, KL divergence: 0.02783678087372088\n",
      "Reconstruction loss: 202.37887295281706, KL divergence: 0.1521382010240609\n",
      "Reconstruction loss: 200.4240424574749, KL divergence: 0.03768474255516274\n",
      "Reconstruction loss: 314.2083331934871, KL divergence: 0.4263842224207977\n",
      "Reconstruction loss: 163.3585647188955, KL divergence: 0.032619212231668304\n",
      "Reconstruction loss: 169.6729808263605, KL divergence: 0.026369697255594904\n",
      "Reconstruction loss: 185.4968792808408, KL divergence: 0.02450519053990352\n",
      "Reconstruction loss: 229.12821856836183, KL divergence: 0.06841782639966049\n",
      "Reconstruction loss: 232.188744871472, KL divergence: 0.1696233887474055\n",
      "Reconstruction loss: 243.21434770321147, KL divergence: 0.1488397299562983\n",
      "Reconstruction loss: 189.70971395244098, KL divergence: 0.03503537258437012\n",
      "Reconstruction loss: 219.49887934746837, KL divergence: 0.12436845025591736\n",
      "Reconstruction loss: 190.64208351346744, KL divergence: 0.03624215018133908\n",
      "Reconstruction loss: 140.13761669152453, KL divergence: 0.02450519053990352\n",
      "Reconstruction loss: 267.42871665005003, KL divergence: 0.26119454001362646\n",
      "Reconstruction loss: 192.27527014112576, KL divergence: 0.14964942549244808\n",
      "Reconstruction loss: 352.1392715133064, KL divergence: 0.08439602916519473\n",
      "Reconstruction loss: 181.2561133626806, KL divergence: 0.025769958348049737\n",
      "Reconstruction loss: 163.82229504419126, KL divergence: 0.02450519053990352\n",
      "Reconstruction loss: 279.3693993303975, KL divergence: 0.43904970926595\n",
      "Reconstruction loss: 219.556346508222, KL divergence: 0.040021822984838085\n",
      "Reconstruction loss: 168.78006734123545, KL divergence: 0.026902371837145278\n",
      "Reconstruction loss: 170.53354144669683, KL divergence: 0.02450519053990352\n",
      "Reconstruction loss: 188.58143833115622, KL divergence: 0.02450519053990352\n",
      "Reconstruction loss: 199.3309220081252, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 192.7302226407898, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 243.45856317115417, KL divergence: 0.028781655578780763\n",
      "Reconstruction loss: 227.8297137880999, KL divergence: 0.20157987036779912\n",
      "Reconstruction loss: 227.98003719866227, KL divergence: 0.03793282722295038\n",
      "Reconstruction loss: 205.08283018475106, KL divergence: 0.23114281163006445\n",
      "Reconstruction loss: 243.30307966690177, KL divergence: 0.21722029323084124\n",
      "Reconstruction loss: 194.94645794909755, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 177.37216658623436, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 159.2062564546145, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 289.56097361839124, KL divergence: 0.257613616914163\n",
      "Reconstruction loss: 249.11690571331832, KL divergence: 0.2843807261944788\n",
      "Reconstruction loss: 167.78080831789873, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 343.85810573846595, KL divergence: 0.2082015199856339\n",
      "Reconstruction loss: 203.5301769670238, KL divergence: 0.07506695043978612\n",
      "Reconstruction loss: 179.57446961109252, KL divergence: 0.026893445929032422\n",
      "Reconstruction loss: 232.724235757302, KL divergence: 0.4846947248691445\n",
      "Reconstruction loss: 220.01884524348117, KL divergence: 0.3873581005365988\n",
      "Reconstruction loss: 127.65518647645031, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 174.95417436072887, KL divergence: 0.024678356217162245\n",
      "Reconstruction loss: 254.13017005746477, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 185.309489558157, KL divergence: 0.026629840354507106\n",
      "Reconstruction loss: 162.40616358806386, KL divergence: 0.025590767187223307\n",
      "Reconstruction loss: 303.2752026661075, KL divergence: 0.05500635560186934\n",
      "Reconstruction loss: 315.66894966734594, KL divergence: 0.47187829960061667\n",
      "Reconstruction loss: 263.0981991240403, KL divergence: 0.41015292590807584\n",
      "Reconstruction loss: 199.70431068441758, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 297.893401437771, KL divergence: 0.4629341041821312\n",
      "Reconstruction loss: 239.16885106479606, KL divergence: 0.15287064896273905\n",
      "Reconstruction loss: 156.14586022871893, KL divergence: 0.024569024113054505\n",
      "Reconstruction loss: 253.95456642878776, KL divergence: 0.08249009071233437\n",
      "Reconstruction loss: 279.30316917879145, KL divergence: 0.4008235216177513\n",
      "Reconstruction loss: 277.5916994741391, KL divergence: 0.10497300314052338\n",
      "Reconstruction loss: 272.14018184289796, KL divergence: 0.3882203361968049\n",
      "Reconstruction loss: 176.54117285349093, KL divergence: 0.02459374964855565\n",
      "Reconstruction loss: 236.90308778791314, KL divergence: 0.14945353327119992\n",
      "Reconstruction loss: 182.88769045669585, KL divergence: 0.02459374964855565\n",
      "Reconstruction loss: 192.9642255845556, KL divergence: 0.18226582649053058\n",
      "Reconstruction loss: 182.59241030228364, KL divergence: 0.026334854821732867\n",
      "Reconstruction loss: 237.95899848225227, KL divergence: 0.02459374964855565\n",
      "Reconstruction loss: 193.3973494888766, KL divergence: 0.02850476763940052\n",
      "Reconstruction loss: 163.5605675186652, KL divergence: 0.03752135363540626\n",
      "Reconstruction loss: 258.24823651306315, KL divergence: 0.03771759151095605\n",
      "Reconstruction loss: 221.64088812022732, KL divergence: 0.06984489447682196\n",
      "Reconstruction loss: 171.82377543027457, KL divergence: 0.03456464626591099\n",
      "Reconstruction loss: 258.18509892935117, KL divergence: 0.03506244042052087\n",
      "Reconstruction loss: 163.14272958495332, KL divergence: 0.05815831213366007\n",
      "Reconstruction loss: 469.9157256304494, KL divergence: 0.6963852178655314\n",
      "Reconstruction loss: 340.55691610734783, KL divergence: 0.2832209467100088\n",
      "Reconstruction loss: 163.05141342950884, KL divergence: 0.02795245777475308\n",
      "Reconstruction loss: 215.97158362588894, KL divergence: 0.02459374964855565\n",
      "Reconstruction loss: 227.72762026892798, KL divergence: 0.16438373178064503\n",
      "Reconstruction loss: 210.93331987009188, KL divergence: 0.15249709262389055\n",
      "Reconstruction loss: 197.185557221766, KL divergence: 0.11660427955456631\n",
      "Reconstruction loss: 191.29676245468843, KL divergence: 0.02459374964855565\n",
      "Reconstruction loss: 140.15500192125307, KL divergence: 0.02459374964855565\n",
      "Reconstruction loss: 148.42948902224936, KL divergence: 0.031955005997753305\n",
      "Reconstruction loss: 237.24526693976728, KL divergence: 0.3011046743045153\n",
      "Reconstruction loss: 224.80602927328556, KL divergence: 0.32346340880338403\n",
      "Reconstruction loss: 238.32857938756837, KL divergence: 0.03364963764011253\n",
      "Reconstruction loss: 255.05379348740084, KL divergence: 0.07724357356018785\n",
      "Reconstruction loss: 213.59968049627935, KL divergence: 0.052286039828234654\n",
      "Reconstruction loss: 153.78873556950055, KL divergence: 0.03311742841849952\n",
      "Reconstruction loss: 216.48476050881038, KL divergence: 0.02459374964855565\n",
      "Reconstruction loss: 178.93574189768967, KL divergence: 0.02490268725451822\n",
      "Reconstruction loss: 332.58213369930905, KL divergence: 0.2416494785427778\n",
      "Reconstruction loss: 248.1590764585031, KL divergence: 0.03216123878101296\n",
      "Reconstruction loss: 237.89720348346964, KL divergence: 0.2512096044356282\n",
      "Reconstruction loss: 154.25255751985569, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 181.13247796422013, KL divergence: 0.03282414539791334\n",
      "Reconstruction loss: 168.8499236099152, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 241.99040551030663, KL divergence: 0.07356774276045408\n",
      "Reconstruction loss: 158.04159983198056, KL divergence: 0.0354892438734461\n",
      "Reconstruction loss: 285.25684666364594, KL divergence: 0.6433198272759801\n",
      "Reconstruction loss: 212.1704846140124, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 201.31158113694343, KL divergence: 0.03286117777465941\n",
      "Reconstruction loss: 233.90358481018652, KL divergence: 0.5316033667581804\n",
      "Reconstruction loss: 214.93681004251704, KL divergence: 0.06604053726345988\n",
      "Reconstruction loss: 183.77296282429583, KL divergence: 0.04775922373196073\n",
      "Reconstruction loss: 287.3508304959086, KL divergence: 0.07272298794088722\n",
      "Reconstruction loss: 148.3467473241607, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 173.26676705544986, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 160.79829251726213, KL divergence: 0.025470484903193202\n",
      "Reconstruction loss: 243.80232929238582, KL divergence: 0.04537448768385577\n",
      "Reconstruction loss: 296.71077160709785, KL divergence: 0.23586974603441135\n",
      "Reconstruction loss: 225.0773393192929, KL divergence: 0.24079056618484074\n",
      "Reconstruction loss: 162.29580982044826, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 226.91517357367758, KL divergence: 0.3169801099663783\n",
      "Reconstruction loss: 193.94258124569322, KL divergence: 0.15116838526225063\n",
      "Reconstruction loss: 227.88721935107057, KL divergence: 0.623537027195529\n",
      "Reconstruction loss: 230.0727438679407, KL divergence: 0.13879145983253338\n",
      "Reconstruction loss: 192.76864114252277, KL divergence: 0.04463537273515178\n",
      "Reconstruction loss: 168.37582824332344, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 158.10904466501887, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 222.35461563003292, KL divergence: 0.13269100986327492\n",
      "Reconstruction loss: 148.21398267309797, KL divergence: 0.02456169349885351\n",
      "Reconstruction loss: 228.67569020695873, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 167.7985153027611, KL divergence: 0.025339485684493346\n",
      "Reconstruction loss: 193.2478635478854, KL divergence: 0.03683309611470398\n",
      "Reconstruction loss: 196.841818996695, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 208.1097922953061, KL divergence: 0.09457755147630709\n",
      "Reconstruction loss: 189.52585292542653, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 184.45111570863145, KL divergence: 0.13156790688083692\n",
      "Reconstruction loss: 171.95327155544916, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 215.78596258211337, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 335.9558970787782, KL divergence: 0.05951820874853947\n",
      "Reconstruction loss: 218.08948726597768, KL divergence: 0.02600774769581693\n",
      "Reconstruction loss: 229.4542869814006, KL divergence: 0.05690197384845319\n",
      "Reconstruction loss: 142.72831500399218, KL divergence: 0.025142662492661227\n",
      "Reconstruction loss: 155.41538672102564, KL divergence: 0.0253857834340101\n",
      "Reconstruction loss: 170.71191370652505, KL divergence: 0.025653151793701312\n",
      "Reconstruction loss: 328.20866168149024, KL divergence: 0.039941322184239236\n",
      "Reconstruction loss: 217.47844374280825, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 255.22384297671374, KL divergence: 0.14522095420975623\n",
      "Reconstruction loss: 260.78559267512463, KL divergence: 0.07120847169750305\n",
      "Reconstruction loss: 234.6373469164071, KL divergence: 0.34158374472173175\n",
      "Reconstruction loss: 153.69786753197906, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 234.91153858259182, KL divergence: 0.055160632735147874\n",
      "Reconstruction loss: 204.8880706825917, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 136.154990701654, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 150.66032853320144, KL divergence: 0.0262734637984528\n",
      "Reconstruction loss: 290.5160722526032, KL divergence: 0.08329854622982524\n",
      "Reconstruction loss: 264.4228553566154, KL divergence: 0.160070777572886\n",
      "Reconstruction loss: 234.52878483696787, KL divergence: 0.03941094599156586\n",
      "Reconstruction loss: 217.10056082458186, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 251.371260768798, KL divergence: 0.048161621679712996\n",
      "Reconstruction loss: 246.34527222388613, KL divergence: 0.06219177173612833\n",
      "Reconstruction loss: 171.76138149236638, KL divergence: 0.024501163074820476\n",
      "Reconstruction loss: 285.1064040542389, KL divergence: 0.09269836423128458\n",
      "Reconstruction loss: 206.12497070010352, KL divergence: 0.030456047508898187\n",
      "Reconstruction loss: 164.62579946285916, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 223.7495102857105, KL divergence: 0.0340972502430803\n",
      "Reconstruction loss: 276.8557478187847, KL divergence: 0.07255022509544179\n",
      "Reconstruction loss: 276.3969433473596, KL divergence: 0.10783199578047487\n",
      "Reconstruction loss: 196.91883934072513, KL divergence: 0.04541755956257815\n",
      "Reconstruction loss: 170.48450830461314, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 154.8685638736999, KL divergence: 0.02573427796089023\n",
      "Reconstruction loss: 136.93860515763953, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 235.77058861509283, KL divergence: 0.0449447587563106\n",
      "Reconstruction loss: 353.3849524798002, KL divergence: 0.051677854505650656\n",
      "Reconstruction loss: 197.7697349509937, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 167.8617399515326, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 194.53829878242226, KL divergence: 0.024510082872858052\n",
      "Reconstruction loss: 201.07722202113095, KL divergence: 0.026145908536439277\n",
      "Reconstruction loss: 236.3486454579858, KL divergence: 0.5948751517254813\n",
      "Reconstruction loss: 223.5967148352733, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 177.5261152320151, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 186.28837076381, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 169.50894115071992, KL divergence: 0.02486800603630035\n",
      "Reconstruction loss: 194.1473733443969, KL divergence: 0.02710065424077651\n",
      "Reconstruction loss: 255.11355411605325, KL divergence: 0.18184512489244842\n",
      "Reconstruction loss: 248.68187305766816, KL divergence: 0.03204587790176244\n",
      "Reconstruction loss: 208.15108193529824, KL divergence: 0.06659336247171804\n",
      "Reconstruction loss: 182.3493638607309, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 184.8112808913322, KL divergence: 0.028364159370609654\n",
      "Reconstruction loss: 171.8493998612438, KL divergence: 0.025987279965902887\n",
      "Reconstruction loss: 220.00142414697063, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 144.73235827170714, KL divergence: 0.02437732370592649\n",
      "Reconstruction loss: 204.60202293659316, KL divergence: 0.06792098802390278\n",
      "Reconstruction loss: 232.41495924031085, KL divergence: 0.0784712730161578\n",
      "Reconstruction loss: 260.53162116542194, KL divergence: 0.04117218657347327\n",
      "Reconstruction loss: 274.2164526351045, KL divergence: 0.2030646063180256\n",
      "Reconstruction loss: 370.95746863974614, KL divergence: 0.6171687104236332\n",
      "Reconstruction loss: 187.9918884260784, KL divergence: 0.024340828187334795\n",
      "Reconstruction loss: 349.0484649221861, KL divergence: 0.13730201571125966\n",
      "Reconstruction loss: 217.5058962655305, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 187.95739497440374, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 208.08753732962572, KL divergence: 0.06843828235564575\n",
      "Reconstruction loss: 227.27633236505483, KL divergence: 0.05514732495697566\n",
      "Reconstruction loss: 182.5855098850339, KL divergence: 0.024243523306797243\n",
      "Reconstruction loss: 176.77345691337666, KL divergence: 0.02924927699831914\n",
      "Reconstruction loss: 187.04848686786875, KL divergence: 0.024945704534726332\n",
      "Reconstruction loss: 227.06105625990594, KL divergence: 0.1015886275871431\n",
      "Reconstruction loss: 244.9766882679822, KL divergence: 0.030379712534533976\n",
      "Reconstruction loss: 176.19113384969592, KL divergence: 0.04181935872381992\n",
      "Reconstruction loss: 193.41274410917478, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 184.8589723916767, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 177.99711412737057, KL divergence: 0.035090374915261124\n",
      "Reconstruction loss: 207.38998162242046, KL divergence: 0.14128179425832194\n",
      "Reconstruction loss: 285.7539778872871, KL divergence: 0.4177499622047748\n",
      "Reconstruction loss: 142.2644244258505, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 252.15383934946044, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 289.67408315766136, KL divergence: 0.1695336557998195\n",
      "Reconstruction loss: 169.26247176052505, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 189.8092852667994, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 226.92865997497864, KL divergence: 0.050537072600556565\n",
      "Reconstruction loss: 209.97587684788525, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 245.18260014617, KL divergence: 0.18731082533078935\n",
      "Reconstruction loss: 251.19980021732502, KL divergence: 0.02422169618785075\n",
      "Reconstruction loss: 230.67269044721104, KL divergence: 0.052180045494578386\n",
      "Reconstruction loss: 221.48517813005668, KL divergence: 0.08820856538030775\n",
      "Reconstruction loss: 125.94452350107649, KL divergence: 0.02798681838828615\n",
      "Reconstruction loss: 229.19060184879993, KL divergence: 0.04237476516328703\n",
      "Reconstruction loss: 170.81396024872873, KL divergence: 0.024094618195070472\n",
      "Reconstruction loss: 211.60904420716153, KL divergence: 0.06474303081874588\n",
      "Reconstruction loss: 349.4599748930578, KL divergence: 0.15284541104182509\n",
      "Reconstruction loss: 186.62985798672375, KL divergence: 0.027829725373490488\n",
      "Reconstruction loss: 254.46061971850736, KL divergence: 0.18472486722288128\n",
      "Reconstruction loss: 217.15349209744636, KL divergence: 0.11274066369339575\n",
      "Reconstruction loss: 170.66650473357717, KL divergence: 0.024094618195070472\n",
      "Reconstruction loss: 178.0978844461862, KL divergence: 0.026268283137913573\n",
      "Reconstruction loss: 222.63941478100156, KL divergence: 0.35853889772835273\n",
      "Reconstruction loss: 244.29108690546192, KL divergence: 0.0726672455724956\n",
      "Reconstruction loss: 131.53897444403123, KL divergence: 0.02445546819576777\n",
      "Reconstruction loss: 252.466327283467, KL divergence: 0.10044225130124251\n",
      "Reconstruction loss: 239.7272777212174, KL divergence: 0.024094618195070472\n",
      "Reconstruction loss: 157.04470629502845, KL divergence: 0.024094618195070472\n",
      "Reconstruction loss: 295.78215574728176, KL divergence: 0.2094134728372754\n",
      "Reconstruction loss: 339.95921821109914, KL divergence: 0.22018825904625972\n",
      "Reconstruction loss: 143.73453187590758, KL divergence: 0.028816749822740295\n",
      "Reconstruction loss: 234.20668850396393, KL divergence: 0.034634323103626574\n",
      "Reconstruction loss: 267.5875691551224, KL divergence: 0.07018547027450794\n",
      "Reconstruction loss: 215.5034984712124, KL divergence: 0.053683013936609236\n",
      "Reconstruction loss: 239.97731828790086, KL divergence: 0.053488212921319056\n",
      "Reconstruction loss: 263.75785866404783, KL divergence: 0.06477411314734965\n",
      "Reconstruction loss: 253.90900572215773, KL divergence: 0.09691338406249167\n",
      "Reconstruction loss: 218.97719829485752, KL divergence: 0.024444813286049416\n",
      "Reconstruction loss: 180.43378177408718, KL divergence: 0.025599292663653905\n",
      "Reconstruction loss: 264.28230820624776, KL divergence: 0.1428678696909531\n",
      "Reconstruction loss: 228.84143680049118, KL divergence: 0.09765059355724276\n",
      "Reconstruction loss: 254.5762302297557, KL divergence: 0.06611703013253256\n",
      "Reconstruction loss: 137.13491544214932, KL divergence: 0.03414953704231777\n",
      "Reconstruction loss: 317.77771646751023, KL divergence: 0.19269164395767036\n",
      "Reconstruction loss: 141.40394076646015, KL divergence: 0.027702746349024365\n",
      "Reconstruction loss: 275.1793179672024, KL divergence: 0.3806210196716465\n",
      "Reconstruction loss: 150.1934346872829, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 196.9201709474724, KL divergence: 0.15742376948279763\n",
      "Reconstruction loss: 232.1496135457191, KL divergence: 0.034407732070736274\n",
      "Reconstruction loss: 146.14654742584264, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 181.9195905900778, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 275.6949317873425, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 162.23607047557212, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 259.23350366459795, KL divergence: 0.5700951456842491\n",
      "Reconstruction loss: 260.9782399211042, KL divergence: 0.264266783001767\n",
      "Reconstruction loss: 144.7376564467191, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 252.49683088715418, KL divergence: 0.5410633870173516\n",
      "Reconstruction loss: 206.05111466227487, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 194.25142961280656, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 202.66980537714875, KL divergence: 0.02658902749231512\n",
      "Reconstruction loss: 196.6407234402242, KL divergence: 0.031372426383760255\n",
      "Reconstruction loss: 177.24322493537176, KL divergence: 0.027116761832041925\n",
      "Reconstruction loss: 320.5268780269972, KL divergence: 0.6790111907254941\n",
      "Reconstruction loss: 277.9011663024431, KL divergence: 0.3029512469652077\n",
      "Reconstruction loss: 133.9508243433138, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 192.0137218674426, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 141.82068210169155, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 284.9492796706852, KL divergence: 0.08313179365094908\n",
      "Reconstruction loss: 165.35764300233927, KL divergence: 0.02916126386014073\n",
      "Reconstruction loss: 197.06544501329194, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 186.94979536363797, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 143.59289969535484, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 315.33569702970016, KL divergence: 0.3881936425780891\n",
      "Reconstruction loss: 189.15442857028464, KL divergence: 0.143771043823595\n",
      "Reconstruction loss: 329.2650395159602, KL divergence: 0.5746549526557048\n",
      "Reconstruction loss: 204.1383493191974, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 175.64354111384057, KL divergence: 0.023974536650027023\n",
      "Reconstruction loss: 177.79347438494688, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 294.38628083428756, KL divergence: 0.27364019070320206\n",
      "Reconstruction loss: 360.3730771189272, KL divergence: 0.4971295828196439\n",
      "Reconstruction loss: 243.41874448104738, KL divergence: 0.2834487680019586\n",
      "Reconstruction loss: 205.48830178837545, KL divergence: 0.037816828171090666\n",
      "Reconstruction loss: 186.38410309873703, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 157.05954517655815, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 190.06048605039433, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 145.08118841669767, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 196.94115636732465, KL divergence: 0.03883968434691604\n",
      "Reconstruction loss: 284.3228059218951, KL divergence: 0.3899324330421199\n",
      "Reconstruction loss: 312.74705677301097, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 181.83158435781425, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 187.9692848369274, KL divergence: 0.028747253174760123\n",
      "Reconstruction loss: 216.91701856599406, KL divergence: 0.028765015150344964\n",
      "Reconstruction loss: 133.6106753162771, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 204.1819013440699, KL divergence: 0.023972097196879205\n",
      "Reconstruction loss: 196.51676368405188, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 219.71993311710168, KL divergence: 0.1504447118517913\n",
      "Reconstruction loss: 230.3758774009849, KL divergence: 0.10014041986343902\n",
      "Reconstruction loss: 387.3094544558601, KL divergence: 1.1150352942373785\n",
      "Reconstruction loss: 350.2849014571126, KL divergence: 0.5490539553161491\n",
      "Reconstruction loss: 295.4768438047132, KL divergence: 0.10715940694680931\n",
      "Reconstruction loss: 311.4694601357621, KL divergence: 0.11102918944233775\n",
      "Reconstruction loss: 194.37141806052404, KL divergence: 0.08852431966473379\n",
      "Reconstruction loss: 247.19257003540392, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 173.47555917138826, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 190.36244634141605, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 225.73244319739837, KL divergence: 0.36795710371795215\n",
      "Reconstruction loss: 177.77395012194424, KL divergence: 0.026393602515678738\n",
      "Reconstruction loss: 218.18662946278607, KL divergence: 0.049485803209983026\n",
      "Reconstruction loss: 173.73370982082463, KL divergence: 0.023844710728547158\n",
      "Reconstruction loss: 174.68019009067976, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 134.60635121285546, KL divergence: 0.029264564538534932\n",
      "Reconstruction loss: 264.0791279687958, KL divergence: 0.5575751398149424\n",
      "Reconstruction loss: 254.05490444767048, KL divergence: 0.02557497502002931\n",
      "Reconstruction loss: 163.75126367931546, KL divergence: 0.02616500893799023\n",
      "Reconstruction loss: 273.8723559188396, KL divergence: 0.7813669750062433\n",
      "Reconstruction loss: 164.57394571688488, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 197.1818085582855, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 199.90560111390494, KL divergence: 0.11739513495702703\n",
      "Reconstruction loss: 236.90533031987883, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 158.984045395605, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 228.47426299190187, KL divergence: 0.12464813952910947\n",
      "Reconstruction loss: 351.96874537736375, KL divergence: 1.2360830957935662\n",
      "Reconstruction loss: 177.82268666656108, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 319.7058856469158, KL divergence: 0.4419242496927891\n",
      "Reconstruction loss: 203.5471132408983, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 244.07717577518088, KL divergence: 0.12439205500869527\n",
      "Reconstruction loss: 301.42281106048154, KL divergence: 0.18711238763177596\n",
      "Reconstruction loss: 193.3134067220692, KL divergence: 0.056489721419047956\n",
      "Reconstruction loss: 249.05355630453528, KL divergence: 0.12822028382586093\n",
      "Reconstruction loss: 165.82469464286578, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 300.95082141808604, KL divergence: 0.8454907784618526\n",
      "Reconstruction loss: 141.35303436517205, KL divergence: 0.02706776366086122\n",
      "Reconstruction loss: 309.8178513604174, KL divergence: 0.364082149348036\n",
      "Reconstruction loss: 238.78808159109045, KL divergence: 0.03855572007565955\n",
      "Reconstruction loss: 307.6292235810056, KL divergence: 1.0759973554521838\n",
      "Reconstruction loss: 214.0951674865642, KL divergence: 0.029797665120277872\n",
      "Reconstruction loss: 199.74336268252088, KL divergence: 0.24930908967223364\n",
      "Reconstruction loss: 271.09590559043846, KL divergence: 0.05276797283355722\n",
      "Reconstruction loss: 167.97462716706423, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 122.17538990952926, KL divergence: 0.03259987410819826\n",
      "Reconstruction loss: 193.93010176042708, KL divergence: 0.023722397695912834\n",
      "Reconstruction loss: 300.215725196001, KL divergence: 0.8373508490199091\n",
      "Reconstruction loss: 150.21108976119564, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 224.59055008945649, KL divergence: 0.13299897980719577\n",
      "Reconstruction loss: 250.56503340034817, KL divergence: 0.16186829506533684\n",
      "Reconstruction loss: 408.8978089167906, KL divergence: 1.4792699425386122\n",
      "Reconstruction loss: 156.81195892944623, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 116.64445469428509, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 245.19362410671516, KL divergence: 0.5019699423471227\n",
      "Reconstruction loss: 220.77217113265718, KL divergence: 0.4317034305350704\n",
      "Reconstruction loss: 223.26769021794098, KL divergence: 0.04655440570236841\n",
      "Reconstruction loss: 199.87743947005313, KL divergence: 0.062204017335851125\n",
      "Reconstruction loss: 196.71542860986472, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 156.3019445955459, KL divergence: 0.029646094668381973\n",
      "Reconstruction loss: 150.23460555825625, KL divergence: 0.0238018786566907\n",
      "Reconstruction loss: 184.22890285709354, KL divergence: 0.02728366415816269\n",
      "Reconstruction loss: 262.8215548147135, KL divergence: 1.7353872206240841\n",
      "Reconstruction loss: 206.0640790171214, KL divergence: 0.4181922353353853\n",
      "Reconstruction loss: 203.43708231202385, KL divergence: 0.17865852986143804\n",
      "Reconstruction loss: 281.3196203408054, KL divergence: 0.1607681617125194\n",
      "Reconstruction loss: 191.71347659122586, KL divergence: 0.02520509826163142\n",
      "Reconstruction loss: 222.03627093177477, KL divergence: 0.1043513978968551\n",
      "Reconstruction loss: 165.32552688476989, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 145.98673896631874, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 191.34599403210672, KL divergence: 0.22001494816076322\n",
      "Reconstruction loss: 233.22048837187373, KL divergence: 0.21031369933297162\n",
      "Reconstruction loss: 360.4534358561208, KL divergence: 1.1000726624007195\n",
      "Reconstruction loss: 154.99136620207628, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 180.78088518837058, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 144.43248751118128, KL divergence: 0.023586304782472756\n",
      "Reconstruction loss: 237.07749576980964, KL divergence: 0.03120496426792896\n",
      "Reconstruction loss: 180.7111419402978, KL divergence: 0.02806996268895695\n",
      "Reconstruction loss: 243.4544412342901, KL divergence: 0.14837215878823817\n",
      "Reconstruction loss: 183.0873784460114, KL divergence: 0.03276631546225617\n",
      "Reconstruction loss: 210.16963072025987, KL divergence: 0.045843076870360266\n",
      "Reconstruction loss: 224.31514951491909, KL divergence: 0.07096631644012774\n",
      "Reconstruction loss: 293.3547691249249, KL divergence: 1.4623349263347891\n",
      "Reconstruction loss: 243.52120418437877, KL divergence: 0.32832353972759243\n",
      "Reconstruction loss: 312.6717488243645, KL divergence: 1.0202462590502985\n",
      "Reconstruction loss: 158.35598904333116, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 260.2853935148858, KL divergence: 0.4329525230896722\n",
      "Reconstruction loss: 222.83182749011738, KL divergence: 0.1393818501290539\n",
      "Reconstruction loss: 156.82207762551025, KL divergence: 0.033357284164209244\n",
      "Reconstruction loss: 215.34712054601584, KL divergence: 0.1946651843155514\n",
      "Reconstruction loss: 194.09523661380052, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 181.20457885407166, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 152.03977231302014, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 177.9932185776833, KL divergence: 0.04762609520631739\n",
      "Reconstruction loss: 207.69499070024125, KL divergence: 0.6285475157615716\n",
      "Reconstruction loss: 295.0270651525482, KL divergence: 0.23092892544936844\n",
      "Reconstruction loss: 240.90739115588073, KL divergence: 0.5322469448657957\n",
      "Reconstruction loss: 163.13695162934567, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 287.009415329288, KL divergence: 0.040437789092301224\n",
      "Reconstruction loss: 217.39437411891828, KL divergence: 0.15947355939312952\n",
      "Reconstruction loss: 210.52773563708678, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 199.9285610874038, KL divergence: 0.05840056483477113\n",
      "Reconstruction loss: 204.4818565652915, KL divergence: 0.04627171801592295\n",
      "Reconstruction loss: 182.690088736787, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 204.76739741908224, KL divergence: 0.27810301648446933\n",
      "Reconstruction loss: 293.8929348699555, KL divergence: 1.2930466212539922\n",
      "Reconstruction loss: 324.71739551813886, KL divergence: 0.837235852529352\n",
      "Reconstruction loss: 203.96944731644186, KL divergence: 0.5166818103721955\n",
      "Reconstruction loss: 169.97940772864413, KL divergence: 0.03420867906001518\n",
      "Reconstruction loss: 316.5542390891806, KL divergence: 1.1878035828325169\n",
      "Reconstruction loss: 195.04154507065283, KL divergence: 0.02339256497793646\n",
      "Reconstruction loss: 190.7552052736747, KL divergence: 0.04112176503472881\n",
      "Reconstruction loss: 151.79164905274382, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 199.77177751479977, KL divergence: 0.23378392557252098\n",
      "Reconstruction loss: 173.21032455039744, KL divergence: 0.09263045095568728\n",
      "Reconstruction loss: 254.13576077835808, KL divergence: 0.030990127029357217\n",
      "Reconstruction loss: 181.24449816345134, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 362.135798270561, KL divergence: 1.367078923044955\n",
      "Reconstruction loss: 204.82884493019665, KL divergence: 0.031187238352584135\n",
      "Reconstruction loss: 251.71439375806497, KL divergence: 0.14517557461919905\n",
      "Reconstruction loss: 231.24841693227478, KL divergence: 0.050353641624727885\n",
      "Reconstruction loss: 119.93485856437616, KL divergence: 0.024207704687926657\n",
      "Reconstruction loss: 178.5471305787337, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 299.7909573441867, KL divergence: 0.25234366384639306\n",
      "Reconstruction loss: 226.817908979219, KL divergence: 0.616099152414874\n",
      "Reconstruction loss: 161.1952493384333, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 166.29728783080355, KL divergence: 0.023223826823834948\n",
      "Reconstruction loss: 204.08494623661863, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 151.53390917320093, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 174.2908832726858, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 120.5340172782946, KL divergence: 0.02323773505913085\n",
      "Reconstruction loss: 221.27146638736014, KL divergence: 0.2831947796247408\n",
      "Reconstruction loss: 215.5123537058853, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 209.4567887422117, KL divergence: 0.025140479852179376\n",
      "Reconstruction loss: 151.07710217360838, KL divergence: 0.03043222770320858\n",
      "Reconstruction loss: 259.82104859447304, KL divergence: 0.32979805868232465\n",
      "Reconstruction loss: 202.88446499260465, KL divergence: 0.08232492836112787\n",
      "Reconstruction loss: 167.5992667369634, KL divergence: 0.026328554000619175\n",
      "Reconstruction loss: 143.89711407828895, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 228.891596664967, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 224.9829884058541, KL divergence: 0.04257935768552279\n",
      "Reconstruction loss: 158.55817194716928, KL divergence: 0.023181338828363873\n",
      "Reconstruction loss: 228.47978015013342, KL divergence: 0.1456642971374626\n",
      "Reconstruction loss: 191.22176314513897, KL divergence: 0.02671367627388188\n",
      "Reconstruction loss: 231.5353095595547, KL divergence: 0.3043625949532183\n",
      "Reconstruction loss: 305.8632835169394, KL divergence: 0.36211897517714564\n",
      "Reconstruction loss: 221.04130349538136, KL divergence: 0.10533121786083371\n",
      "Reconstruction loss: 144.74901499634458, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 150.24613895720853, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 225.64134932741788, KL divergence: 0.023597266983912735\n",
      "Reconstruction loss: 254.57687415934748, KL divergence: 0.05070583075959095\n",
      "Reconstruction loss: 401.0143045827399, KL divergence: 1.3218463204915492\n",
      "Reconstruction loss: 138.89791631306565, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 245.64709051117399, KL divergence: 0.03200181260083057\n",
      "Reconstruction loss: 294.0204226561835, KL divergence: 0.4632674910218507\n",
      "Reconstruction loss: 256.75775193834386, KL divergence: 0.3078853473859138\n",
      "Reconstruction loss: 202.82739062087845, KL divergence: 0.06899758591856492\n",
      "Reconstruction loss: 193.26163644862402, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 213.52991271666377, KL divergence: 0.043855252242450515\n",
      "Reconstruction loss: 266.1943544031509, KL divergence: 0.26641293317299886\n",
      "Reconstruction loss: 275.09336563292095, KL divergence: 0.8417966707013569\n",
      "Reconstruction loss: 152.82661246330414, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 258.9680624549355, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 236.51054445432194, KL divergence: 0.08301991545386495\n",
      "Reconstruction loss: 173.70879671016826, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 283.76415374616715, KL divergence: 0.14556294443229661\n",
      "Reconstruction loss: 345.00697701446154, KL divergence: 1.6706536118092445\n",
      "Reconstruction loss: 309.2428607371196, KL divergence: 0.21325696637076658\n",
      "Reconstruction loss: 314.7195016734313, KL divergence: 0.04732850577569825\n",
      "Reconstruction loss: 173.23713382785309, KL divergence: 0.022913581119548332\n",
      "Reconstruction loss: 251.9856055588239, KL divergence: 0.13963940819224574\n",
      "Reconstruction loss: 216.45709956761277, KL divergence: 0.03157404449195017\n",
      "Reconstruction loss: 275.59043960478493, KL divergence: 1.250247523196808\n",
      "Reconstruction loss: 212.54113026888257, KL divergence: 0.023024358183225957\n",
      "Reconstruction loss: 268.66212162106024, KL divergence: 0.5521673506058081\n",
      "Reconstruction loss: 280.2402587497799, KL divergence: 0.049259737284947536\n",
      "Reconstruction loss: 173.21559204899117, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 143.3283487995625, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 210.35961033705178, KL divergence: 0.030723293102497717\n",
      "Reconstruction loss: 136.62754301219871, KL divergence: 0.028965706082743348\n",
      "Reconstruction loss: 260.07192545977966, KL divergence: 0.042834418158087806\n",
      "Reconstruction loss: 207.54772334104743, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 178.1098303669818, KL divergence: 0.08673283901309792\n",
      "Reconstruction loss: 178.6910708726307, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 363.1820469552084, KL divergence: 0.25306418343397913\n",
      "Reconstruction loss: 193.6861512063861, KL divergence: 0.04136297935023164\n",
      "Reconstruction loss: 152.88520245101586, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 288.5223257559253, KL divergence: 0.07095384222119294\n",
      "Reconstruction loss: 281.55855490298336, KL divergence: 0.033581258153994153\n",
      "Reconstruction loss: 246.67743155305925, KL divergence: 0.06465063329918763\n",
      "Reconstruction loss: 187.10838767344597, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 151.70008044602378, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 230.24131657430974, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 178.62575888069478, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 381.34622207084635, KL divergence: 0.5946921603886044\n",
      "Reconstruction loss: 314.65357775284895, KL divergence: 0.33063025723665623\n",
      "Reconstruction loss: 185.54558150524514, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 301.8457552278668, KL divergence: 0.5436076101438823\n",
      "Reconstruction loss: 320.34241634495066, KL divergence: 0.2764591672310961\n",
      "Reconstruction loss: 374.22454269902624, KL divergence: 0.7703067678274318\n",
      "Reconstruction loss: 146.97008805948013, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 252.2773419508286, KL divergence: 0.3364734725085517\n",
      "Reconstruction loss: 271.30534200698423, KL divergence: 1.0751961356853887\n",
      "Reconstruction loss: 297.18190310005514, KL divergence: 0.2966141946166118\n",
      "Reconstruction loss: 257.1014352807096, KL divergence: 0.11959261360187784\n",
      "Reconstruction loss: 153.6473847922361, KL divergence: 0.02268277197376256\n",
      "Reconstruction loss: 226.86859106042516, KL divergence: 0.09948930132664341\n",
      "Reconstruction loss: 176.56787066352518, KL divergence: 0.09949138804463398\n",
      "Reconstruction loss: 205.62095308208643, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 303.53937437231906, KL divergence: 0.3643688582784386\n",
      "Reconstruction loss: 312.3130858930184, KL divergence: 0.8657639265126114\n",
      "Reconstruction loss: 249.71328809281852, KL divergence: 0.11786437489783791\n",
      "Reconstruction loss: 200.58837553591584, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 197.47291386789098, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 239.87239887551158, KL divergence: 0.04349233783659745\n",
      "Reconstruction loss: 238.92556688629872, KL divergence: 0.3992364853419356\n",
      "Reconstruction loss: 233.2311023583975, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 332.97207649967083, KL divergence: 0.40631871923873647\n",
      "Reconstruction loss: 264.63352368198605, KL divergence: 0.06431628716374505\n",
      "Reconstruction loss: 201.6523594300723, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 193.11759837457828, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 181.02992728119762, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 157.13680322482995, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 240.7301309755499, KL divergence: 0.08584406580135823\n",
      "Reconstruction loss: 210.20526134628952, KL divergence: 0.140114888847008\n",
      "Reconstruction loss: 258.1461868405642, KL divergence: 0.26944399799613716\n",
      "Reconstruction loss: 230.6515131616423, KL divergence: 0.1324671690136474\n",
      "Reconstruction loss: 343.0579128728163, KL divergence: 0.7119016872028069\n",
      "Reconstruction loss: 215.16023348614993, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 263.7189975993644, KL divergence: 0.029592774252767795\n",
      "Reconstruction loss: 263.55482858191306, KL divergence: 0.942707762689684\n",
      "Reconstruction loss: 255.41879911541417, KL divergence: 0.05422930709952267\n",
      "Reconstruction loss: 187.02612571850085, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 195.6445986296745, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 309.95032756184816, KL divergence: 0.5990395838086007\n",
      "Reconstruction loss: 188.55416803383474, KL divergence: 0.023107115133686906\n",
      "Reconstruction loss: 252.65369334994892, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 150.76633245098822, KL divergence: 0.022499555586335263\n",
      "Reconstruction loss: 238.06839577304888, KL divergence: 0.09157108492646199\n",
      "Reconstruction loss: 241.95262653094994, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 243.35470809684003, KL divergence: 0.18105345224738528\n",
      "Reconstruction loss: 217.06099471149886, KL divergence: 0.022780586600246622\n",
      "Reconstruction loss: 187.30095940325202, KL divergence: 0.08103347239253722\n",
      "Reconstruction loss: 179.39761200884868, KL divergence: 0.0433225173146678\n",
      "Reconstruction loss: 148.86100446820726, KL divergence: 0.025959274167035584\n",
      "Reconstruction loss: 189.4955808472176, KL divergence: 0.025193475407913224\n",
      "Reconstruction loss: 277.2325844310961, KL divergence: 0.643451975031301\n",
      "Reconstruction loss: 136.92906765406144, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 198.14570530384884, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 210.72149975226148, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 163.17124963080465, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 328.37071158764957, KL divergence: 0.9348314515333405\n",
      "Reconstruction loss: 176.52888956575305, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 327.15957421154104, KL divergence: 0.1803144945249977\n",
      "Reconstruction loss: 386.33482171885544, KL divergence: 0.040223460389626875\n",
      "Reconstruction loss: 176.62603540335925, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 165.02165830263277, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 168.61855302550256, KL divergence: 0.02370377000899615\n",
      "Reconstruction loss: 170.85074957258382, KL divergence: 0.039536162273905995\n",
      "Reconstruction loss: 171.88482189423138, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 246.49989508458458, KL divergence: 0.029322976280321822\n",
      "Reconstruction loss: 131.55746309050193, KL divergence: 0.022833655570092537\n",
      "Reconstruction loss: 276.4985027520611, KL divergence: 0.1907380948478461\n",
      "Reconstruction loss: 216.218023293986, KL divergence: 0.051329186801247206\n",
      "Reconstruction loss: 242.42281950594258, KL divergence: 0.0794192580195498\n",
      "Reconstruction loss: 165.99993254275452, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 206.74854012125076, KL divergence: 0.06387421441805008\n",
      "Reconstruction loss: 203.4130948990903, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 148.53475123087458, KL divergence: 0.02279889075467162\n",
      "Reconstruction loss: 209.9999636754617, KL divergence: 0.022364408863401575\n",
      "Reconstruction loss: 209.4612690717459, KL divergence: 0.023977672577629006\n",
      "Reconstruction loss: 269.3855573877139, KL divergence: 1.1058152544833002\n",
      "Reconstruction loss: 213.5261943975815, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 126.74217303077003, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 210.498206082129, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 192.70444039878922, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 166.8735863329194, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 184.35458930576198, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 226.25007866230473, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 221.1098205308041, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 157.8662763154765, KL divergence: 0.022998054254259992\n",
      "Reconstruction loss: 214.5732948692984, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 189.1835269202936, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 277.8361823759614, KL divergence: 0.02483960953592751\n",
      "Reconstruction loss: 157.17301042018966, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 265.6841159742271, KL divergence: 0.2393058623950831\n",
      "Reconstruction loss: 154.12786230588952, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 212.15464423509619, KL divergence: 0.038997149577981505\n",
      "Reconstruction loss: 204.4587797902453, KL divergence: 0.10291402503959551\n",
      "Reconstruction loss: 298.0578754813162, KL divergence: 1.0089418234886511\n",
      "Reconstruction loss: 204.55113978856434, KL divergence: 0.026385914040634617\n",
      "Reconstruction loss: 211.99845870618844, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 238.94688301988091, KL divergence: 0.42554961682171494\n",
      "Reconstruction loss: 274.0624671730386, KL divergence: 0.1902448876812552\n",
      "Reconstruction loss: 199.89331905115222, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 159.93131974574408, KL divergence: 0.034306212021598115\n",
      "Reconstruction loss: 190.58954306553161, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 197.37544680560572, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 200.18628077411006, KL divergence: 0.03868417300446492\n",
      "Reconstruction loss: 155.40051639851524, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 257.64397129926886, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 138.05998164904952, KL divergence: 0.04858916861175572\n",
      "Reconstruction loss: 148.1189759774057, KL divergence: 0.022224443604813515\n",
      "Reconstruction loss: 151.1253850744322, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 146.52958973827126, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 240.27291210721376, KL divergence: 0.030182825394892254\n",
      "Reconstruction loss: 162.2394623288581, KL divergence: 0.022535040785166882\n",
      "Reconstruction loss: 166.04724324855817, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 167.83836855065294, KL divergence: 0.02432673380681749\n",
      "Reconstruction loss: 187.45079885509654, KL divergence: 0.025672136030065906\n",
      "Reconstruction loss: 281.1678994736832, KL divergence: 0.7632271963418491\n",
      "Reconstruction loss: 176.30913474702766, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 152.7052966961691, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 185.43701826488197, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 182.45911431297387, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 248.64996898023367, KL divergence: 0.1217688011228224\n",
      "Reconstruction loss: 252.0844477474732, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 192.51963463498527, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 166.41898431449104, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 214.7266530775457, KL divergence: 0.18064552056292504\n",
      "Reconstruction loss: 281.82628701361193, KL divergence: 0.026447080475887286\n",
      "Reconstruction loss: 242.08951130892441, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 143.70587071613733, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 297.1579596480784, KL divergence: 0.19452729658193635\n",
      "Reconstruction loss: 218.43132483704414, KL divergence: 0.02297196975349064\n",
      "Reconstruction loss: 164.19222435244188, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 174.99011380585597, KL divergence: 0.025303420019246903\n",
      "Reconstruction loss: 254.93712325251002, KL divergence: 0.1421952886117137\n",
      "Reconstruction loss: 212.78388089982826, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 330.1404707869964, KL divergence: 0.37972358162177894\n",
      "Reconstruction loss: 186.17309301245857, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 218.111616849776, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 269.436019481295, KL divergence: 0.31596258349946227\n",
      "Reconstruction loss: 279.40444166642465, KL divergence: 0.27586062568873826\n",
      "Reconstruction loss: 147.17188883605093, KL divergence: 0.022068325696105118\n",
      "Reconstruction loss: 157.62384875165782, KL divergence: 0.02936478012329896\n",
      "Reconstruction loss: 279.5053135220745, KL divergence: 0.23466367627641627\n",
      "Reconstruction loss: 230.26290044828374, KL divergence: 0.022310412339597463\n",
      "Reconstruction loss: 213.34046829087404, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 236.10857212479974, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 254.78711790015265, KL divergence: 0.02198511460039365\n",
      "Reconstruction loss: 265.09412982832714, KL divergence: 0.11732548416598909\n",
      "Reconstruction loss: 118.79268720241494, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 200.919537257948, KL divergence: 0.05376674362014339\n",
      "Reconstruction loss: 267.91399636267977, KL divergence: 1.1233364972254805\n",
      "Reconstruction loss: 381.0673162306145, KL divergence: 0.0699146239186399\n",
      "Reconstruction loss: 202.4847480137307, KL divergence: 0.05408561966482617\n",
      "Reconstruction loss: 195.68848349186038, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 237.37632768386462, KL divergence: 0.026067253715803762\n",
      "Reconstruction loss: 208.25153574529486, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 215.23301224135287, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 196.2707705367477, KL divergence: 0.02479111496646158\n",
      "Reconstruction loss: 181.52476547042818, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 238.68582499583675, KL divergence: 0.08297323531470702\n",
      "Reconstruction loss: 180.77906059888704, KL divergence: 0.025882509984974267\n",
      "Reconstruction loss: 216.7555595593228, KL divergence: 0.04198647560523011\n",
      "Reconstruction loss: 242.48855306705292, KL divergence: 0.041155302835389596\n",
      "Reconstruction loss: 247.81046716784624, KL divergence: 0.03303613856091575\n",
      "Reconstruction loss: 164.17527251386878, KL divergence: 0.022684440997448263\n",
      "Reconstruction loss: 217.66730821141675, KL divergence: 0.022186237859252667\n",
      "Reconstruction loss: 206.9545058406014, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 191.13989428789984, KL divergence: 0.021915843412562197\n",
      "Reconstruction loss: 219.04013983275036, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 194.01829175964167, KL divergence: 0.030072880944835234\n",
      "Reconstruction loss: 273.4575132125984, KL divergence: 0.27169509969144545\n",
      "Reconstruction loss: 193.7290403652513, KL divergence: 0.02192464780001374\n",
      "Reconstruction loss: 218.48982549462988, KL divergence: 0.1272596585037737\n",
      "Reconstruction loss: 151.12565858819477, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 239.91023567477717, KL divergence: 0.11220921049158411\n",
      "Reconstruction loss: 255.65161239651914, KL divergence: 0.2134293352442983\n",
      "Reconstruction loss: 215.64088199526938, KL divergence: 0.042594446435374134\n",
      "Reconstruction loss: 146.9276805351716, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 288.70656213214943, KL divergence: 0.5737314749809295\n",
      "Reconstruction loss: 289.23489725506056, KL divergence: 0.11688758546543049\n",
      "Reconstruction loss: 167.98289952718207, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 228.41014545725085, KL divergence: 0.1988662770573011\n",
      "Reconstruction loss: 189.6816423297212, KL divergence: 0.08154465855335963\n",
      "Reconstruction loss: 239.37259543612637, KL divergence: 0.38099465130004356\n",
      "Reconstruction loss: 178.2036803691716, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 212.38460582944904, KL divergence: 0.04764822355658044\n",
      "Reconstruction loss: 268.301767956543, KL divergence: 0.768563407351774\n",
      "Reconstruction loss: 254.35183472629205, KL divergence: 0.26674193067844837\n",
      "Reconstruction loss: 258.16394218859716, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 233.5872269690161, KL divergence: 0.09768564453320688\n",
      "Reconstruction loss: 205.67705712109122, KL divergence: 0.0313766064847909\n",
      "Reconstruction loss: 186.4941905018618, KL divergence: 0.027063566284911023\n",
      "Reconstruction loss: 363.8071916359338, KL divergence: 0.58273322823272\n",
      "Reconstruction loss: 169.53296430594332, KL divergence: 0.02316695960517645\n",
      "Reconstruction loss: 207.41765089749538, KL divergence: 0.029491012553723384\n",
      "Reconstruction loss: 240.23055105594153, KL divergence: 0.22014930990191983\n",
      "Reconstruction loss: 170.6003874485569, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 108.80708744829006, KL divergence: 0.04149164143383505\n",
      "Reconstruction loss: 185.94609565756565, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 224.0180948285988, KL divergence: 0.07008240987484748\n",
      "Reconstruction loss: 269.8199639470675, KL divergence: 0.31314756043683895\n",
      "Reconstruction loss: 184.27751334591986, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 310.4676558667129, KL divergence: 0.18914014989329592\n",
      "Reconstruction loss: 162.2554754110543, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 148.42345055746858, KL divergence: 0.021800311251260085\n",
      "Reconstruction loss: 181.81585565096785, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 155.20929425715593, KL divergence: 0.02172307050401512\n",
      "Reconstruction loss: 197.0841060409607, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 203.26909595294825, KL divergence: 0.024966321694541993\n",
      "Reconstruction loss: 173.18770369842906, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 188.68402176435671, KL divergence: 0.02169494084801421\n",
      "Reconstruction loss: 241.2845696904362, KL divergence: 0.317160791674458\n",
      "Reconstruction loss: 256.97692714494144, KL divergence: 0.2389506835961856\n",
      "Reconstruction loss: 312.505065790819, KL divergence: 0.6638317925124966\n",
      "Reconstruction loss: 218.77291054792863, KL divergence: 0.14137596647260753\n",
      "Reconstruction loss: 204.18102585817184, KL divergence: 0.028291414266482462\n",
      "Reconstruction loss: 155.07436524214825, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 309.6708986874636, KL divergence: 0.3495987103155534\n",
      "Reconstruction loss: 271.26257533864595, KL divergence: 0.03434614913291012\n",
      "Reconstruction loss: 186.2348403500114, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 236.22483391979358, KL divergence: 0.16888162917311417\n",
      "Reconstruction loss: 217.9609825246648, KL divergence: 0.07817742642677061\n",
      "Reconstruction loss: 208.64207072837885, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 226.3539501288397, KL divergence: 0.20985710975577393\n",
      "Reconstruction loss: 313.56558780197224, KL divergence: 0.7051514344837081\n",
      "Reconstruction loss: 368.0054621617975, KL divergence: 0.7793136215164045\n",
      "Reconstruction loss: 203.99424358448198, KL divergence: 0.10871092097864399\n",
      "Reconstruction loss: 197.2852065250521, KL divergence: 0.06783766604304292\n",
      "Reconstruction loss: 409.95065897259565, KL divergence: 0.5224322876898994\n",
      "Reconstruction loss: 212.00524899200474, KL divergence: 0.03882097334557638\n",
      "Reconstruction loss: 229.79783815936037, KL divergence: 0.06156553291382372\n",
      "Reconstruction loss: 306.7326100606582, KL divergence: 0.2785831399311658\n",
      "Reconstruction loss: 172.92949295784268, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 187.700556110783, KL divergence: 0.11653926431800893\n",
      "Reconstruction loss: 300.3985552076965, KL divergence: 0.5382131569082134\n",
      "Reconstruction loss: 262.87534178990495, KL divergence: 0.7185383219462635\n",
      "Reconstruction loss: 144.5967913590302, KL divergence: 0.021630506628946222\n",
      "Reconstruction loss: 316.6959091388142, KL divergence: 0.7290077988318157\n",
      "Reconstruction loss: 216.65641519438998, KL divergence: 0.06970975202023466\n",
      "Reconstruction loss: 256.340329926884, KL divergence: 0.04397937725479478\n",
      "Reconstruction loss: 262.9501861028752, KL divergence: 0.14492822682594153\n",
      "Reconstruction loss: 243.7015443332869, KL divergence: 0.15641246818654542\n",
      "Reconstruction loss: 196.74936567958758, KL divergence: 0.021452727709198816\n",
      "Reconstruction loss: 180.12570401411176, KL divergence: 0.021452727709198816\n",
      "Reconstruction loss: 196.11201280891515, KL divergence: 0.027353754417552723\n",
      "Reconstruction loss: 200.2933277312376, KL divergence: 0.024142600556846583\n",
      "Reconstruction loss: 281.42091256813643, KL divergence: 1.7485201305319653\n",
      "Reconstruction loss: 201.54045390557243, KL divergence: 0.08526581827384344\n",
      "Reconstruction loss: 256.7265306414821, KL divergence: 0.28762399513066056\n",
      "Reconstruction loss: 228.93167915868125, KL divergence: 0.021452727709198816\n",
      "Reconstruction loss: 213.71509141368952, KL divergence: 0.028288665709251815\n",
      "Reconstruction loss: 297.372002578358, KL divergence: 0.4992486691317498\n",
      "Reconstruction loss: 229.14894571733947, KL divergence: 0.03953802268926804\n",
      "Reconstruction loss: 239.30017852664787, KL divergence: 0.3875254670215239\n",
      "Reconstruction loss: 176.31985390416753, KL divergence: 0.021452727709198816\n",
      "Reconstruction loss: 229.95303534342202, KL divergence: 0.11961984524672548\n",
      "Reconstruction loss: 195.71084961135514, KL divergence: 0.11567439089081827\n",
      "Reconstruction loss: 193.09898699906458, KL divergence: 0.021452727709198816\n",
      "Reconstruction loss: 231.1308280829409, KL divergence: 0.03628227898226655\n",
      "Reconstruction loss: 176.34131760671661, KL divergence: 0.021452727709198816\n",
      "Reconstruction loss: 182.882146754024, KL divergence: 0.027566012809971285\n",
      "Reconstruction loss: 212.05054721277605, KL divergence: 0.2513978894492712\n",
      "Reconstruction loss: 250.40169694770907, KL divergence: 0.025189508770591063\n",
      "Reconstruction loss: 252.25542794106144, KL divergence: 0.2095586603543576\n",
      "Reconstruction loss: 357.2122772655098, KL divergence: 2.347555713599488\n",
      "Reconstruction loss: 237.4576482060467, KL divergence: 0.16455718956832666\n",
      "Reconstruction loss: 195.16456241505853, KL divergence: 0.04826020582301227\n",
      "Reconstruction loss: 242.72779936473236, KL divergence: 0.2537344198954067\n",
      "Reconstruction loss: 208.57667197246906, KL divergence: 0.021452727709198816\n",
      "Reconstruction loss: 166.21470572230282, KL divergence: 0.021201091315616538\n",
      "Reconstruction loss: 190.61237757244623, KL divergence: 0.07595682037946239\n",
      "Reconstruction loss: 309.7071179155018, KL divergence: 1.5891346695284985\n",
      "Reconstruction loss: 159.72114419477495, KL divergence: 0.024807925740591152\n",
      "Reconstruction loss: 328.57693387149794, KL divergence: 1.335873927197469\n",
      "Reconstruction loss: 196.6598723485717, KL divergence: 0.12438852656497834\n",
      "Reconstruction loss: 252.04675727523525, KL divergence: 0.16735259454182488\n",
      "Reconstruction loss: 203.91737256333772, KL divergence: 0.0589406326872543\n",
      "Reconstruction loss: 252.32983935818135, KL divergence: 0.34114797885250814\n",
      "Reconstruction loss: 227.20039068725728, KL divergence: 0.21898565295373523\n",
      "Reconstruction loss: 249.5777693175445, KL divergence: 0.0931467714266464\n",
      "Reconstruction loss: 216.5304065225837, KL divergence: 0.24499603068598053\n",
      "Reconstruction loss: 258.07374547014547, KL divergence: 1.2828768439467053\n",
      "Reconstruction loss: 183.6308392622877, KL divergence: 0.04460587277533795\n",
      "Reconstruction loss: 187.01833116655044, KL divergence: 0.12939398049790007\n",
      "Reconstruction loss: 176.22614246533576, KL divergence: 0.025459926673282618\n",
      "Reconstruction loss: 238.21215895645986, KL divergence: 0.2732049967623705\n",
      "Reconstruction loss: 230.7704649518294, KL divergence: 0.4364404713417859\n",
      "Reconstruction loss: 236.49854334335495, KL divergence: 0.17292412551504927\n",
      "Reconstruction loss: 237.17495826846078, KL divergence: 0.17099423542501857\n",
      "Reconstruction loss: 194.87766642893862, KL divergence: 0.2455532234858102\n",
      "Reconstruction loss: 244.42700058664383, KL divergence: 0.028696142294093285\n",
      "Reconstruction loss: 175.79491586633787, KL divergence: 0.021201091315616538\n",
      "Reconstruction loss: 169.9471745440904, KL divergence: 0.03935652684148172\n",
      "Reconstruction loss: 204.38920696760692, KL divergence: 0.14382573624744638\n",
      "Reconstruction loss: 243.3137117873699, KL divergence: 0.18264133741906935\n",
      "Reconstruction loss: 207.28365760751865, KL divergence: 0.0598092231514481\n",
      "Reconstruction loss: 202.00593186716233, KL divergence: 0.3503543772124098\n",
      "Reconstruction loss: 155.72176465247875, KL divergence: 0.021201091315616538\n",
      "Reconstruction loss: 219.40133896781492, KL divergence: 0.17699759366941226\n",
      "Reconstruction loss: 252.13342323709338, KL divergence: 0.07215256699063344\n",
      "Reconstruction loss: 207.61295108059173, KL divergence: 0.030252231901376825\n",
      "Reconstruction loss: 194.79306103374657, KL divergence: 0.041384387340576034\n",
      "Reconstruction loss: 272.91204716930235, KL divergence: 1.2537123280894227\n",
      "Reconstruction loss: 214.2166943820801, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 172.7851989661366, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 241.8359189622482, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 219.6984541811072, KL divergence: 0.03186156976673821\n",
      "Reconstruction loss: 247.0693487653902, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 219.08952051907718, KL divergence: 0.0700519574480058\n",
      "Reconstruction loss: 173.56406909259817, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 235.9506230606508, KL divergence: 0.07160276796530607\n",
      "Reconstruction loss: 211.9877669855943, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 271.1360317735018, KL divergence: 0.02338936705104877\n",
      "Reconstruction loss: 220.44770997783735, KL divergence: 0.04378302224818381\n",
      "Reconstruction loss: 165.58236507550782, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 285.67449534348737, KL divergence: 0.15013071191037036\n",
      "Reconstruction loss: 215.41663765466504, KL divergence: 0.0628708194436347\n",
      "Reconstruction loss: 218.48075888519688, KL divergence: 0.12043353651136934\n",
      "Reconstruction loss: 139.77305320242647, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 291.4556855963463, KL divergence: 1.7668356442587276\n",
      "Reconstruction loss: 145.95867218254182, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 208.20838635099045, KL divergence: 0.48612736789831806\n",
      "Reconstruction loss: 263.38002124036717, KL divergence: 0.10911405152567255\n",
      "Reconstruction loss: 200.01885526923377, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 164.6378212975472, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 255.66029767387886, KL divergence: 0.38503798046904647\n",
      "Reconstruction loss: 312.92157750096885, KL divergence: 0.42516853252502534\n",
      "Reconstruction loss: 220.60630003714283, KL divergence: 0.8795014652313617\n",
      "Reconstruction loss: 262.59256286653317, KL divergence: 0.7867183893671995\n",
      "Reconstruction loss: 200.66595517403283, KL divergence: 0.06771290880484832\n",
      "Reconstruction loss: 201.23324157378283, KL divergence: 0.030174774833812312\n",
      "Reconstruction loss: 292.7360433095737, KL divergence: 0.1403859534740839\n",
      "Reconstruction loss: 180.8996759251575, KL divergence: 0.020937444438232256\n",
      "Reconstruction loss: 203.8974701241189, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 236.96754919365128, KL divergence: 0.18246297428545294\n",
      "Reconstruction loss: 228.4824825029991, KL divergence: 0.5947360941398183\n",
      "Reconstruction loss: 267.91982215239614, KL divergence: 0.2203550293090346\n",
      "Reconstruction loss: 201.40516170781325, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 235.15789222186197, KL divergence: 0.4729940658012597\n",
      "Reconstruction loss: 297.9002669740546, KL divergence: 0.8475625171965478\n",
      "Reconstruction loss: 223.38412412623126, KL divergence: 0.12420679638639381\n",
      "Reconstruction loss: 176.722049794837, KL divergence: 0.025282558840398872\n",
      "Reconstruction loss: 251.51466690088944, KL divergence: 0.2528517960972721\n",
      "Reconstruction loss: 252.45235184387116, KL divergence: 0.32424188368863655\n",
      "Reconstruction loss: 282.07179497308425, KL divergence: 1.4022075234437725\n",
      "Reconstruction loss: 222.52626743182893, KL divergence: 0.12799093607977502\n",
      "Reconstruction loss: 196.11177908586126, KL divergence: 0.057856320731481836\n",
      "Reconstruction loss: 162.42715998068647, KL divergence: 0.024539732903306688\n",
      "Reconstruction loss: 197.0460861481389, KL divergence: 0.02174554571678544\n",
      "Reconstruction loss: 174.79153272992076, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 178.82693111150238, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 318.0094163358642, KL divergence: 1.184345697758972\n",
      "Reconstruction loss: 215.98942625464815, KL divergence: 0.03653976240852913\n",
      "Reconstruction loss: 186.72505368347538, KL divergence: 0.04270880748905498\n",
      "Reconstruction loss: 170.0871127707886, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 296.3139655358413, KL divergence: 0.5746954821291513\n",
      "Reconstruction loss: 181.8034380777524, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 163.96628570550945, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 192.1012016322834, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 214.23360796279053, KL divergence: 0.030880671434279583\n",
      "Reconstruction loss: 231.5053665928422, KL divergence: 0.07175530966999366\n",
      "Reconstruction loss: 175.9415455273222, KL divergence: 0.020694992635572063\n",
      "Reconstruction loss: 184.53257049095976, KL divergence: 0.04169684544946095\n",
      "Reconstruction loss: 259.36500985976096, KL divergence: 0.47707412115890463\n",
      "Reconstruction loss: 210.52045475579567, KL divergence: 0.030387431364530815\n",
      "Reconstruction loss: 152.265378800509, KL divergence: 0.020793824731992427\n",
      "Reconstruction loss: 227.9966557044686, KL divergence: 0.03746440449223093\n",
      "Reconstruction loss: 240.81954033969888, KL divergence: 0.22224321705478506\n",
      "Reconstruction loss: 214.67946417830984, KL divergence: 0.029983711423414605\n",
      "Reconstruction loss: 181.4723851540208, KL divergence: 0.027022380384640998\n",
      "Reconstruction loss: 178.03198902694487, KL divergence: 0.020829283273750054\n",
      "Reconstruction loss: 254.1057168178021, KL divergence: 0.05947452006735504\n",
      "Reconstruction loss: 225.70962086883105, KL divergence: 0.10518971739667005\n",
      "Reconstruction loss: 150.43095496275157, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 157.83963313461396, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 165.72367934995813, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 149.4220880291927, KL divergence: 0.020616359264744977\n",
      "Reconstruction loss: 212.20674312747246, KL divergence: 0.023094215888645464\n",
      "Reconstruction loss: 255.63275327905626, KL divergence: 0.5517522493190345\n",
      "Reconstruction loss: 259.6758097893273, KL divergence: 0.4359869598459259\n",
      "Reconstruction loss: 176.47518255559362, KL divergence: 0.036629122322490326\n",
      "Reconstruction loss: 212.16834959127652, KL divergence: 0.05239283693964525\n",
      "Reconstruction loss: 173.54084767747872, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 243.562255118407, KL divergence: 0.28824608204442986\n",
      "Reconstruction loss: 206.11984452585773, KL divergence: 0.34815124096251726\n",
      "Reconstruction loss: 403.52411316479936, KL divergence: 1.2148114824345995\n",
      "Reconstruction loss: 278.572144425663, KL divergence: 0.10765279448521353\n",
      "Reconstruction loss: 222.5904905101957, KL divergence: 0.046803949479002294\n",
      "Reconstruction loss: 186.0679314656388, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 188.51825410514675, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 149.5519820485991, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 200.5601767223691, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 261.58762564071793, KL divergence: 0.137461433745922\n",
      "Reconstruction loss: 248.4517237483459, KL divergence: 0.15290055003705083\n",
      "Reconstruction loss: 140.59817638620015, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 162.04611949925425, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 166.1694454270462, KL divergence: 0.020454775625444677\n",
      "Reconstruction loss: 283.8627613497457, KL divergence: 0.10254420050066937\n",
      "Reconstruction loss: 224.57516569388326, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 198.80916824097784, KL divergence: 0.23564322008575922\n",
      "Reconstruction loss: 193.50721946317032, KL divergence: 0.023603855528614992\n",
      "Reconstruction loss: 231.96208308974548, KL divergence: 0.262772782636694\n",
      "Reconstruction loss: 222.45939383385584, KL divergence: 0.4663379349830087\n",
      "Reconstruction loss: 196.61549585660117, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 190.32269172138984, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 193.91502580299874, KL divergence: 0.08068654159139871\n",
      "Reconstruction loss: 264.5773953985643, KL divergence: 0.04147449601754932\n",
      "Reconstruction loss: 237.27353879280028, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 267.087107959187, KL divergence: 0.1215179638551872\n",
      "Reconstruction loss: 197.49659963372883, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 293.92723860349406, KL divergence: 0.13162123176617918\n",
      "Reconstruction loss: 203.29940218206062, KL divergence: 0.026594363106706376\n",
      "Reconstruction loss: 221.13721104532902, KL divergence: 0.060363371843813185\n",
      "Reconstruction loss: 229.9746780220102, KL divergence: 0.17343601841053596\n",
      "Reconstruction loss: 127.70121167502083, KL divergence: 0.03944200104992196\n",
      "Reconstruction loss: 183.07318188339804, KL divergence: 0.021244875067726987\n",
      "Reconstruction loss: 169.65913164364628, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 238.33266815009029, KL divergence: 0.06668139092254832\n",
      "Reconstruction loss: 157.43250402416146, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 187.07008517346458, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 302.82890165854167, KL divergence: 0.42914880013205775\n",
      "Reconstruction loss: 205.98652730261603, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 177.79335464796776, KL divergence: 0.02102523293371933\n",
      "Reconstruction loss: 230.80134767617702, KL divergence: 0.024345997764418037\n",
      "Reconstruction loss: 256.98088430972, KL divergence: 0.14178566697735545\n",
      "Reconstruction loss: 142.80028124262157, KL divergence: 0.020246986450706383\n",
      "Reconstruction loss: 290.6140205807042, KL divergence: 0.7672072247558006\n",
      "Reconstruction loss: 196.21282865913975, KL divergence: 0.1907882356792474\n",
      "Reconstruction loss: 234.27149845666528, KL divergence: 0.08272241636807148\n",
      "Reconstruction loss: 289.83717512836705, KL divergence: 0.08884205735312062\n",
      "Reconstruction loss: 205.10298638151096, KL divergence: 0.03377866623188458\n",
      "Reconstruction loss: 260.1348080687856, KL divergence: 0.07587642400533257\n",
      "Reconstruction loss: 280.7135763142872, KL divergence: 0.0666339735346494\n",
      "Reconstruction loss: 163.88579624196086, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 312.62208576461796, KL divergence: 0.7824297913632726\n",
      "Reconstruction loss: 181.86549750223026, KL divergence: 0.02146507669668185\n",
      "Reconstruction loss: 166.6778074821325, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 234.77609359416198, KL divergence: 0.06303957388908071\n",
      "Reconstruction loss: 149.14119943108693, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 284.7136260101659, KL divergence: 0.059953746823464105\n",
      "Reconstruction loss: 322.4936946758628, KL divergence: 0.2209768791781196\n",
      "Reconstruction loss: 211.2900044331166, KL divergence: 0.02103048462515622\n",
      "Reconstruction loss: 290.06776352255963, KL divergence: 0.678749320447941\n",
      "Reconstruction loss: 211.22378591769333, KL divergence: 0.0329484101190054\n",
      "Reconstruction loss: 218.2462638026692, KL divergence: 0.05750821387911581\n",
      "Reconstruction loss: 141.74563901008372, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 250.39773860526054, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 174.10667112804487, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 203.56521727349184, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 275.655262827429, KL divergence: 0.18182291753054602\n",
      "Reconstruction loss: 144.6061416466315, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 308.14846527838824, KL divergence: 0.05698057840579551\n",
      "Reconstruction loss: 151.18718588056643, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 182.50169117095726, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 232.43482599328487, KL divergence: 0.04961888809176518\n",
      "Reconstruction loss: 184.15784335968215, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 350.2851537728707, KL divergence: 1.0877982519071558\n",
      "Reconstruction loss: 178.77043158845458, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 261.65000391821667, KL divergence: 0.21396098255301216\n",
      "Reconstruction loss: 201.4820871132577, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 191.66123557907332, KL divergence: 0.020055905267201135\n",
      "Reconstruction loss: 283.1902962989999, KL divergence: 0.7509958796572123\n",
      "Reconstruction loss: 167.00679375063777, KL divergence: 0.036028590277569716\n",
      "Reconstruction loss: 307.23771657253417, KL divergence: 0.3859557124487477\n",
      "Reconstruction loss: 302.52101490539866, KL divergence: 0.08469245011423926\n",
      "Reconstruction loss: 254.27451720564534, KL divergence: 0.2103706277804121\n",
      "Reconstruction loss: 186.86308666869348, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 207.61876478500267, KL divergence: 0.026612921470334172\n",
      "Reconstruction loss: 154.6448191393715, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 141.77365117017868, KL divergence: 0.026001229181973395\n",
      "Reconstruction loss: 316.3292789051653, KL divergence: 0.37957637804501243\n",
      "Reconstruction loss: 182.78173225139423, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 178.39863444529175, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 231.37724333396488, KL divergence: 0.029932781650580886\n",
      "Reconstruction loss: 214.27744513021656, KL divergence: 0.060973251269604456\n",
      "Reconstruction loss: 201.25264501461425, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 179.97320849954366, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 274.2194543794767, KL divergence: 0.2545993065207855\n",
      "Reconstruction loss: 317.9486112506123, KL divergence: 0.4844637316170087\n",
      "Reconstruction loss: 353.7494268234424, KL divergence: 0.09394130389357924\n",
      "Reconstruction loss: 199.94367008175016, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 231.30602673173956, KL divergence: 0.11083646095179428\n",
      "Reconstruction loss: 183.98970611435692, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 169.7694597982794, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 191.8153955736277, KL divergence: 0.02352073396848192\n",
      "Reconstruction loss: 221.8382663557262, KL divergence: 0.12887325305497627\n",
      "Reconstruction loss: 204.95853906310003, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 315.63553827243067, KL divergence: 0.17209867897932957\n",
      "Reconstruction loss: 170.679221061618, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 246.3462656691845, KL divergence: 0.04540261531211098\n",
      "Reconstruction loss: 178.06590939300415, KL divergence: 0.019828262395034157\n",
      "Reconstruction loss: 194.790126040337, KL divergence: 0.019880594760735726\n",
      "Reconstruction loss: 219.99474510996743, KL divergence: 0.0641775064540211\n",
      "Reconstruction loss: 235.22722071211393, KL divergence: 0.08870049215910669\n",
      "Reconstruction loss: 294.31236136930886, KL divergence: 0.22611469034257908\n",
      "Reconstruction loss: 236.63590318530626, KL divergence: 0.05273449795032931\n",
      "Reconstruction loss: 208.44297940084567, KL divergence: 0.023584417745554753\n",
      "Reconstruction loss: 216.07239187678687, KL divergence: 0.14158702565633768\n",
      "Reconstruction loss: 254.79534057411757, KL divergence: 0.6404573849288544\n",
      "Reconstruction loss: 203.14650715890377, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 250.76664466998824, KL divergence: 0.12567731794654197\n",
      "Reconstruction loss: 188.88741426122255, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 140.85806017215313, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 211.3172820920455, KL divergence: 0.028153016588032875\n",
      "Reconstruction loss: 287.3355750085004, KL divergence: 0.1179753307475307\n",
      "Reconstruction loss: 205.49307013834394, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 152.97312553914705, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 254.8647617577421, KL divergence: 0.16094823520464652\n",
      "Reconstruction loss: 220.81944123774056, KL divergence: 0.06059739791620433\n",
      "Reconstruction loss: 181.99948852909006, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 182.9867766500892, KL divergence: 0.021342844667613325\n",
      "Reconstruction loss: 160.38359345318503, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 187.9259029068972, KL divergence: 0.022666674580308854\n",
      "Reconstruction loss: 285.50224018354766, KL divergence: 0.04565157108656753\n",
      "Reconstruction loss: 157.889754544734, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 235.9754947947442, KL divergence: 0.27042983068350257\n",
      "Reconstruction loss: 173.5160170817203, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 300.4092572418256, KL divergence: 0.22821357390222857\n",
      "Reconstruction loss: 203.68837806989922, KL divergence: 0.02143499887450845\n",
      "Reconstruction loss: 160.9465848763562, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 284.78971058430375, KL divergence: 0.49483632324560906\n",
      "Reconstruction loss: 313.57093224883545, KL divergence: 0.31088934516179634\n",
      "Reconstruction loss: 176.1343043640103, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 193.9029198490632, KL divergence: 0.01972709503719322\n",
      "Reconstruction loss: 322.4361811288568, KL divergence: 0.9217705747160572\n",
      "Reconstruction loss: 251.02681038906897, KL divergence: 0.28881556234366934\n",
      "Reconstruction loss: 177.31597428178654, KL divergence: 0.025131587507460973\n",
      "Reconstruction loss: 195.7708792740442, KL divergence: 0.02098976898914723\n",
      "Reconstruction loss: 207.2260898815541, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 217.5168636057948, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 194.9555863376923, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 259.01403776853215, KL divergence: 0.10229358471834127\n",
      "Reconstruction loss: 140.04995335414364, KL divergence: 0.020197954387023764\n",
      "Reconstruction loss: 252.88877105010852, KL divergence: 0.20935625783240308\n",
      "Reconstruction loss: 261.59643861852305, KL divergence: 0.28731265181719595\n",
      "Reconstruction loss: 263.4698303093716, KL divergence: 0.5832161462583652\n",
      "Reconstruction loss: 188.47073332287025, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 216.80541369308293, KL divergence: 0.04255152121348266\n",
      "Reconstruction loss: 248.4904852625542, KL divergence: 0.34972207365917674\n",
      "Reconstruction loss: 278.2953352195774, KL divergence: 0.020518573346075042\n",
      "Reconstruction loss: 281.0312603956961, KL divergence: 0.019563253077916787\n",
      "Reconstruction loss: 351.7996897643017, KL divergence: 0.12439116774707382\n",
      "Reconstruction loss: 343.61621467012117, KL divergence: 0.27595207433744545\n",
      "Reconstruction loss: 212.93464459764277, KL divergence: 0.033797043730321163\n",
      "Reconstruction loss: 199.91002465953432, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 239.08131594033546, KL divergence: 0.147367884508944\n",
      "Reconstruction loss: 208.4786324661784, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 342.64020565175525, KL divergence: 0.20535108240429517\n",
      "Reconstruction loss: 189.59723283422161, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 217.43296150105425, KL divergence: 0.02552894774635095\n",
      "Reconstruction loss: 225.2222087658693, KL divergence: 0.021629422864749948\n",
      "Reconstruction loss: 223.64851898834297, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 154.3532643402849, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 257.4831380325796, KL divergence: 0.3921805922262164\n",
      "Reconstruction loss: 179.89750450524974, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 181.29838253874334, KL divergence: 0.027179204734215257\n",
      "Reconstruction loss: 156.73492726000632, KL divergence: 0.019562781075983637\n",
      "Reconstruction loss: 164.1821976810716, KL divergence: 0.019411501942991072\n",
      "Reconstruction loss: 248.19772978291482, KL divergence: 0.06085530457467647\n",
      "Reconstruction loss: 192.93372834225505, KL divergence: 0.019411501942991072\n",
      "Reconstruction loss: 258.76596355624804, KL divergence: 0.031202136013606996\n",
      "Reconstruction loss: 222.789894614099, KL divergence: 0.13220975523606615\n",
      "Reconstruction loss: 395.35068529767096, KL divergence: 0.30307109670087584\n",
      "Reconstruction loss: 204.70841012075135, KL divergence: 0.09437910268542082\n",
      "Reconstruction loss: 354.5425134087664, KL divergence: 0.2054209725003296\n",
      "Reconstruction loss: 202.6211114266157, KL divergence: 0.06901471223781058\n",
      "Reconstruction loss: 308.82842985957427, KL divergence: 1.0044662339105228\n",
      "Reconstruction loss: 256.7797912739169, KL divergence: 0.638619323920128\n",
      "Reconstruction loss: 279.9853017330256, KL divergence: 0.49923658757083345\n",
      "Reconstruction loss: 214.82442959763927, KL divergence: 0.04135604097745904\n",
      "Reconstruction loss: 215.61509453370684, KL divergence: 0.05242405218323776\n",
      "Reconstruction loss: 264.7140576364061, KL divergence: 0.21168588060718635\n",
      "Reconstruction loss: 167.26700518006373, KL divergence: 0.030334988271915497\n",
      "Reconstruction loss: 152.6488964231704, KL divergence: 0.019411501942991072\n",
      "Reconstruction loss: 227.6593473887872, KL divergence: 0.052479157878070226\n",
      "Reconstruction loss: 180.6051051757429, KL divergence: 0.019411501942991072\n",
      "Reconstruction loss: 162.84795190242784, KL divergence: 0.019578071253302243\n",
      "Reconstruction loss: 247.4841689647251, KL divergence: 0.12794108837366858\n",
      "Reconstruction loss: 175.053118990013, KL divergence: 0.019411501942991072\n",
      "Reconstruction loss: 225.43984828697575, KL divergence: 0.022201477893696586\n",
      "Reconstruction loss: 226.64819350845838, KL divergence: 0.5408524861705437\n",
      "Reconstruction loss: 212.0282175857713, KL divergence: 0.1769704927443031\n",
      "Reconstruction loss: 156.7256727294308, KL divergence: 0.019411501942991072\n",
      "Reconstruction loss: 226.19655282235811, KL divergence: 0.02767832066102277\n",
      "Reconstruction loss: 229.7368980372865, KL divergence: 0.5228901309213114\n",
      "Reconstruction loss: 147.54531201843815, KL divergence: 0.01959831083174224\n",
      "Reconstruction loss: 225.6010577274402, KL divergence: 0.042596650559119154\n",
      "Reconstruction loss: 173.4499593927281, KL divergence: 0.019411501942991072\n",
      "Reconstruction loss: 278.51282095166107, KL divergence: 0.398490752633022\n",
      "Reconstruction loss: 292.56678762866414, KL divergence: 0.06422151497441425\n",
      "Reconstruction loss: 253.98399587860456, KL divergence: 0.024256624625276668\n",
      "Reconstruction loss: 180.49822854077632, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 246.97229053519632, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 219.17600675834353, KL divergence: 0.022620847482178574\n",
      "Reconstruction loss: 287.1483530755139, KL divergence: 0.6639632889979372\n",
      "Reconstruction loss: 307.21308009773566, KL divergence: 0.31370213730758595\n",
      "Reconstruction loss: 234.3369207162021, KL divergence: 0.26047360447140383\n",
      "Reconstruction loss: 228.8233122860399, KL divergence: 0.0813841200769631\n",
      "Reconstruction loss: 179.24830108279093, KL divergence: 0.021096738702791107\n",
      "Reconstruction loss: 230.2680665952587, KL divergence: 0.08820597522945667\n",
      "Reconstruction loss: 188.79792722271208, KL divergence: 0.06611334380791967\n",
      "Reconstruction loss: 316.9189673311754, KL divergence: 0.46483950070432434\n",
      "Reconstruction loss: 178.16302692570167, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 256.0270604759556, KL divergence: 0.2580571841104456\n",
      "Reconstruction loss: 187.25729109610012, KL divergence: 0.029424096376019226\n",
      "Reconstruction loss: 282.34958158188454, KL divergence: 0.238224868844766\n",
      "Reconstruction loss: 304.24951741713664, KL divergence: 0.8870774229758107\n",
      "Reconstruction loss: 220.86048591545426, KL divergence: 0.17659314806730558\n",
      "Reconstruction loss: 197.50031905831926, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 205.49788482065816, KL divergence: 0.034328980767427686\n",
      "Reconstruction loss: 184.80636237679795, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 184.88408522262768, KL divergence: 0.1948073215539602\n",
      "Reconstruction loss: 246.11472682108325, KL divergence: 0.21827773794794303\n",
      "Reconstruction loss: 153.37547046502522, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 183.09109575953025, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 298.53630645963904, KL divergence: 2.111267254562141\n",
      "Reconstruction loss: 340.60860435505776, KL divergence: 0.3331526289526415\n",
      "Reconstruction loss: 208.85489769584046, KL divergence: 0.03214433163569741\n",
      "Reconstruction loss: 160.68042756944476, KL divergence: 0.019260896478215717\n",
      "Reconstruction loss: 238.65957639493337, KL divergence: 0.15652585228953247\n",
      "Reconstruction loss: 218.2602468104342, KL divergence: 0.10764678299235947\n",
      "Reconstruction loss: 188.55819756581516, KL divergence: 0.02455516817021952\n",
      "Reconstruction loss: 243.16949826902317, KL divergence: 0.14550837239563347\n",
      "Reconstruction loss: 249.77512540490864, KL divergence: 0.8848932833110589\n",
      "Reconstruction loss: 164.38757477003648, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 190.08914509153115, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 225.13895088381872, KL divergence: 0.06282645876281401\n",
      "Reconstruction loss: 202.24688708125095, KL divergence: 0.09102759563215207\n",
      "Reconstruction loss: 158.24790782856212, KL divergence: 0.021716200225152194\n",
      "Reconstruction loss: 160.96078181432267, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 163.03301222406088, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 226.8009416248281, KL divergence: 0.026148818738229218\n",
      "Reconstruction loss: 220.7418994237139, KL divergence: 0.7112446640628239\n",
      "Reconstruction loss: 216.28871937279402, KL divergence: 0.02619477442903334\n",
      "Reconstruction loss: 314.81796075180773, KL divergence: 2.040624717151607\n",
      "Reconstruction loss: 196.24838631513416, KL divergence: 0.022920270618263017\n",
      "Reconstruction loss: 187.4176454583486, KL divergence: 0.019460577394546097\n",
      "Reconstruction loss: 154.9702082104602, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 173.42685963513617, KL divergence: 0.02253345682196989\n",
      "Reconstruction loss: 241.3153398910797, KL divergence: 0.2630916118768781\n",
      "Reconstruction loss: 187.78141755850572, KL divergence: 0.02033004950428219\n",
      "Reconstruction loss: 259.52423113626946, KL divergence: 0.5209059137033594\n",
      "Reconstruction loss: 169.68081259451324, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 163.26263578248592, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 236.73900119811788, KL divergence: 0.029652354035074824\n",
      "Reconstruction loss: 241.3248586275389, KL divergence: 0.18066395756500203\n",
      "Reconstruction loss: 207.00550314488575, KL divergence: 0.023145103828861202\n",
      "Reconstruction loss: 171.67689059732112, KL divergence: 0.020266920546172684\n",
      "Reconstruction loss: 310.1200509756155, KL divergence: 0.7806142219987765\n",
      "Reconstruction loss: 183.76707641168633, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 273.06972983102276, KL divergence: 0.5969702307123117\n",
      "Reconstruction loss: 147.10790696738286, KL divergence: 0.01915077960644207\n",
      "Reconstruction loss: 179.9759624015183, KL divergence: 0.018721539700408518\n",
      "Reconstruction loss: 254.47298496146652, KL divergence: 0.39500716224498916\n",
      "Reconstruction loss: 262.19936228088864, KL divergence: 0.6994387490878496\n",
      "Reconstruction loss: 323.2786373564163, KL divergence: 1.0208607048848188\n",
      "Reconstruction loss: 159.14382857289698, KL divergence: 0.019098608793205185\n",
      "Reconstruction loss: 162.39348114973893, KL divergence: 0.019098608793205185\n",
      "Reconstruction loss: 255.36651515957598, KL divergence: 0.24200421982388637\n",
      "Reconstruction loss: 201.3420612262002, KL divergence: 0.023141261372725375\n",
      "Reconstruction loss: 259.2836067997613, KL divergence: 0.9929549040291167\n",
      "Reconstruction loss: 269.46295897862336, KL divergence: 0.06650762472171406\n",
      "Reconstruction loss: 218.95472281457455, KL divergence: 0.036921046549392544\n",
      "Reconstruction loss: 278.97723393432136, KL divergence: 0.7923524856003129\n",
      "Reconstruction loss: 229.91375948322866, KL divergence: 0.0905938370039116\n",
      "Reconstruction loss: 213.65555764043384, KL divergence: 0.018906731309874858\n",
      "Reconstruction loss: 196.02416263076088, KL divergence: 0.01880235923502005\n",
      "Reconstruction loss: 209.53405200883444, KL divergence: 0.03497026285395011\n",
      "Reconstruction loss: 209.86042164408417, KL divergence: 0.03169771553955214\n",
      "Reconstruction loss: 209.64676205887014, KL divergence: 0.06168868029910335\n",
      "Reconstruction loss: 226.20459832112525, KL divergence: 0.2540259748579934\n",
      "Reconstruction loss: 253.4627872088701, KL divergence: 0.0805495827985031\n",
      "Reconstruction loss: 148.5568048743115, KL divergence: 0.019098608793205185\n",
      "Reconstruction loss: 166.6800192739653, KL divergence: 0.019098608793205185\n",
      "Reconstruction loss: 223.95216965228565, KL divergence: 0.09987094624326037\n",
      "Reconstruction loss: 185.63597354007968, KL divergence: 0.019098608793205185\n",
      "Reconstruction loss: 169.96638748376785, KL divergence: 0.020768991463665842\n",
      "Reconstruction loss: 240.79285197164748, KL divergence: 0.3858950457636255\n",
      "Reconstruction loss: 287.69818183487223, KL divergence: 0.3811571904659206\n",
      "Reconstruction loss: 261.0322993581695, KL divergence: 0.10678428830377856\n",
      "Reconstruction loss: 159.5735445117508, KL divergence: 0.019098608793205185\n",
      "Reconstruction loss: 193.99556640777473, KL divergence: 0.030291216797163456\n",
      "Reconstruction loss: 248.44731706884454, KL divergence: 0.10397397007729636\n",
      "Reconstruction loss: 304.5217566151496, KL divergence: 0.35570645710656634\n",
      "Reconstruction loss: 226.11124127035464, KL divergence: 0.05788306985061914\n",
      "Reconstruction loss: 219.5177411844781, KL divergence: 0.044970776558641345\n",
      "Reconstruction loss: 318.1838008493629, KL divergence: 0.5270516679425825\n",
      "Reconstruction loss: 172.6538704379101, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 295.0507453024716, KL divergence: 1.2882302827412238\n",
      "Reconstruction loss: 162.69270477737018, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 298.9891198580216, KL divergence: 0.4208900760567706\n",
      "Reconstruction loss: 229.81607595438072, KL divergence: 0.13916495025510417\n",
      "Reconstruction loss: 168.11389727390423, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 209.04541118774065, KL divergence: 0.04361482163755609\n",
      "Reconstruction loss: 209.0467998902497, KL divergence: 0.04446517586086646\n",
      "Reconstruction loss: 244.45425904116155, KL divergence: 0.053106040290097245\n",
      "Reconstruction loss: 245.39496328890593, KL divergence: 0.5930749468035674\n",
      "Reconstruction loss: 214.56552994718436, KL divergence: 0.09017851343917765\n",
      "Reconstruction loss: 261.64403585641026, KL divergence: 0.1182612258841812\n",
      "Reconstruction loss: 179.7077671872804, KL divergence: 0.01926865561636093\n",
      "Reconstruction loss: 279.75542373430176, KL divergence: 1.2657619064903867\n",
      "Reconstruction loss: 241.0891280314845, KL divergence: 0.0412916242598928\n",
      "Reconstruction loss: 144.69413108785577, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 258.1169948919685, KL divergence: 0.6820202743068801\n",
      "Reconstruction loss: 147.79758015408743, KL divergence: 0.02925165304056271\n",
      "Reconstruction loss: 236.99508849385944, KL divergence: 0.022190857230957928\n",
      "Reconstruction loss: 261.63027592294435, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 181.37750205770115, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 162.65781352964774, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 192.58252772667413, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 264.99530711051807, KL divergence: 0.6164989765442783\n",
      "Reconstruction loss: 352.9873496228423, KL divergence: 0.05857480895239364\n",
      "Reconstruction loss: 247.5767169849625, KL divergence: 1.1317634665872316\n",
      "Reconstruction loss: 232.09481132086194, KL divergence: 0.08780599140943646\n",
      "Reconstruction loss: 219.2378448499261, KL divergence: 0.04632978158008372\n",
      "Reconstruction loss: 198.12358084385093, KL divergence: 0.043521269993576917\n",
      "Reconstruction loss: 222.74760474468468, KL divergence: 0.01909327336947564\n",
      "Reconstruction loss: 173.19875680628553, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 156.6253138018709, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 266.8129538548353, KL divergence: 0.31688602984029296\n",
      "Reconstruction loss: 244.20008317000563, KL divergence: 0.6707240499780671\n",
      "Reconstruction loss: 210.58003303854744, KL divergence: 0.03058528769074137\n",
      "Reconstruction loss: 201.10080774464217, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 142.45304163945522, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 198.61683900505253, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 280.46583201065715, KL divergence: 0.21207171003621011\n",
      "Reconstruction loss: 285.9788920167214, KL divergence: 0.2798682921347865\n",
      "Reconstruction loss: 227.4240346790619, KL divergence: 0.06797030589898334\n",
      "Reconstruction loss: 246.4310382136049, KL divergence: 0.03980431932526729\n",
      "Reconstruction loss: 206.8110132591252, KL divergence: 0.01880461637100611\n",
      "Reconstruction loss: 161.4335257902921, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 307.00602371670595, KL divergence: 0.2923569296631622\n",
      "Reconstruction loss: 174.68186835465178, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 207.96090168620313, KL divergence: 0.024130806468994292\n",
      "Reconstruction loss: 190.54795067367945, KL divergence: 0.018853159864235913\n",
      "Reconstruction loss: 204.64852844162084, KL divergence: 0.10372812229352668\n",
      "Reconstruction loss: 221.15433471428165, KL divergence: 0.058614447396602154\n",
      "Reconstruction loss: 178.87088940060536, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 188.83090903207832, KL divergence: 0.01881451813539281\n",
      "Reconstruction loss: 229.3497684318192, KL divergence: 0.20512441639753254\n",
      "Reconstruction loss: 194.54315116398575, KL divergence: 0.019121295278729133\n",
      "Reconstruction loss: 326.6734737746207, KL divergence: 0.09838189423580668\n",
      "Reconstruction loss: 162.83531092264207, KL divergence: 0.024340692294071986\n",
      "Reconstruction loss: 196.5631719192641, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 140.75448736336986, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 188.31296756002772, KL divergence: 0.05911182773605983\n",
      "Reconstruction loss: 142.14487290011897, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 192.66548411934258, KL divergence: 0.019845054157517805\n",
      "Reconstruction loss: 178.77869652973592, KL divergence: 0.01911624439442683\n",
      "Reconstruction loss: 253.09571182931833, KL divergence: 0.7056714977215568\n",
      "Reconstruction loss: 238.97380054832115, KL divergence: 0.4197767289194361\n",
      "Reconstruction loss: 279.08479636480854, KL divergence: 0.1179963635885769\n",
      "Reconstruction loss: 277.75378941823783, KL divergence: 0.16482719386150757\n",
      "Reconstruction loss: 233.85914633445014, KL divergence: 0.21777142449938974\n",
      "Reconstruction loss: 197.11303255372962, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 269.77808277034035, KL divergence: 0.27949716171383276\n",
      "Reconstruction loss: 210.84179659361726, KL divergence: 0.0321941834587392\n",
      "Reconstruction loss: 214.49264727803592, KL divergence: 0.019404751550595067\n",
      "Reconstruction loss: 267.21520777089745, KL divergence: 0.08701755047775728\n",
      "Reconstruction loss: 215.92820965814178, KL divergence: 0.04273181862000652\n",
      "Reconstruction loss: 198.25726243695263, KL divergence: 0.041183477451565975\n",
      "Reconstruction loss: 176.23800621957835, KL divergence: 0.02495361887209918\n",
      "Reconstruction loss: 184.86887850954707, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 310.03028968439776, KL divergence: 1.1574401547836857\n",
      "Reconstruction loss: 226.12360103640495, KL divergence: 0.0608433524279719\n",
      "Reconstruction loss: 299.5176488906809, KL divergence: 0.21150536596146996\n",
      "Reconstruction loss: 159.28703599194245, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 171.30566456508114, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 219.33702824953758, KL divergence: 0.04161649553860847\n",
      "Reconstruction loss: 186.30694824880834, KL divergence: 0.01862411737188613\n",
      "Reconstruction loss: 195.8722179754778, KL divergence: 0.019085441925597024\n",
      "Reconstruction loss: 154.71251896080986, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 144.97809132602552, KL divergence: 0.02150021164099808\n",
      "Reconstruction loss: 272.81286645127886, KL divergence: 1.4977592167130531\n",
      "Reconstruction loss: 286.3836199598934, KL divergence: 0.6757995042231874\n",
      "Reconstruction loss: 141.07608688757682, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 163.6503715718539, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 154.66869052338967, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 206.17390014528098, KL divergence: 0.0654197851978427\n",
      "Reconstruction loss: 239.40722712081381, KL divergence: 0.0908438055063363\n",
      "Reconstruction loss: 165.90695405107638, KL divergence: 0.019191446428356906\n",
      "Reconstruction loss: 189.0292092103343, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 203.8292824411169, KL divergence: 0.020423795141548684\n",
      "Reconstruction loss: 188.26156911435157, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 175.09382952363842, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 185.61643832780044, KL divergence: 0.019168332957637535\n",
      "Reconstruction loss: 271.2671681535131, KL divergence: 0.17289604795143004\n",
      "Reconstruction loss: 215.27666590135865, KL divergence: 0.16131173344181798\n",
      "Reconstruction loss: 203.4270533116489, KL divergence: 0.018916979729934447\n",
      "Reconstruction loss: 128.06010628828153, KL divergence: 0.03510668835459446\n",
      "Reconstruction loss: 267.6493877408167, KL divergence: 0.2170185905527247\n",
      "Reconstruction loss: 189.31026378183788, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 205.4721694323731, KL divergence: 0.020049714047744915\n",
      "Reconstruction loss: 188.35944349537402, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 287.18241685944815, KL divergence: 0.57100549511396\n",
      "Reconstruction loss: 159.96212121032926, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 180.7381410703116, KL divergence: 0.032481888111644486\n",
      "Reconstruction loss: 312.7182032918377, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 240.50004521079788, KL divergence: 0.021241420852988135\n",
      "Reconstruction loss: 218.53006563110566, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 278.00534243927655, KL divergence: 0.35195173785886397\n",
      "Reconstruction loss: 128.15735177973886, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 256.165176855367, KL divergence: 0.11146450560299132\n",
      "Reconstruction loss: 180.73030673764433, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 173.79188080629598, KL divergence: 0.023077840330224586\n",
      "Reconstruction loss: 154.63051827388156, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 358.2423122015, KL divergence: 0.23595287376066731\n",
      "Reconstruction loss: 299.73416051532047, KL divergence: 0.1199251689102494\n",
      "Reconstruction loss: 282.363862732475, KL divergence: 0.6316466182681141\n",
      "Reconstruction loss: 360.83160336981314, KL divergence: 2.090602832628349\n",
      "Reconstruction loss: 170.72397106316026, KL divergence: 0.01846084443162993\n",
      "Reconstruction loss: 151.81328947637593, KL divergence: 0.01931717606700789\n",
      "Reconstruction loss: 171.292399552902, KL divergence: 0.018501674381910582\n",
      "Reconstruction loss: 162.91500911407297, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 211.32686652651512, KL divergence: 0.019432916140078138\n",
      "Reconstruction loss: 200.82215188143695, KL divergence: 0.018737790951237454\n",
      "Reconstruction loss: 232.66440966344692, KL divergence: 0.09991625833888451\n",
      "Reconstruction loss: 250.95465685066108, KL divergence: 0.04084225501362104\n",
      "Reconstruction loss: 294.5856364184441, KL divergence: 0.9825429346721055\n",
      "Reconstruction loss: 261.0971445270887, KL divergence: 0.23674870635409606\n",
      "Reconstruction loss: 281.8281860767767, KL divergence: 0.4133980915221956\n",
      "Reconstruction loss: 216.39114793569502, KL divergence: 0.03220431351422737\n",
      "Reconstruction loss: 177.9875413732371, KL divergence: 0.021649231530868118\n",
      "Reconstruction loss: 145.16434097161692, KL divergence: 0.042589340259877095\n",
      "Reconstruction loss: 176.2348849283904, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 189.89623789759992, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 210.84226283132034, KL divergence: 0.021002656160844235\n",
      "Reconstruction loss: 242.49693454591124, KL divergence: 0.46432330487935336\n",
      "Reconstruction loss: 201.68104658033675, KL divergence: 0.022077517268290603\n",
      "Reconstruction loss: 218.07728527405524, KL divergence: 0.0278921296128557\n",
      "Reconstruction loss: 206.17924200213744, KL divergence: 0.02379121650296112\n",
      "Reconstruction loss: 160.01236311735096, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 201.5833056974895, KL divergence: 0.09778227080840912\n",
      "Reconstruction loss: 174.50526943295984, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 169.69918013445636, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 126.44449776523936, KL divergence: 0.03112394364158977\n",
      "Reconstruction loss: 178.5441908148089, KL divergence: 0.0270262984555264\n",
      "Reconstruction loss: 230.377283629704, KL divergence: 0.20614164747022085\n",
      "Reconstruction loss: 276.7845781488082, KL divergence: 0.3239990374456963\n",
      "Reconstruction loss: 145.24690717452597, KL divergence: 0.01995115308531692\n",
      "Reconstruction loss: 206.03991570740902, KL divergence: 0.018708018316916042\n",
      "Reconstruction loss: 192.54844665139996, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 288.05327504714495, KL divergence: 0.6506078689579295\n",
      "Reconstruction loss: 188.35277731340608, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 166.1703021602734, KL divergence: 0.019439283872388857\n",
      "Reconstruction loss: 251.4173959082333, KL divergence: 0.2829741473651173\n",
      "Reconstruction loss: 228.02136951183337, KL divergence: 0.019204343995218642\n",
      "Reconstruction loss: 283.0820599011856, KL divergence: 1.2369764890020356\n",
      "Reconstruction loss: 145.461590204669, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 215.13146531818956, KL divergence: 0.019438633278127004\n",
      "Reconstruction loss: 182.37547040290715, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 260.39803516322087, KL divergence: 0.4203332485491725\n",
      "Reconstruction loss: 195.26233642576494, KL divergence: 0.11285943331800091\n",
      "Reconstruction loss: 189.47240131400966, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 272.084835115149, KL divergence: 0.2183278930002907\n",
      "Reconstruction loss: 246.20833412146095, KL divergence: 0.2600413470760633\n",
      "Reconstruction loss: 188.25770332196151, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 162.23335587459053, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 256.6725579971916, KL divergence: 0.17824470310597446\n",
      "Reconstruction loss: 426.9370007748432, KL divergence: 0.31327634597060633\n",
      "Reconstruction loss: 192.5830354625104, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 201.4787665101521, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 247.60577856186885, KL divergence: 0.21109256080415018\n",
      "Reconstruction loss: 273.16379614038306, KL divergence: 0.3199935909723352\n",
      "Reconstruction loss: 179.05505184340626, KL divergence: 0.01865455025173146\n",
      "Reconstruction loss: 167.2902588462308, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 340.617103885417, KL divergence: 1.6396359771918734\n",
      "Reconstruction loss: 205.79882109168494, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 215.77646314546098, KL divergence: 0.08284359411220887\n",
      "Reconstruction loss: 140.96690349229533, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 128.84210752059315, KL divergence: 0.019584732754055645\n",
      "Reconstruction loss: 300.2167287420737, KL divergence: 0.5864190157218818\n",
      "Reconstruction loss: 401.86326775006273, KL divergence: 0.4943186529960861\n",
      "Reconstruction loss: 252.5972766752427, KL divergence: 0.04495056877496184\n",
      "Reconstruction loss: 186.35706733321126, KL divergence: 0.0185707292368964\n",
      "Reconstruction loss: 169.41836934998418, KL divergence: 0.02966468647412762\n",
      "Reconstruction loss: 185.28036831200393, KL divergence: 0.01954864737418599\n",
      "Reconstruction loss: 230.72495812959568, KL divergence: 0.10399951394074286\n",
      "Reconstruction loss: 228.91611934609148, KL divergence: 0.2840965294850844\n",
      "Reconstruction loss: 187.2438774788063, KL divergence: 0.03182343934963344\n",
      "Reconstruction loss: 154.23168224857613, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 271.74432296212444, KL divergence: 0.03291008277728402\n",
      "Reconstruction loss: 217.7866410033663, KL divergence: 0.018580130916085236\n",
      "Reconstruction loss: 243.72861453695998, KL divergence: 0.35286492144001963\n",
      "Reconstruction loss: 166.4616090850679, KL divergence: 0.01853791285847478\n",
      "Reconstruction loss: 267.9338241417639, KL divergence: 0.39802570492685774\n",
      "Reconstruction loss: 309.0059442757424, KL divergence: 0.32360870104945966\n",
      "Reconstruction loss: 136.79666131059025, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 245.48033772452536, KL divergence: 0.09614353473900344\n",
      "Reconstruction loss: 297.9892299194651, KL divergence: 0.05209234350472419\n",
      "Reconstruction loss: 217.87676088719314, KL divergence: 0.0520117990485488\n",
      "Reconstruction loss: 220.34037781564663, KL divergence: 0.13531100009571922\n",
      "Reconstruction loss: 267.36060047047846, KL divergence: 0.14087847505908152\n",
      "Reconstruction loss: 210.4942467758632, KL divergence: 0.02586569714407766\n",
      "Reconstruction loss: 252.66901034752135, KL divergence: 0.23476057361845082\n",
      "Reconstruction loss: 200.38359664226346, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 202.91185181990517, KL divergence: 0.13705733137364168\n",
      "Reconstruction loss: 150.72331759280343, KL divergence: 0.019967578944601994\n",
      "Reconstruction loss: 231.17377678012676, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 182.0450884698411, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 210.50720712748864, KL divergence: 0.052385505750672634\n",
      "Reconstruction loss: 161.18379301026428, KL divergence: 0.019426517671444044\n",
      "Reconstruction loss: 143.6015694850019, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 171.05277098110676, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 235.76985560204844, KL divergence: 0.3475554571207624\n",
      "Reconstruction loss: 210.123342967228, KL divergence: 0.11368600578623206\n",
      "Reconstruction loss: 226.8009819320427, KL divergence: 0.07572026820380767\n",
      "Reconstruction loss: 236.47154046585842, KL divergence: 0.2515752005492213\n",
      "Reconstruction loss: 169.31135400713697, KL divergence: 0.019636939984590207\n",
      "Reconstruction loss: 383.0800712061501, KL divergence: 1.6904463306796123\n",
      "Reconstruction loss: 203.4997463934722, KL divergence: 0.05748393626935633\n",
      "Reconstruction loss: 209.75083770855662, KL divergence: 0.311711649389986\n",
      "Reconstruction loss: 300.1463315590998, KL divergence: 0.3590052242125038\n",
      "Reconstruction loss: 234.65396910864905, KL divergence: 0.021379376264599292\n",
      "Reconstruction loss: 214.6577349351308, KL divergence: 0.18699623862975667\n",
      "Reconstruction loss: 301.91720532820136, KL divergence: 0.956802730772992\n",
      "Reconstruction loss: 232.9170366313901, KL divergence: 0.40227166398489717\n",
      "Reconstruction loss: 191.4958945655274, KL divergence: 0.019736173229420118\n",
      "Reconstruction loss: 203.6555334813286, KL divergence: 0.02803683243336086\n",
      "Reconstruction loss: 149.9563498795245, KL divergence: 0.019736173229420118\n",
      "Reconstruction loss: 216.32191543573967, KL divergence: 0.22479735461916556\n",
      "Reconstruction loss: 203.15289547538202, KL divergence: 0.028235587983008237\n",
      "Reconstruction loss: 257.2262815712664, KL divergence: 0.4820219415621579\n",
      "Reconstruction loss: 229.55729773597582, KL divergence: 0.17090444894225254\n",
      "Reconstruction loss: 211.6694544401917, KL divergence: 0.05158801787025802\n",
      "Reconstruction loss: 276.2712464074522, KL divergence: 0.16788574123018674\n",
      "Reconstruction loss: 223.17519135154362, KL divergence: 0.01881176371071902\n",
      "Reconstruction loss: 298.54302639505045, KL divergence: 0.927457771817547\n",
      "Reconstruction loss: 161.31457706254972, KL divergence: 0.019736173229420118\n",
      "Reconstruction loss: 186.8521917033491, KL divergence: 0.035882706197366776\n",
      "Reconstruction loss: 260.696281751059, KL divergence: 0.24955175864465462\n",
      "Reconstruction loss: 127.39093173158668, KL divergence: 0.019736173229420118\n",
      "Reconstruction loss: 171.2496280083205, KL divergence: 0.03245555095203845\n",
      "Reconstruction loss: 174.95409337907597, KL divergence: 0.019736173229420118\n",
      "Reconstruction loss: 198.13991434051724, KL divergence: 0.04504325776961027\n",
      "Reconstruction loss: 256.5534511740356, KL divergence: 0.2132114432090405\n",
      "Reconstruction loss: 181.95264154885982, KL divergence: 0.01886425181412671\n",
      "Reconstruction loss: 243.95591063738155, KL divergence: 0.34719661406757196\n",
      "Reconstruction loss: 131.0947757318147, KL divergence: 0.025845544183451086\n",
      "Reconstruction loss: 192.12553564080116, KL divergence: 0.018235874489847037\n",
      "Reconstruction loss: 189.85927566832032, KL divergence: 0.023210603616802605\n",
      "Reconstruction loss: 202.4622704010322, KL divergence: 0.15771071123862546\n",
      "Reconstruction loss: 279.69402609209044, KL divergence: 1.1316835082600294\n",
      "Reconstruction loss: 286.0290461638307, KL divergence: 0.029269126158489367\n",
      "Reconstruction loss: 275.16573373841936, KL divergence: 0.162917130331739\n",
      "Reconstruction loss: 209.89533285686292, KL divergence: 0.10058002879932099\n",
      "Reconstruction loss: 299.33033939032197, KL divergence: 1.6795294915064591\n",
      "Reconstruction loss: 171.42872926741944, KL divergence: 0.01871417615121479\n",
      "Reconstruction loss: 329.98901285214157, KL divergence: 0.9174692408782472\n",
      "Reconstruction loss: 197.38113086559503, KL divergence: 0.01986387547854951\n",
      "Reconstruction loss: 221.00735613776874, KL divergence: 0.08368855360020389\n",
      "Reconstruction loss: 191.56121400030347, KL divergence: 0.04903389728528962\n",
      "Reconstruction loss: 200.5850758962535, KL divergence: 0.024828978535560986\n",
      "Reconstruction loss: 146.30084285168803, KL divergence: 0.01986379334882382\n",
      "Reconstruction loss: 328.4783128481521, KL divergence: 0.369775370586763\n",
      "Reconstruction loss: 240.86815531862447, KL divergence: 0.5640279874903578\n",
      "Reconstruction loss: 241.8379800921158, KL divergence: 0.3113363729213993\n",
      "Reconstruction loss: 158.16173715384565, KL divergence: 0.01986387547854951\n",
      "Reconstruction loss: 184.44952304063642, KL divergence: 0.020169393213856024\n",
      "Reconstruction loss: 177.3847628962294, KL divergence: 0.01986387547854951\n",
      "Reconstruction loss: 245.57423644188145, KL divergence: 0.66682235230455\n",
      "Reconstruction loss: 143.09579819156386, KL divergence: 0.01986387547854951\n",
      "Reconstruction loss: 164.00090713970854, KL divergence: 0.01986387547854951\n",
      "Reconstruction loss: 189.96357444415764, KL divergence: 0.018827356457965017\n",
      "Reconstruction loss: 264.1248663308705, KL divergence: 0.19287456645847956\n",
      "Reconstruction loss: 170.62456033830279, KL divergence: 0.01986387547854951\n",
      "Reconstruction loss: 187.0268799497398, KL divergence: 0.09167042326779296\n",
      "Reconstruction loss: 236.43469140882388, KL divergence: 0.7176366820321107\n",
      "Reconstruction loss: 123.55213957147717, KL divergence: 0.0396629680324444\n",
      "Reconstruction loss: 184.50807869372238, KL divergence: 0.07132938861954868\n",
      "Reconstruction loss: 137.199334089314, KL divergence: 0.030219060028399802\n",
      "Reconstruction loss: 240.08410439909912, KL divergence: 0.11955066692204608\n",
      "Reconstruction loss: 176.24256616220802, KL divergence: 0.028308059907138916\n",
      "Reconstruction loss: 174.21564684076236, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 160.91694369011304, KL divergence: 0.01858330014273979\n",
      "Reconstruction loss: 280.97336513775576, KL divergence: 0.9978782804875836\n",
      "Reconstruction loss: 257.3605933840919, KL divergence: 0.6856239153287161\n",
      "Reconstruction loss: 198.5061023490724, KL divergence: 0.17723150782788122\n",
      "Reconstruction loss: 201.0507464981909, KL divergence: 0.030011544787766575\n",
      "Reconstruction loss: 204.66816686368642, KL divergence: 0.019084485369372906\n",
      "Reconstruction loss: 151.99678525649784, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 208.47277020020124, KL divergence: 0.1549068718008056\n",
      "Reconstruction loss: 275.2782000885302, KL divergence: 0.8166567332464276\n",
      "Reconstruction loss: 181.67247943540517, KL divergence: 0.05072950259707676\n",
      "Reconstruction loss: 177.94137971614947, KL divergence: 0.048729711341824244\n",
      "Reconstruction loss: 191.35489207498014, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 157.46737971783188, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 150.02859391468814, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 238.43704942869772, KL divergence: 0.0318795200668508\n",
      "Reconstruction loss: 194.31595925104472, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 186.9522059039249, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 232.1682466357936, KL divergence: 0.2679176978589857\n",
      "Reconstruction loss: 152.29694961206343, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 240.46298680447705, KL divergence: 0.4849300421114253\n",
      "Reconstruction loss: 292.80762893779115, KL divergence: 1.7855323811214552\n",
      "Reconstruction loss: 249.29962813716998, KL divergence: 0.29828591742528765\n",
      "Reconstruction loss: 209.3962478594913, KL divergence: 0.10511806014874586\n",
      "Reconstruction loss: 176.15322942707277, KL divergence: 0.01785908439276157\n",
      "Reconstruction loss: 277.5927680832091, KL divergence: 0.7255041904474635\n",
      "Reconstruction loss: 348.4751089215622, KL divergence: 1.279459916366379\n",
      "Reconstruction loss: 192.7316766119058, KL divergence: 0.020017543920548164\n",
      "Reconstruction loss: 259.13652944941697, KL divergence: 0.36893240239612374\n",
      "Reconstruction loss: 177.08636478807966, KL divergence: 0.063684563737256\n",
      "Reconstruction loss: 187.03783955816903, KL divergence: 0.02197185041677957\n",
      "Reconstruction loss: 236.85625642786033, KL divergence: 0.18413259037623003\n",
      "Reconstruction loss: 244.71822027064096, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 217.71290930997142, KL divergence: 0.1784599035369323\n",
      "Reconstruction loss: 208.9464836059417, KL divergence: 0.141078395187006\n",
      "Reconstruction loss: 165.1583484620349, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 235.11559455117043, KL divergence: 0.2031709300071589\n",
      "Reconstruction loss: 238.33428186709054, KL divergence: 0.20128819396988729\n",
      "Reconstruction loss: 187.73646354131893, KL divergence: 0.018179123226334803\n",
      "Reconstruction loss: 201.48785106401127, KL divergence: 0.2220731029169033\n",
      "Reconstruction loss: 184.95111308279436, KL divergence: 0.04354111730927751\n",
      "Reconstruction loss: 265.7419773434341, KL divergence: 1.187306708806989\n",
      "Reconstruction loss: 130.94266750404717, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 276.4516767615047, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 234.01269018306618, KL divergence: 0.4627166947102732\n",
      "Reconstruction loss: 234.62685448121067, KL divergence: 0.291532495531381\n",
      "Reconstruction loss: 165.92103172299082, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 148.23200360874193, KL divergence: 0.018920467531751584\n",
      "Reconstruction loss: 274.57458537114576, KL divergence: 0.0962097429627023\n",
      "Reconstruction loss: 281.4219459147576, KL divergence: 0.8092292246538193\n",
      "Reconstruction loss: 275.6706056883878, KL divergence: 0.7601327107267781\n",
      "Reconstruction loss: 206.18189439597182, KL divergence: 0.11526481002071931\n",
      "Reconstruction loss: 294.51392247149784, KL divergence: 1.1704448604229287\n",
      "Reconstruction loss: 178.04947650975785, KL divergence: 0.03588798416182293\n",
      "Reconstruction loss: 293.15908120554093, KL divergence: 0.4726822687734754\n",
      "Reconstruction loss: 191.38224906369663, KL divergence: 0.02805355970300638\n",
      "Reconstruction loss: 214.66258225430894, KL divergence: 0.18549034519317809\n",
      "Reconstruction loss: 154.57672316003277, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 173.54918423821601, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 229.23483711599823, KL divergence: 0.36264685453883133\n",
      "Reconstruction loss: 166.89828038238232, KL divergence: 0.020188997872348124\n",
      "Reconstruction loss: 206.21965870735966, KL divergence: 0.20237016486672704\n",
      "Reconstruction loss: 231.02930874888634, KL divergence: 0.3832920060874469\n",
      "Reconstruction loss: 264.68197967132824, KL divergence: 0.4708746572741061\n",
      "Reconstruction loss: 230.96650454262948, KL divergence: 0.1629464172177703\n",
      "Reconstruction loss: 153.72178647330117, KL divergence: 0.018429084897156334\n",
      "Reconstruction loss: 207.77920426759664, KL divergence: 0.042118557056504136\n",
      "Reconstruction loss: 215.17698580125793, KL divergence: 0.6037461717860104\n",
      "Reconstruction loss: 118.3809597248466, KL divergence: 0.031217779650380695\n",
      "Reconstruction loss: 239.50893650336863, KL divergence: 0.07480550741351777\n",
      "Reconstruction loss: 229.552671057962, KL divergence: 0.29727811351683847\n",
      "Reconstruction loss: 174.5792762662789, KL divergence: 0.0204067948532064\n",
      "Reconstruction loss: 245.68454994583686, KL divergence: 0.27437704978156047\n",
      "Reconstruction loss: 295.61641563542287, KL divergence: 1.2920842824646657\n",
      "Reconstruction loss: 265.3207849703804, KL divergence: 1.5117255147858317\n",
      "Reconstruction loss: 197.3602592275563, KL divergence: 0.22667476567434913\n",
      "Reconstruction loss: 276.55722609197426, KL divergence: 0.8984638193413301\n",
      "Reconstruction loss: 186.0211838485885, KL divergence: 0.017743387000431876\n",
      "Reconstruction loss: 298.00591251637667, KL divergence: 0.8469757766711585\n",
      "Reconstruction loss: 173.9954451787341, KL divergence: 0.02423054030591243\n",
      "Reconstruction loss: 205.12110357956328, KL divergence: 0.05255109728559282\n",
      "Reconstruction loss: 204.17143775873134, KL divergence: 0.07379482575448126\n",
      "Reconstruction loss: 283.5193162103997, KL divergence: 2.2603103647327627\n",
      "Reconstruction loss: 274.98698563201754, KL divergence: 1.3008496758864827\n",
      "Reconstruction loss: 192.63129563075455, KL divergence: 0.04599485032905226\n",
      "Reconstruction loss: 208.48678790311345, KL divergence: 0.28389017994011\n",
      "Reconstruction loss: 325.90381268894424, KL divergence: 1.2569337229304396\n",
      "Reconstruction loss: 158.53842917373316, KL divergence: 0.0204067948532064\n",
      "Reconstruction loss: 201.75513905280775, KL divergence: 0.09756715261802429\n",
      "Reconstruction loss: 241.37043867479184, KL divergence: 1.43256754547103\n",
      "Reconstruction loss: 152.9728803476211, KL divergence: 0.0204067948532064\n",
      "Reconstruction loss: 166.61599794270742, KL divergence: 0.01860096243566428\n",
      "Reconstruction loss: 171.36879414032026, KL divergence: 0.0204067948532064\n",
      "Reconstruction loss: 171.51996758666127, KL divergence: 0.0204067948532064\n",
      "Reconstruction loss: 160.67163405015165, KL divergence: 0.0204067948532064\n",
      "Reconstruction loss: 180.9248746589454, KL divergence: 0.019964559617802247\n",
      "Reconstruction loss: 129.79472185993365, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 177.2863656292139, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 127.5204906347694, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 248.1262732885069, KL divergence: 0.2289964803900303\n",
      "Reconstruction loss: 168.03244595841403, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 207.41695152158786, KL divergence: 0.017348509183594263\n",
      "Reconstruction loss: 159.86561186048533, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 169.02561492950593, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 138.59448020619948, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 186.79843528522298, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 303.99745032833124, KL divergence: 0.2970678680086466\n",
      "Reconstruction loss: 162.11241399113604, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 187.81317138903196, KL divergence: 0.29577286713432566\n",
      "Reconstruction loss: 220.6575367237836, KL divergence: 0.08239931855947308\n",
      "Reconstruction loss: 304.28798932501195, KL divergence: 0.48618967377304007\n",
      "Reconstruction loss: 127.87018354831876, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 261.6607374137877, KL divergence: 0.06717204929220744\n",
      "Reconstruction loss: 215.73098158918953, KL divergence: 0.039872273750448495\n",
      "Reconstruction loss: 267.7384847994565, KL divergence: 0.5855906207220094\n",
      "Reconstruction loss: 202.3965908860191, KL divergence: 0.10685744153789983\n",
      "Reconstruction loss: 234.03427203769223, KL divergence: 0.5465424679255448\n",
      "Reconstruction loss: 155.12259191766066, KL divergence: 0.01893290270239717\n",
      "Reconstruction loss: 204.51903861996175, KL divergence: 0.12246770498188575\n",
      "Reconstruction loss: 177.20520117465884, KL divergence: 0.02032389087274533\n",
      "Reconstruction loss: 248.9176265135317, KL divergence: 0.41908843180048216\n",
      "Reconstruction loss: 199.59030098627653, KL divergence: 0.07556288747176187\n",
      "Reconstruction loss: 146.0882083584208, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 157.7559132021948, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 171.75780389520776, KL divergence: 0.017357616263870224\n",
      "Reconstruction loss: 151.73075588921083, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 167.71297268065243, KL divergence: 0.020658329362110206\n",
      "Reconstruction loss: 246.7449115797857, KL divergence: 0.3422221825187202\n",
      "Reconstruction loss: 151.9984498790419, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 131.94314741902616, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 285.937553709424, KL divergence: 0.17238244575319694\n",
      "Reconstruction loss: 214.73521450215682, KL divergence: 0.07823155742514637\n",
      "Reconstruction loss: 166.72851817029573, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 210.96528299102508, KL divergence: 0.01987677468332416\n",
      "Reconstruction loss: 257.64154536475417, KL divergence: 0.43747776429321866\n",
      "Reconstruction loss: 185.90018856881423, KL divergence: 0.02581591221382168\n",
      "Reconstruction loss: 197.05214005352184, KL divergence: 0.017769134872486825\n",
      "Reconstruction loss: 244.27764616944106, KL divergence: 0.2546611773042932\n",
      "Reconstruction loss: 282.60138140502636, KL divergence: 0.11357522910055007\n",
      "Reconstruction loss: 192.30277355026632, KL divergence: 0.11016547160748014\n",
      "Reconstruction loss: 180.27132857962212, KL divergence: 0.042223103034076004\n",
      "Reconstruction loss: 200.69353384308647, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 150.59475799773764, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 163.39553244639143, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 367.8119509151446, KL divergence: 0.3739076726149385\n",
      "Reconstruction loss: 212.8533030606468, KL divergence: 0.035375142309094\n",
      "Reconstruction loss: 190.6722908258277, KL divergence: 0.03425137191906108\n",
      "Reconstruction loss: 205.45809229572887, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 208.76638910818042, KL divergence: 0.029483145677631106\n",
      "Reconstruction loss: 260.12340208250146, KL divergence: 1.0271700557303665\n",
      "Reconstruction loss: 181.52172467896074, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 264.8137054339606, KL divergence: 0.07436059282925711\n",
      "Reconstruction loss: 151.77374696762075, KL divergence: 0.0209614115529852\n",
      "Reconstruction loss: 374.04116863451026, KL divergence: 1.6529582223699997\n",
      "Reconstruction loss: 179.9130719449959, KL divergence: 0.07186718537928116\n",
      "Reconstruction loss: 233.05047972358017, KL divergence: 0.19614249293562275\n",
      "Reconstruction loss: 196.55516142764597, KL divergence: 0.09387464901232945\n",
      "Reconstruction loss: 370.890902994636, KL divergence: 1.7131740578239252\n",
      "Reconstruction loss: 231.44010657044822, KL divergence: 0.3626899176312701\n",
      "Reconstruction loss: 300.36697517351564, KL divergence: 0.7317566928548915\n",
      "Reconstruction loss: 174.41223344333935, KL divergence: 0.01752758931947479\n",
      "Reconstruction loss: 140.0021218426631, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 193.96083036828958, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 280.983130837417, KL divergence: 0.2939490492922594\n",
      "Reconstruction loss: 183.4501017379863, KL divergence: 0.02176887066675076\n",
      "Reconstruction loss: 224.03154207726067, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 163.61500825857345, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 151.48168778034034, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 166.19538508573254, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 167.38985104163612, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 164.87207200674266, KL divergence: 0.02359043662148269\n",
      "Reconstruction loss: 299.70441495814225, KL divergence: 0.7223452629570467\n",
      "Reconstruction loss: 256.0503064416457, KL divergence: 1.6045621542716932\n",
      "Reconstruction loss: 260.5525265786576, KL divergence: 0.11768370562833919\n",
      "Reconstruction loss: 161.00122462622872, KL divergence: 0.024362596866472686\n",
      "Reconstruction loss: 144.22393923997356, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 198.53671328345385, KL divergence: 0.021154686798210753\n",
      "Reconstruction loss: 123.67207641924084, KL divergence: 0.04964630675321208\n",
      "Reconstruction loss: 330.2674922312958, KL divergence: 1.7055443450705896\n",
      "Reconstruction loss: 226.89852608138935, KL divergence: 0.100385574886589\n",
      "Reconstruction loss: 145.0506075276702, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 225.3120231126645, KL divergence: 0.028507270002073037\n",
      "Reconstruction loss: 183.20253538381854, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 236.32829485909713, KL divergence: 0.33587102004403896\n",
      "Reconstruction loss: 258.17352979599536, KL divergence: 0.026013828625734425\n",
      "Reconstruction loss: 282.7845314927482, KL divergence: 1.364265231168081\n",
      "Reconstruction loss: 212.75893227434446, KL divergence: 0.023679804712846697\n",
      "Reconstruction loss: 179.57640484200093, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 169.0277346559112, KL divergence: 0.021316614529337585\n",
      "Reconstruction loss: 235.60285267608958, KL divergence: 0.10919748806169322\n",
      "Reconstruction loss: 335.4731813301879, KL divergence: 1.4297187510033527\n",
      "Reconstruction loss: 203.42459584130836, KL divergence: 0.03163833748920791\n",
      "Reconstruction loss: 170.76863393553117, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 226.63348183809387, KL divergence: 0.3862864515213446\n",
      "Reconstruction loss: 217.41657433692092, KL divergence: 0.11819593812672391\n",
      "Reconstruction loss: 137.07728190053945, KL divergence: 0.0665820147657718\n",
      "Reconstruction loss: 165.48156565954434, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 222.40384515344255, KL divergence: 0.12328580255270877\n",
      "Reconstruction loss: 162.54748868275092, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 203.27116872743568, KL divergence: 0.017180453534607565\n",
      "Reconstruction loss: 215.82358670327795, KL divergence: 0.056027828368383104\n",
      "Reconstruction loss: 223.81726292842416, KL divergence: 0.09248050231572241\n",
      "Reconstruction loss: 206.45890788666247, KL divergence: 0.11784083196804318\n",
      "Reconstruction loss: 243.75448080601572, KL divergence: 0.16037630756729232\n",
      "Reconstruction loss: 188.00547978481285, KL divergence: 0.029804652143536614\n",
      "Reconstruction loss: 209.48376318946526, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 237.21495877153154, KL divergence: 0.3102525465859872\n",
      "Reconstruction loss: 202.48149475884858, KL divergence: 0.07200756144483639\n",
      "Reconstruction loss: 215.0328261893212, KL divergence: 0.2967333004261604\n",
      "Reconstruction loss: 177.04557860579536, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 162.60024388396454, KL divergence: 0.01946659228516079\n",
      "Reconstruction loss: 147.56746592111713, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 260.2517299832941, KL divergence: 0.0864102828311985\n",
      "Reconstruction loss: 198.6634704749124, KL divergence: 0.01745891281867562\n",
      "Reconstruction loss: 243.45563694734528, KL divergence: 1.0517801893707808\n",
      "Reconstruction loss: 156.9313875160892, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 159.38683757968113, KL divergence: 0.021688201678087493\n",
      "Reconstruction loss: 180.50996332709767, KL divergence: 0.11671782192796198\n",
      "Reconstruction loss: 222.43502061802167, KL divergence: 0.06892812522779862\n",
      "Reconstruction loss: 327.43463850909836, KL divergence: 1.3043899142241502\n",
      "Reconstruction loss: 204.2261284017803, KL divergence: 0.03788764223099972\n",
      "Reconstruction loss: 308.22577568869167, KL divergence: 2.517596330893144\n",
      "Reconstruction loss: 187.474882478355, KL divergence: 0.02515265012181983\n",
      "Reconstruction loss: 128.73478086821194, KL divergence: 0.03584686586936192\n",
      "Reconstruction loss: 186.129469504538, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 198.56273334144893, KL divergence: 0.06396349702636023\n",
      "Reconstruction loss: 125.09409505598768, KL divergence: 0.045657367777614366\n",
      "Reconstruction loss: 222.0427560862643, KL divergence: 0.26076077169945944\n",
      "Reconstruction loss: 232.96524573477183, KL divergence: 0.10956277790801155\n",
      "Reconstruction loss: 249.71334810433592, KL divergence: 0.3070544392298491\n",
      "Reconstruction loss: 193.48769917283838, KL divergence: 0.023132823105888378\n",
      "Reconstruction loss: 231.9140663211038, KL divergence: 0.3155707806720764\n",
      "Reconstruction loss: 203.9591740314998, KL divergence: 0.14662041249777996\n",
      "Reconstruction loss: 192.18486889835015, KL divergence: 0.04134790252774717\n",
      "Reconstruction loss: 219.8689010086816, KL divergence: 0.14639487741001977\n",
      "Reconstruction loss: 168.63684185462594, KL divergence: 0.020452368734296178\n",
      "Reconstruction loss: 164.35802435572418, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 155.81218095685387, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 273.42744827269377, KL divergence: 2.1700977668293744\n",
      "Reconstruction loss: 245.37942093629775, KL divergence: 0.2381928499234261\n",
      "Reconstruction loss: 172.92360172226887, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 205.33483870606642, KL divergence: 0.11061931089033256\n",
      "Reconstruction loss: 160.4327772734909, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 219.6940586191359, KL divergence: 0.23082264448855339\n",
      "Reconstruction loss: 197.12391433829487, KL divergence: 0.017700710874802905\n",
      "Reconstruction loss: 240.5492894024125, KL divergence: 0.438071286601907\n",
      "Reconstruction loss: 307.9253422479206, KL divergence: 0.21809538381097776\n",
      "Reconstruction loss: 182.77357051827, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 188.51581270206646, KL divergence: 0.019271643004318606\n",
      "Reconstruction loss: 169.67576311750716, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 254.1000027223451, KL divergence: 0.7980623896374806\n",
      "Reconstruction loss: 169.5825088887571, KL divergence: 0.027691212499841578\n",
      "Reconstruction loss: 148.61954359694153, KL divergence: 0.025430392371238508\n",
      "Reconstruction loss: 172.1418231757197, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 258.97814408839497, KL divergence: 1.6299106860004073\n",
      "Reconstruction loss: 183.36149399819422, KL divergence: 0.022119144802557722\n",
      "Reconstruction loss: 239.40961083464083, KL divergence: 0.01949336002428731\n",
      "Reconstruction loss: 158.28177007364025, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 185.82006838759736, KL divergence: 0.0853093579486151\n",
      "Reconstruction loss: 209.14476506176084, KL divergence: 0.19581455867649766\n",
      "Reconstruction loss: 147.56549714489145, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 156.18414060973404, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 198.14971412914403, KL divergence: 0.0288845384353143\n",
      "Reconstruction loss: 317.20018493103464, KL divergence: 2.2932052555911255\n",
      "Reconstruction loss: 247.4965483620898, KL divergence: 0.7406326014207603\n",
      "Reconstruction loss: 284.4645010925049, KL divergence: 0.09209066997105775\n",
      "Reconstruction loss: 201.04251140132462, KL divergence: 0.022792604018080476\n",
      "Reconstruction loss: 238.47538172726587, KL divergence: 0.13591602477512799\n",
      "Reconstruction loss: 168.90247116406027, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 262.6207572279842, KL divergence: 0.455534092544239\n",
      "Reconstruction loss: 203.08371240966687, KL divergence: 0.01716567813117864\n",
      "Reconstruction loss: 131.7211030195426, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 206.75210719023585, KL divergence: 0.029665266519673383\n",
      "Reconstruction loss: 290.8353483327726, KL divergence: 0.2688785475803995\n",
      "Reconstruction loss: 223.64959816230314, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 218.9730971588152, KL divergence: 0.07388613208601463\n",
      "Reconstruction loss: 255.47298234068302, KL divergence: 0.5118102761021164\n",
      "Reconstruction loss: 221.1352596787928, KL divergence: 0.1317016534649918\n",
      "Reconstruction loss: 174.1016581908295, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 219.60457490077454, KL divergence: 0.03033025144085738\n",
      "Reconstruction loss: 319.020073516284, KL divergence: 2.1727831477438193\n",
      "Reconstruction loss: 275.32764293570847, KL divergence: 0.6080494265423875\n",
      "Reconstruction loss: 192.97184222071232, KL divergence: 0.058635843946174715\n",
      "Reconstruction loss: 169.22705701074776, KL divergence: 0.016852173505800205\n",
      "Reconstruction loss: 127.26717071597214, KL divergence: 0.0542681672931708\n",
      "Reconstruction loss: 177.05947740884403, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 135.9602487933754, KL divergence: 0.026842707566223756\n",
      "Reconstruction loss: 195.87075546676422, KL divergence: 0.022572509171723987\n",
      "Reconstruction loss: 210.4567420411437, KL divergence: 0.023106257104037298\n",
      "Reconstruction loss: 198.11060960732254, KL divergence: 0.13155319117329056\n",
      "Reconstruction loss: 233.06680220908703, KL divergence: 0.3130242355446709\n",
      "Reconstruction loss: 277.4768510447626, KL divergence: 0.018994635550784567\n",
      "Reconstruction loss: 228.95247479995277, KL divergence: 0.506734922129662\n",
      "Reconstruction loss: 190.1207859392428, KL divergence: 0.026599169088802666\n",
      "Reconstruction loss: 314.26303571142034, KL divergence: 1.1687952447555714\n",
      "Reconstruction loss: 204.47690723388922, KL divergence: 0.10585785192486119\n",
      "Reconstruction loss: 311.46071845453616, KL divergence: 0.5487096353883321\n",
      "Reconstruction loss: 281.00400279468954, KL divergence: 1.2942399152073238\n",
      "Reconstruction loss: 205.56270621396868, KL divergence: 0.03861821377361985\n",
      "Reconstruction loss: 213.17353665947076, KL divergence: 0.06949400820623075\n",
      "Reconstruction loss: 137.0809604517092, KL divergence: 0.05234417444011957\n",
      "Reconstruction loss: 163.95887791709583, KL divergence: 0.024448682822889367\n",
      "Reconstruction loss: 213.5860703971787, KL divergence: 0.6281472802054957\n",
      "Reconstruction loss: 201.06941003625047, KL divergence: 0.023106257104037298\n",
      "Reconstruction loss: 174.33216371166318, KL divergence: 0.03295187615982853\n",
      "Reconstruction loss: 318.374366850996, KL divergence: 3.7344652888002066\n",
      "Reconstruction loss: 154.24753121167439, KL divergence: 0.1013258090992894\n",
      "Reconstruction loss: 216.12159891100217, KL divergence: 0.07716788847543787\n",
      "Reconstruction loss: 178.07295539698782, KL divergence: 0.023106257104037298\n",
      "Reconstruction loss: 271.53666195470873, KL divergence: 0.9466907359578771\n",
      "Reconstruction loss: 250.7609785435489, KL divergence: 0.08243098781477026\n",
      "Reconstruction loss: 129.57843273748824, KL divergence: 0.03404298759755642\n",
      "Reconstruction loss: 256.2574593390491, KL divergence: 0.4042859090938795\n",
      "Reconstruction loss: 268.1461779155434, KL divergence: 0.023106257104037298\n",
      "Reconstruction loss: 209.1428064897691, KL divergence: 0.019448846326067804\n",
      "Reconstruction loss: 261.95456881188653, KL divergence: 0.6815710690702823\n",
      "Reconstruction loss: 231.71218454969193, KL divergence: 0.15805936548976196\n",
      "Reconstruction loss: 171.60312024906193, KL divergence: 0.024468576569575795\n",
      "Reconstruction loss: 205.9334989142669, KL divergence: 0.09947535086827047\n",
      "Reconstruction loss: 229.4882177177338, KL divergence: 0.018220780313712104\n",
      "Reconstruction loss: 214.0553020385109, KL divergence: 0.02374070330689637\n",
      "Reconstruction loss: 216.09009271625064, KL divergence: 0.16565073528578367\n",
      "Reconstruction loss: 230.36437208296837, KL divergence: 0.2274648906282447\n",
      "Reconstruction loss: 245.5671652878746, KL divergence: 0.19630791649584511\n",
      "Reconstruction loss: 300.39068205468703, KL divergence: 0.02374070330689637\n",
      "Reconstruction loss: 151.02180457103415, KL divergence: 0.02374070330689637\n",
      "Reconstruction loss: 179.23381446799414, KL divergence: 0.023208335976744376\n",
      "Reconstruction loss: 346.6490583897877, KL divergence: 3.876488657412057\n",
      "Reconstruction loss: 196.89754687649167, KL divergence: 0.032752500587865274\n",
      "Reconstruction loss: 280.06991901276297, KL divergence: 1.1916255333495065\n",
      "Reconstruction loss: 245.22162080987522, KL divergence: 0.3689797263313624\n",
      "Reconstruction loss: 139.52396805816198, KL divergence: 0.02374070330689637\n",
      "Reconstruction loss: 196.04418329074045, KL divergence: 0.06519430171675855\n",
      "Reconstruction loss: 218.86753033304822, KL divergence: 0.030062196203742564\n",
      "Reconstruction loss: 200.57068817946788, KL divergence: 0.13710985462534336\n",
      "Reconstruction loss: 235.9379573973843, KL divergence: 0.14137486110805192\n",
      "Reconstruction loss: 213.2921639964436, KL divergence: 0.0489658823013886\n",
      "Reconstruction loss: 162.5541812709916, KL divergence: 0.02374070330689637\n",
      "Reconstruction loss: 210.6179531550984, KL divergence: 0.23040644462081916\n",
      "Reconstruction loss: 207.68752608516894, KL divergence: 0.024648174562966474\n",
      "Reconstruction loss: 188.2741694337596, KL divergence: 0.017178125541458533\n",
      "Reconstruction loss: 284.63091182601744, KL divergence: 1.4085252649855289\n",
      "Reconstruction loss: 198.82447930020368, KL divergence: 0.06606173209757304\n",
      "Reconstruction loss: 282.5847514918146, KL divergence: 0.8151582563831421\n",
      "Reconstruction loss: 269.85478000781956, KL divergence: 1.8113432290187772\n",
      "Reconstruction loss: 141.50171578486353, KL divergence: 0.027771942902000912\n",
      "Reconstruction loss: 238.87476192731697, KL divergence: 0.06021572083926224\n",
      "Reconstruction loss: 167.28583862704045, KL divergence: 0.02374070330689637\n",
      "Reconstruction loss: 212.9632763436119, KL divergence: 0.02374070330689637\n",
      "Reconstruction loss: 234.43400962524106, KL divergence: 0.042502545549328086\n",
      "Reconstruction loss: 243.31407488392563, KL divergence: 0.042017825256321306\n",
      "Reconstruction loss: 189.22529227426594, KL divergence: 0.03129586134107759\n",
      "Reconstruction loss: 145.11268837821427, KL divergence: 0.04702442659972994\n",
      "Reconstruction loss: 155.89189287320514, KL divergence: 0.023798603351458225\n",
      "Reconstruction loss: 183.0883037629915, KL divergence: 0.01701150094440851\n",
      "Reconstruction loss: 191.07008882511101, KL divergence: 0.08584179384626567\n",
      "Reconstruction loss: 207.33558586830765, KL divergence: 0.020979913030810704\n",
      "Reconstruction loss: 197.97836284185246, KL divergence: 0.11420976732480531\n",
      "Reconstruction loss: 164.868844820042, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 214.52991628925832, KL divergence: 0.16490852512360987\n",
      "Reconstruction loss: 189.32248779430722, KL divergence: 0.11533092787899685\n",
      "Reconstruction loss: 244.59251482510712, KL divergence: 1.6667904264737066\n",
      "Reconstruction loss: 175.12839775929785, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 158.60101994993, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 171.83378694351057, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 224.92787226940032, KL divergence: 0.023943459818442037\n",
      "Reconstruction loss: 176.29791869218627, KL divergence: 0.016505964477538437\n",
      "Reconstruction loss: 296.3644401107467, KL divergence: 0.35870728524816897\n",
      "Reconstruction loss: 282.66755656751167, KL divergence: 1.0936559299633266\n",
      "Reconstruction loss: 183.75321694140004, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 166.60861139148244, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 417.67770486004895, KL divergence: 4.955822712507265\n",
      "Reconstruction loss: 174.0624128675127, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 212.4476096984008, KL divergence: 0.025258977003742134\n",
      "Reconstruction loss: 295.9807544379665, KL divergence: 0.8883545486951769\n",
      "Reconstruction loss: 173.44565177920492, KL divergence: 0.024365115799341808\n",
      "Reconstruction loss: 188.82567767122865, KL divergence: 0.01840168757649363\n",
      "Reconstruction loss: 223.22101404040495, KL divergence: 0.030568672009292253\n",
      "Reconstruction loss: 186.72637993574733, KL divergence: 0.02150840731513659\n",
      "Reconstruction loss: 267.71361472466447, KL divergence: 0.4072811998967851\n",
      "Reconstruction loss: 138.32062065434326, KL divergence: 0.033850908103304655\n",
      "Reconstruction loss: 142.0380136174146, KL divergence: 0.02802030733034544\n",
      "Reconstruction loss: 380.62761173882615, KL divergence: 1.4785486323526855\n",
      "Reconstruction loss: 220.09958680001068, KL divergence: 0.017013008192648782\n",
      "Reconstruction loss: 208.5657707329122, KL divergence: 0.10185229476917662\n",
      "Reconstruction loss: 222.51398168977633, KL divergence: 0.17680838816796235\n",
      "Reconstruction loss: 148.17569870994845, KL divergence: 0.07515519448900226\n",
      "Reconstruction loss: 132.97289034053813, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 269.7997081146907, KL divergence: 0.5596650905591134\n",
      "Reconstruction loss: 176.72859550872084, KL divergence: 0.01711216824805961\n",
      "Reconstruction loss: 207.39310564685798, KL divergence: 0.042258925083570154\n",
      "Reconstruction loss: 265.7012448350003, KL divergence: 0.23842043105066196\n",
      "Reconstruction loss: 231.70104859669203, KL divergence: 0.09566736902110368\n",
      "Reconstruction loss: 193.8019986976467, KL divergence: 0.03570286397947403\n",
      "Reconstruction loss: 278.23736581582136, KL divergence: 0.8492429142559587\n",
      "Reconstruction loss: 239.16711295075348, KL divergence: 0.4813725304562902\n",
      "Reconstruction loss: 191.0268000067517, KL divergence: 0.021863871000151636\n",
      "Reconstruction loss: 238.7653572433809, KL divergence: 1.1797556502961104\n",
      "Reconstruction loss: 213.55063102506273, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 164.2211935117457, KL divergence: 0.1204451353621731\n",
      "Reconstruction loss: 202.41242777277483, KL divergence: 0.1267192737863176\n",
      "Reconstruction loss: 230.95582870658092, KL divergence: 0.8689128502711092\n",
      "Reconstruction loss: 281.31104088279676, KL divergence: 0.024981393032692978\n",
      "Reconstruction loss: 220.24372200359142, KL divergence: 0.08015038992779272\n",
      "Reconstruction loss: 154.7231548085888, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 181.78467273548694, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 188.9234624029955, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 158.0010943837497, KL divergence: 0.04100489157038528\n",
      "Reconstruction loss: 224.56399403322285, KL divergence: 0.12809720378085981\n",
      "Reconstruction loss: 219.98095818647423, KL divergence: 0.09343608547725957\n",
      "Reconstruction loss: 156.70747328007423, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 174.64357145776495, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 293.6778159669626, KL divergence: 0.48186958164225957\n",
      "Reconstruction loss: 292.77743747225657, KL divergence: 0.06494583238638213\n",
      "Reconstruction loss: 159.39077526056343, KL divergence: 0.025021305164050267\n",
      "Reconstruction loss: 232.62899667862592, KL divergence: 0.07377273294374836\n",
      "Reconstruction loss: 183.2801795289639, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 168.5573265363663, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 204.3709227878836, KL divergence: 0.3876246310787811\n",
      "Reconstruction loss: 189.01300807197848, KL divergence: 0.0183111162611167\n",
      "Reconstruction loss: 246.96286109317026, KL divergence: 0.4772563483698567\n",
      "Reconstruction loss: 222.84065289284987, KL divergence: 0.5001269655595766\n",
      "Reconstruction loss: 174.95334833712423, KL divergence: 0.04141300140625487\n",
      "Reconstruction loss: 362.3369804043615, KL divergence: 1.4173593504729605\n",
      "Reconstruction loss: 229.84930670178733, KL divergence: 0.4592840719122618\n",
      "Reconstruction loss: 185.5833711687859, KL divergence: 0.019320031952773653\n",
      "Reconstruction loss: 223.46253766729205, KL divergence: 0.04641742666646259\n",
      "Reconstruction loss: 131.39389587571776, KL divergence: 0.03838679556632668\n",
      "Reconstruction loss: 284.6888385911983, KL divergence: 1.2157645647748399\n",
      "Reconstruction loss: 178.7958208614561, KL divergence: 0.021348193391493797\n",
      "Reconstruction loss: 192.78827348260626, KL divergence: 0.17496465977319392\n",
      "Reconstruction loss: 205.97192134366045, KL divergence: 0.10963816413334043\n",
      "Reconstruction loss: 195.20566507346365, KL divergence: 0.27722170192846163\n",
      "Reconstruction loss: 305.117390754425, KL divergence: 0.7839810071273263\n",
      "Reconstruction loss: 207.35622941252618, KL divergence: 0.04359696525196688\n",
      "Reconstruction loss: 184.77656441159417, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 198.39495491580487, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 191.84284040356056, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 395.48569942685396, KL divergence: 1.3451946312291356\n",
      "Reconstruction loss: 160.48979736655144, KL divergence: 0.017261396818865016\n",
      "Reconstruction loss: 142.6798895967185, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 186.32256545979712, KL divergence: 0.032424159450832823\n",
      "Reconstruction loss: 192.1331490545825, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 316.110999831233, KL divergence: 1.5134701787000715\n",
      "Reconstruction loss: 160.06031697032768, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 189.22607750911908, KL divergence: 0.029552729620423435\n",
      "Reconstruction loss: 233.12438385584048, KL divergence: 1.085161632937473\n",
      "Reconstruction loss: 172.40389222706966, KL divergence: 0.025665571816171684\n",
      "Reconstruction loss: 224.498071833131, KL divergence: 0.02641926120066801\n",
      "Reconstruction loss: 284.26599883698907, KL divergence: 0.8417465505213868\n",
      "Reconstruction loss: 184.7723231354118, KL divergence: 0.01722712468614218\n",
      "Reconstruction loss: 194.31757409939013, KL divergence: 0.05955503514526367\n",
      "Reconstruction loss: 157.90197866969066, KL divergence: 0.023372354181376453\n",
      "Reconstruction loss: 192.78806951397493, KL divergence: 0.023588451355521622\n",
      "Reconstruction loss: 142.48649109754692, KL divergence: 0.02641926120066801\n",
      "Reconstruction loss: 162.45020508561043, KL divergence: 0.02641926120066801\n",
      "Reconstruction loss: 206.36882585823076, KL divergence: 0.08339568873509895\n",
      "Reconstruction loss: 205.7665630615269, KL divergence: 0.09224935660313249\n",
      "Reconstruction loss: 174.72003528349325, KL divergence: 0.19733162669928855\n",
      "Reconstruction loss: 177.17188398031482, KL divergence: 0.02641926120066801\n",
      "Reconstruction loss: 188.60354366202375, KL divergence: 0.02266963632360336\n",
      "Reconstruction loss: 247.1029676060616, KL divergence: 0.02641926120066801\n",
      "Reconstruction loss: 154.82744603480035, KL divergence: 0.02641926120066801\n",
      "Reconstruction loss: 261.9528141848694, KL divergence: 1.1876585352990527\n",
      "Reconstruction loss: 324.7061638250575, KL divergence: 3.884316490268335\n",
      "Reconstruction loss: 278.7471223076812, KL divergence: 1.7075337540103446\n",
      "Reconstruction loss: 157.8716516396468, KL divergence: 0.05395110031202077\n",
      "Reconstruction loss: 250.43942603121042, KL divergence: 1.539205757064647\n",
      "Reconstruction loss: 372.354900097451, KL divergence: 0.42932285634683454\n",
      "Reconstruction loss: 308.76965558849633, KL divergence: 1.4233482031155391\n",
      "Reconstruction loss: 255.66065646107006, KL divergence: 0.23044483869049676\n",
      "Reconstruction loss: 263.6378370512904, KL divergence: 0.5929545794178434\n",
      "Reconstruction loss: 258.923439547425, KL divergence: 0.5303553154460532\n",
      "Reconstruction loss: 184.34985561343376, KL divergence: 0.04176850045217689\n",
      "Reconstruction loss: 264.52000050521724, KL divergence: 0.018458622951203207\n",
      "Reconstruction loss: 226.91916213017765, KL divergence: 0.502831959764279\n",
      "Reconstruction loss: 185.2056967254349, KL divergence: 0.0170288875113791\n",
      "Reconstruction loss: 332.42778012545233, KL divergence: 2.261676977502409\n",
      "Reconstruction loss: 212.35126231008564, KL divergence: 0.21949305293387694\n",
      "Reconstruction loss: 152.5304610494574, KL divergence: 0.021981607056398678\n",
      "Reconstruction loss: 215.0525522138449, KL divergence: 0.08217283304880718\n",
      "Reconstruction loss: 215.8380424549061, KL divergence: 0.04772905438849706\n",
      "Reconstruction loss: 280.300700999564, KL divergence: 1.2739202594284684\n",
      "Reconstruction loss: 163.96431613056262, KL divergence: 0.027249148601913742\n",
      "Reconstruction loss: 221.75205811406835, KL divergence: 0.1043214148044509\n",
      "Reconstruction loss: 197.4125720408657, KL divergence: 0.046499942444869324\n",
      "Reconstruction loss: 197.39433283506722, KL divergence: 0.2757469850984354\n",
      "Reconstruction loss: 255.46328994478168, KL divergence: 0.295384763303153\n",
      "Reconstruction loss: 208.1065979329822, KL divergence: 0.046879338097723766\n",
      "Reconstruction loss: 181.82509924237547, KL divergence: 0.020547619446467935\n",
      "Reconstruction loss: 196.20418288868166, KL divergence: 0.28060423197524925\n",
      "Reconstruction loss: 208.83254562361316, KL divergence: 0.01784866279603381\n",
      "Reconstruction loss: 142.97898737754394, KL divergence: 0.0371550824032813\n",
      "Reconstruction loss: 233.60650503449122, KL divergence: 0.07139641629675286\n",
      "Reconstruction loss: 181.45140128076326, KL divergence: 0.02970766670342906\n",
      "Reconstruction loss: 172.24154613409894, KL divergence: 0.027249148601913742\n",
      "Reconstruction loss: 187.4231836332989, KL divergence: 0.01919301748614244\n",
      "Reconstruction loss: 169.58469914175413, KL divergence: 0.018036472678567894\n",
      "Reconstruction loss: 227.77048288836264, KL divergence: 1.0296086879386799\n",
      "Reconstruction loss: 207.2363946036178, KL divergence: 0.061400519218945626\n",
      "Reconstruction loss: 234.40342383494016, KL divergence: 0.43135851353204335\n",
      "Reconstruction loss: 218.43164932152894, KL divergence: 0.022410391951498243\n",
      "Reconstruction loss: 202.7415647667954, KL divergence: 0.01880147290962164\n",
      "Reconstruction loss: 235.42494252405749, KL divergence: 0.07622086134764061\n",
      "Reconstruction loss: 228.7998926516308, KL divergence: 0.5701543801524716\n",
      "Reconstruction loss: 235.9669656721187, KL divergence: 0.7463337308429899\n",
      "Reconstruction loss: 199.77745691077342, KL divergence: 0.027249148601913742\n",
      "Reconstruction loss: 160.82416867129558, KL divergence: 0.027249148601913742\n",
      "Reconstruction loss: 150.90924405730829, KL divergence: 0.04234429931378364\n",
      "Reconstruction loss: 177.17652336611292, KL divergence: 0.027249148601913742\n",
      "Reconstruction loss: 250.6614122785577, KL divergence: 2.5749590997697838\n",
      "Reconstruction loss: 174.5984821372121, KL divergence: 0.027249148601913742\n",
      "Reconstruction loss: 146.56044896271055, KL divergence: 0.09886033266124672\n",
      "Reconstruction loss: 166.50584539798734, KL divergence: 0.02814764785027457\n",
      "Reconstruction loss: 208.62698185269534, KL divergence: 0.15305751367107462\n",
      "Reconstruction loss: 154.97951981923129, KL divergence: 0.02814764785027457\n",
      "Reconstruction loss: 145.15512011590437, KL divergence: 0.13230771902746047\n",
      "Reconstruction loss: 161.6569913022207, KL divergence: 0.025921396910352168\n",
      "Reconstruction loss: 237.8631909482906, KL divergence: 0.06623370332562922\n",
      "Reconstruction loss: 310.8622303770004, KL divergence: 4.231163099159802\n",
      "Reconstruction loss: 214.00339389451864, KL divergence: 0.20336612207803345\n",
      "Reconstruction loss: 278.94589002820214, KL divergence: 0.01755100585941466\n",
      "Reconstruction loss: 189.13563133091654, KL divergence: 0.023263168888392194\n",
      "Reconstruction loss: 198.42000596745322, KL divergence: 0.13124808618230838\n",
      "Reconstruction loss: 184.92157934165527, KL divergence: 0.03412358394345111\n",
      "Reconstruction loss: 253.1040740274261, KL divergence: 1.9064808630143977\n",
      "Reconstruction loss: 205.78539733620812, KL divergence: 0.7209291414630469\n",
      "Reconstruction loss: 295.62451510954645, KL divergence: 3.346910226021695\n",
      "Reconstruction loss: 162.01770687488187, KL divergence: 0.04005030910228058\n",
      "Reconstruction loss: 251.34912069436325, KL divergence: 0.017542994330838668\n",
      "Reconstruction loss: 212.57548041719426, KL divergence: 0.09091558190879906\n",
      "Reconstruction loss: 361.6168497378702, KL divergence: 0.4549999966081167\n",
      "Reconstruction loss: 179.5155872508555, KL divergence: 0.018486094674310782\n",
      "Reconstruction loss: 304.84311525332333, KL divergence: 0.1920649373422758\n",
      "Reconstruction loss: 303.6927589794648, KL divergence: 3.800474656439836\n",
      "Reconstruction loss: 221.6952097015676, KL divergence: 0.08364173562383032\n",
      "Reconstruction loss: 250.74466700058906, KL divergence: 0.4785089249265535\n",
      "Reconstruction loss: 240.7894804890057, KL divergence: 0.02814764785027457\n",
      "Reconstruction loss: 184.21821338105633, KL divergence: 0.023004271093011897\n",
      "Reconstruction loss: 275.5117769858988, KL divergence: 1.2870876127610453\n",
      "Reconstruction loss: 244.82013643178698, KL divergence: 0.03883351912479632\n",
      "Reconstruction loss: 156.46933316808497, KL divergence: 0.02814764785027457\n",
      "Reconstruction loss: 247.69265346589202, KL divergence: 0.3044257070004892\n",
      "Reconstruction loss: 194.59355040987606, KL divergence: 0.02814764785027457\n",
      "Reconstruction loss: 151.29774488459753, KL divergence: 0.0644598520894864\n",
      "Reconstruction loss: 224.13312430164402, KL divergence: 0.020630352120795725\n",
      "Reconstruction loss: 171.4335525746332, KL divergence: 0.029195995192640378\n",
      "Reconstruction loss: 153.3152490866533, KL divergence: 0.05020716799984959\n",
      "Reconstruction loss: 185.1263966665047, KL divergence: 0.029195995192640378\n",
      "Reconstruction loss: 329.2439800845067, KL divergence: 2.7525555732197398\n",
      "Reconstruction loss: 185.28791732988702, KL divergence: 0.02107831443112207\n",
      "Reconstruction loss: 213.0052504713645, KL divergence: 0.05359520626786857\n",
      "Reconstruction loss: 271.62734330999393, KL divergence: 0.24094355449760096\n",
      "Reconstruction loss: 198.37577298738057, KL divergence: 0.020878510805185435\n",
      "Reconstruction loss: 259.1614351739725, KL divergence: 0.05409012377217265\n",
      "Reconstruction loss: 187.02509180377518, KL divergence: 0.021356650121881093\n",
      "Reconstruction loss: 211.13797011257884, KL divergence: 0.025788047543892356\n",
      "Reconstruction loss: 163.50569090633434, KL divergence: 0.04094196130459288\n",
      "Reconstruction loss: 168.95417035150376, KL divergence: 0.029195995192640378\n",
      "Reconstruction loss: 300.10599831329034, KL divergence: 0.335314873319657\n",
      "Reconstruction loss: 207.5425436099677, KL divergence: 0.017597869879371064\n",
      "Reconstruction loss: 213.42488235609977, KL divergence: 0.420951508753035\n",
      "Reconstruction loss: 161.91192762413914, KL divergence: 0.029195995192640378\n",
      "Reconstruction loss: 158.65341233338034, KL divergence: 0.029195995192640378\n",
      "Reconstruction loss: 186.73406907147927, KL divergence: 0.018751820856540813\n",
      "Reconstruction loss: 316.99039229584116, KL divergence: 1.4268281710581536\n",
      "Reconstruction loss: 254.12258478087324, KL divergence: 0.1331697199877817\n",
      "Reconstruction loss: 178.89752043711144, KL divergence: 0.029195995192640378\n",
      "Reconstruction loss: 241.05121112437757, KL divergence: 0.1666209149316326\n",
      "Reconstruction loss: 232.88654728671276, KL divergence: 0.08505914435084921\n",
      "Reconstruction loss: 308.57688234060265, KL divergence: 0.9876492707449325\n",
      "Reconstruction loss: 174.4493698381795, KL divergence: 0.03998826854775467\n",
      "Reconstruction loss: 150.89967905357412, KL divergence: 0.034967516038759416\n",
      "Reconstruction loss: 272.5486122325861, KL divergence: 0.5151234561372547\n",
      "Reconstruction loss: 217.74703070353513, KL divergence: 0.16546196228891719\n",
      "Reconstruction loss: 282.4959496664951, KL divergence: 0.139435199196892\n",
      "Reconstruction loss: 212.095034305621, KL divergence: 0.021105270379547125\n",
      "Reconstruction loss: 192.5092956825349, KL divergence: 0.08528505546370851\n",
      "Reconstruction loss: 144.21340006151763, KL divergence: 0.030374813282320112\n",
      "Reconstruction loss: 203.77851972984453, KL divergence: 0.018049138970259837\n",
      "Reconstruction loss: 203.58227940883316, KL divergence: 0.1420904548598328\n",
      "Reconstruction loss: 184.81867207920487, KL divergence: 0.030374813282320112\n",
      "Reconstruction loss: 312.7886887723801, KL divergence: 0.02888079017703582\n",
      "Reconstruction loss: 332.57232753928156, KL divergence: 0.9819029845568945\n",
      "Reconstruction loss: 229.74733940944623, KL divergence: 0.018175172901210068\n",
      "Reconstruction loss: 190.30409981946013, KL divergence: 0.02292885592214039\n",
      "Reconstruction loss: 250.14465575344897, KL divergence: 0.07114628980128807\n",
      "Reconstruction loss: 259.26041691817005, KL divergence: 0.13889218105317563\n",
      "Reconstruction loss: 195.77605469368171, KL divergence: 0.018388894180332738\n",
      "Reconstruction loss: 314.27780467105924, KL divergence: 0.5595072353614854\n",
      "Reconstruction loss: 256.29882558297584, KL divergence: 0.1115141161736713\n",
      "Reconstruction loss: 179.198151639059, KL divergence: 0.030374813282320112\n",
      "Reconstruction loss: 156.68811023825828, KL divergence: 0.030374813282320112\n",
      "Reconstruction loss: 235.99770530832598, KL divergence: 0.25401907231538523\n",
      "Reconstruction loss: 279.4343517733784, KL divergence: 0.5991571525056375\n",
      "Reconstruction loss: 260.51464403973563, KL divergence: 0.6021634660793718\n",
      "Reconstruction loss: 236.37631587862992, KL divergence: 0.34443822633417653\n",
      "Reconstruction loss: 213.6154433175317, KL divergence: 0.24956882696120608\n",
      "Reconstruction loss: 158.24285457464936, KL divergence: 0.0332252526897861\n",
      "Reconstruction loss: 204.73948862455592, KL divergence: 0.030374813282320112\n",
      "Reconstruction loss: 208.03858719009872, KL divergence: 0.026233307024713937\n",
      "Reconstruction loss: 202.8247747196396, KL divergence: 0.056005349109317204\n",
      "Reconstruction loss: 201.78178604352462, KL divergence: 0.018792539404946795\n",
      "Reconstruction loss: 250.50254285678201, KL divergence: 0.10560880397518779\n",
      "Reconstruction loss: 217.21426309263967, KL divergence: 0.01808984571840072\n",
      "Reconstruction loss: 269.6385927000591, KL divergence: 0.8712554316664853\n",
      "Reconstruction loss: 190.39799437116898, KL divergence: 0.030374813282320112\n",
      "Reconstruction loss: 294.3361729052316, KL divergence: 1.921212442870927\n",
      "Reconstruction loss: 244.565875546993, KL divergence: 0.019624066961953957\n",
      "Reconstruction loss: 227.1968595563555, KL divergence: 0.01841448939766971\n",
      "Reconstruction loss: 202.43169868514272, KL divergence: 0.07682670101761124\n",
      "Reconstruction loss: 197.96858895067362, KL divergence: 0.0189766792648014\n",
      "Reconstruction loss: 204.42227398198196, KL divergence: 0.07161306002454687\n",
      "Reconstruction loss: 235.33011710252958, KL divergence: 0.03152508587150665\n",
      "Reconstruction loss: 263.10013903090237, KL divergence: 0.390131998603014\n",
      "Reconstruction loss: 233.67568425918745, KL divergence: 0.019651800133355823\n",
      "Reconstruction loss: 198.62747754838205, KL divergence: 0.03152508587150665\n",
      "Reconstruction loss: 192.28030148122582, KL divergence: 0.10649499993190392\n",
      "Reconstruction loss: 314.98524633574175, KL divergence: 0.09327713433726736\n",
      "Reconstruction loss: 155.6543806370426, KL divergence: 0.03616827455023974\n",
      "Reconstruction loss: 137.03055089256304, KL divergence: 0.0910224279562753\n",
      "Reconstruction loss: 248.88253976075288, KL divergence: 0.04896776133073166\n",
      "Reconstruction loss: 221.05367142356118, KL divergence: 0.01962996187194216\n",
      "Reconstruction loss: 188.88750031012233, KL divergence: 0.020293355741570374\n",
      "Reconstruction loss: 245.64643763962124, KL divergence: 0.029665995907636855\n",
      "Reconstruction loss: 157.03955965651983, KL divergence: 0.03152508587150665\n",
      "Reconstruction loss: 305.7671861073321, KL divergence: 0.09419306607614225\n",
      "Reconstruction loss: 179.38470582466442, KL divergence: 0.025102399241272444\n",
      "Reconstruction loss: 199.64992460704704, KL divergence: 0.0680928646963993\n",
      "Reconstruction loss: 144.3141750281008, KL divergence: 0.04215048492250639\n",
      "Reconstruction loss: 199.50143257930202, KL divergence: 0.034659234233790026\n",
      "Reconstruction loss: 193.51366563990274, KL divergence: 0.30089311527007095\n",
      "Reconstruction loss: 218.30278170773823, KL divergence: 0.03152508587150665\n",
      "Reconstruction loss: 170.30165770247868, KL divergence: 0.04166121617542967\n",
      "Reconstruction loss: 259.6687751506515, KL divergence: 0.05478846585884123\n",
      "Reconstruction loss: 155.89532854163065, KL divergence: 0.11125125408617398\n",
      "Reconstruction loss: 203.2062366420195, KL divergence: 0.03032823983646432\n",
      "Reconstruction loss: 293.05347669701985, KL divergence: 0.022884675777756935\n",
      "Reconstruction loss: 242.51971213038885, KL divergence: 0.21225238361660947\n",
      "Reconstruction loss: 142.65687433695615, KL divergence: 0.06274045786945048\n",
      "Reconstruction loss: 265.73616096485597, KL divergence: 0.11234831196262535\n",
      "Reconstruction loss: 322.85455451719247, KL divergence: 1.172233213902822\n",
      "Reconstruction loss: 198.3833536421458, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 292.8043019084927, KL divergence: 0.03731696677641133\n",
      "Reconstruction loss: 248.74769551719945, KL divergence: 0.7521840507160091\n",
      "Reconstruction loss: 263.2971092206881, KL divergence: 0.2016114319822872\n",
      "Reconstruction loss: 298.4458906453292, KL divergence: 0.2190215722714322\n",
      "Reconstruction loss: 211.8518429385995, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 187.148695188796, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 245.15460471726013, KL divergence: 0.04968526088253661\n",
      "Reconstruction loss: 278.49415437112833, KL divergence: 1.4283099146841747\n",
      "Reconstruction loss: 170.91819940511937, KL divergence: 0.03228439075823464\n",
      "Reconstruction loss: 216.59074583238734, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 257.97445346180007, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 190.4582065098666, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 210.22979470441828, KL divergence: 0.030331165643281588\n",
      "Reconstruction loss: 230.89297490747725, KL divergence: 0.1212984219385767\n",
      "Reconstruction loss: 217.96964492967203, KL divergence: 0.13661406279895288\n",
      "Reconstruction loss: 232.55641328546355, KL divergence: 0.021981701070063142\n",
      "Reconstruction loss: 181.4586595922642, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 271.0797684831487, KL divergence: 0.037029952203572136\n",
      "Reconstruction loss: 181.32657846608458, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 196.52002636625258, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 159.57324217716967, KL divergence: 0.046194062845974104\n",
      "Reconstruction loss: 249.78013990974762, KL divergence: 0.04977636659695234\n",
      "Reconstruction loss: 188.83442468435365, KL divergence: 0.07437153032593696\n",
      "Reconstruction loss: 174.2660309578641, KL divergence: 0.03223856480833909\n",
      "Reconstruction loss: 127.06003803552652, KL divergence: 0.12156158854985671\n",
      "Reconstruction loss: 144.46579566764672, KL divergence: 0.05734727853248561\n",
      "Reconstruction loss: 271.26043492105657, KL divergence: 0.13396410005536163\n",
      "Reconstruction loss: 279.4493213200206, KL divergence: 1.0402928251447585\n",
      "Reconstruction loss: 181.47906958846852, KL divergence: 0.03261532551524704\n",
      "Reconstruction loss: 365.79866033978703, KL divergence: 0.27500508206151164\n",
      "Reconstruction loss: 202.1002111501329, KL divergence: 0.028234868120011858\n",
      "Reconstruction loss: 117.50271809063454, KL divergence: 0.12953943852795646\n",
      "Reconstruction loss: 165.0694941872739, KL divergence: 0.06994588585347628\n",
      "Reconstruction loss: 187.46338090434068, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 153.14826533226093, KL divergence: 0.031069342282548873\n",
      "Reconstruction loss: 286.4697534567611, KL divergence: 0.019333706310339482\n",
      "Reconstruction loss: 279.2151322159133, KL divergence: 0.44102698332295115\n",
      "Reconstruction loss: 178.84283327423375, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 218.55029461372618, KL divergence: 0.12313845880099988\n",
      "Reconstruction loss: 169.07550629157623, KL divergence: 0.036957770414818236\n",
      "Reconstruction loss: 137.42585553277695, KL divergence: 0.04013976992591606\n",
      "Reconstruction loss: 205.57337003060388, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 221.43051130456786, KL divergence: 0.03611194815585267\n",
      "Reconstruction loss: 213.77767544640875, KL divergence: 0.06855569988964966\n",
      "Reconstruction loss: 217.43300270403336, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 294.66220783131257, KL divergence: 0.25967590186346623\n",
      "Reconstruction loss: 214.14181027262225, KL divergence: 0.03040926151326917\n",
      "Reconstruction loss: 177.1617433439693, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 171.22837208886892, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 241.648516929543, KL divergence: 0.026551180130262086\n",
      "Reconstruction loss: 159.02233463751253, KL divergence: 0.0661526287769511\n",
      "Reconstruction loss: 130.15467678912688, KL divergence: 0.22734812060522236\n",
      "Reconstruction loss: 255.07755204160978, KL divergence: 0.32441539123411217\n",
      "Reconstruction loss: 351.60415803543697, KL divergence: 0.1918921256287568\n",
      "Reconstruction loss: 173.83887530598457, KL divergence: 0.018417779489646346\n",
      "Reconstruction loss: 204.8626024584799, KL divergence: 0.039681649514825945\n",
      "Reconstruction loss: 142.7734010342669, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 250.62817352749056, KL divergence: 0.3084362177347881\n",
      "Reconstruction loss: 174.40540720234708, KL divergence: 0.01883662243931783\n",
      "Reconstruction loss: 165.71130422945595, KL divergence: 0.03360731382537463\n",
      "Reconstruction loss: 258.84283389682025, KL divergence: 0.029810040393723147\n",
      "Reconstruction loss: 221.34928490820064, KL divergence: 0.3129871968974116\n",
      "Reconstruction loss: 211.83744114777093, KL divergence: 0.02355680991676168\n",
      "Reconstruction loss: 229.50716075941227, KL divergence: 0.10887709502957532\n",
      "Reconstruction loss: 179.82952196225602, KL divergence: 0.03168540382424723\n",
      "Reconstruction loss: 208.123455299093, KL divergence: 0.10509486439087085\n",
      "Reconstruction loss: 207.42580084607295, KL divergence: 0.01850535996939401\n",
      "Reconstruction loss: 216.02603006926876, KL divergence: 0.05246787464440128\n",
      "Reconstruction loss: 267.51866129258514, KL divergence: 0.5269155461682062\n",
      "Reconstruction loss: 325.5385620195263, KL divergence: 1.560788130658537\n",
      "Reconstruction loss: 280.0599903486213, KL divergence: 0.5927083972280052\n",
      "Reconstruction loss: 200.04453645980885, KL divergence: 0.03441282733022433\n",
      "Reconstruction loss: 209.89129012872075, KL divergence: 0.02687719448808712\n",
      "Reconstruction loss: 287.0040943558171, KL divergence: 0.5796803684983394\n",
      "Reconstruction loss: 241.44537014491, KL divergence: 0.08139596289336176\n",
      "Reconstruction loss: 174.78481634130924, KL divergence: 0.03441282733022433\n",
      "Reconstruction loss: 185.0437824142099, KL divergence: 0.028070193871519766\n",
      "Reconstruction loss: 182.23461054004338, KL divergence: 0.04381772945051865\n",
      "Reconstruction loss: 234.31955699295128, KL divergence: 0.5338764804431599\n",
      "Reconstruction loss: 218.71616948345007, KL divergence: 0.07895427345535616\n",
      "Reconstruction loss: 223.96944078939077, KL divergence: 0.052358239292902786\n",
      "Reconstruction loss: 231.38250441372583, KL divergence: 0.2913818928973354\n",
      "Reconstruction loss: 169.39081773604585, KL divergence: 0.03441282733022433\n",
      "Reconstruction loss: 323.67093516112, KL divergence: 1.9791626432110003\n",
      "Reconstruction loss: 303.47302905711024, KL divergence: 0.7385137013973007\n",
      "Reconstruction loss: 234.29254218075403, KL divergence: 0.05555750535325765\n",
      "Reconstruction loss: 180.41673291668542, KL divergence: 0.03441282733022433\n",
      "Reconstruction loss: 211.8738851428135, KL divergence: 0.10071810591097335\n",
      "Reconstruction loss: 281.6041216815954, KL divergence: 1.2421427266617624\n",
      "Reconstruction loss: 165.4114308334841, KL divergence: 0.04642038491724304\n",
      "Reconstruction loss: 197.91620296782276, KL divergence: 0.0327879672499799\n",
      "Reconstruction loss: 189.88776160768117, KL divergence: 0.026968971444859546\n",
      "Reconstruction loss: 191.53161554337737, KL divergence: 0.022450554926652544\n",
      "Reconstruction loss: 279.5335839220589, KL divergence: 0.3763047101172144\n",
      "Reconstruction loss: 219.31130182952015, KL divergence: 0.34880703100368515\n",
      "Reconstruction loss: 174.0288905616893, KL divergence: 0.03513152224007726\n",
      "Reconstruction loss: 203.62133817788356, KL divergence: 0.03481945938753084\n",
      "Reconstruction loss: 177.99196719543926, KL divergence: 0.17752011138680268\n",
      "Reconstruction loss: 170.8946651263414, KL divergence: 0.03513152224007726\n",
      "Reconstruction loss: 212.71060098766404, KL divergence: 0.06141918123882961\n",
      "Reconstruction loss: 205.60747712728292, KL divergence: 0.08749514872978353\n",
      "Reconstruction loss: 189.69837078088503, KL divergence: 0.06993294030774527\n",
      "Reconstruction loss: 213.25433347538817, KL divergence: 0.21170978436001087\n",
      "Reconstruction loss: 213.31951031918277, KL divergence: 0.03451171674065251\n",
      "Reconstruction loss: 253.08974070030473, KL divergence: 0.136606502452913\n",
      "Reconstruction loss: 128.62137732953911, KL divergence: 0.0944158488688665\n",
      "Reconstruction loss: 202.96940420226218, KL divergence: 0.03973190529902115\n",
      "Reconstruction loss: 202.4697162073782, KL divergence: 0.021507460758970365\n",
      "Reconstruction loss: 180.3721419193992, KL divergence: 0.03513152224007726\n",
      "Reconstruction loss: 152.5995006933552, KL divergence: 0.035790750935429216\n",
      "Reconstruction loss: 145.02830632994863, KL divergence: 0.03513152224007726\n",
      "Reconstruction loss: 186.39369938492626, KL divergence: 0.022250011277197057\n",
      "Reconstruction loss: 123.75610903341936, KL divergence: 0.03632615789486621\n",
      "Reconstruction loss: 250.38631348613296, KL divergence: 0.3988562058966128\n",
      "Reconstruction loss: 207.5584900933262, KL divergence: 0.02501337296929579\n",
      "Reconstruction loss: 228.00034703761997, KL divergence: 0.8632215431913349\n",
      "Reconstruction loss: 190.43107934718026, KL divergence: 0.03513152224007726\n",
      "Reconstruction loss: 271.6597486774776, KL divergence: 0.445905177782955\n",
      "Reconstruction loss: 250.52383796210535, KL divergence: 0.06640894447852541\n",
      "Reconstruction loss: 195.41259812484438, KL divergence: 0.11228931436412787\n",
      "Reconstruction loss: 203.3905303378952, KL divergence: 0.03513152224007726\n",
      "Reconstruction loss: 159.17695191530507, KL divergence: 0.0753608859268825\n",
      "Reconstruction loss: 211.023013177324, KL divergence: 0.247248727376721\n",
      "Reconstruction loss: 287.56459927764166, KL divergence: 0.8717670692789672\n",
      "Reconstruction loss: 220.6133113707217, KL divergence: 0.08531363618182536\n",
      "Reconstruction loss: 283.8384687248275, KL divergence: 1.9933348138211053\n",
      "Reconstruction loss: 194.41645219785715, KL divergence: 0.055079763570665796\n",
      "Reconstruction loss: 156.53418814415025, KL divergence: 0.03596704918407556\n",
      "Reconstruction loss: 180.36493818905595, KL divergence: 0.1017979046451174\n",
      "Reconstruction loss: 274.99852586411043, KL divergence: 0.5821223793539392\n",
      "Reconstruction loss: 219.3213965799424, KL divergence: 0.04212888691927896\n",
      "Reconstruction loss: 195.13502983560429, KL divergence: 0.03596704918407556\n",
      "Reconstruction loss: 192.72511403926507, KL divergence: 0.10696408730690932\n",
      "Reconstruction loss: 165.08819910973693, KL divergence: 0.03596704918407556\n",
      "Reconstruction loss: 234.0653915649026, KL divergence: 0.1381791004993968\n",
      "Reconstruction loss: 184.70174833524368, KL divergence: 0.03596704918407556\n",
      "Reconstruction loss: 233.20462533569133, KL divergence: 0.07461468785689584\n",
      "Reconstruction loss: 186.58181138935618, KL divergence: 0.1296104364927061\n",
      "Reconstruction loss: 241.25841396331197, KL divergence: 0.10792346351692989\n",
      "Reconstruction loss: 272.59222757050094, KL divergence: 3.013794756437351\n",
      "Reconstruction loss: 232.94369292243928, KL divergence: 0.11327000595991088\n",
      "Reconstruction loss: 198.54662688661864, KL divergence: 0.03596704918407556\n",
      "Reconstruction loss: 158.65888991913562, KL divergence: 0.0355232327262171\n",
      "Reconstruction loss: 270.63432486423426, KL divergence: 1.5040836637442412\n",
      "Reconstruction loss: 224.6399128567454, KL divergence: 0.402096637924125\n",
      "Reconstruction loss: 286.93919966144915, KL divergence: 0.6233947413717836\n",
      "Reconstruction loss: 196.7040099841077, KL divergence: 0.2684228717405278\n",
      "Reconstruction loss: 139.08095023484896, KL divergence: 0.03634071113790338\n",
      "Reconstruction loss: 206.89157472743636, KL divergence: 0.11423383004653015\n",
      "Reconstruction loss: 168.4207157426561, KL divergence: 0.01917753855372145\n",
      "Reconstruction loss: 194.619537825632, KL divergence: 0.10212676605786153\n",
      "Reconstruction loss: 232.43453807890515, KL divergence: 0.37761870834506034\n",
      "Reconstruction loss: 200.35512995106438, KL divergence: 0.04265474970737304\n",
      "Reconstruction loss: 134.33843938085727, KL divergence: 0.03596704918407556\n",
      "Reconstruction loss: 226.45470328166414, KL divergence: 0.6996666370061735\n",
      "Reconstruction loss: 233.09684202983215, KL divergence: 0.7292786934345153\n",
      "Reconstruction loss: 227.94765298564153, KL divergence: 0.4630465258881291\n",
      "Reconstruction loss: 195.20116039226656, KL divergence: 0.41531133963533706\n",
      "Reconstruction loss: 267.98182965291824, KL divergence: 0.6122725455502656\n",
      "Reconstruction loss: 249.417120651983, KL divergence: 0.058656153471707384\n",
      "Reconstruction loss: 214.32108412192704, KL divergence: 0.32200660845274337\n",
      "Reconstruction loss: 255.06246624045184, KL divergence: 0.5156919257482222\n",
      "Reconstruction loss: 201.86479954502676, KL divergence: 0.09305300348977902\n",
      "Reconstruction loss: 236.39599559981315, KL divergence: 0.539495839556559\n",
      "Reconstruction loss: 188.24264175755525, KL divergence: 0.03548727952902414\n",
      "Reconstruction loss: 187.66074766220947, KL divergence: 0.37331005507371195\n",
      "Reconstruction loss: 289.94994836048824, KL divergence: 1.1722598127703336\n",
      "Reconstruction loss: 226.2751395777762, KL divergence: 0.050354206736690554\n",
      "Reconstruction loss: 226.72740479625588, KL divergence: 0.04413609767303017\n",
      "Reconstruction loss: 148.09246047649447, KL divergence: 0.04289455020688937\n",
      "Reconstruction loss: 221.27649309513953, KL divergence: 0.03690329898718714\n",
      "Reconstruction loss: 262.46402242817294, KL divergence: 0.291557707614485\n",
      "Reconstruction loss: 231.53402752821597, KL divergence: 0.6022205087450783\n",
      "Reconstruction loss: 262.60897208939855, KL divergence: 1.0471049539677226\n",
      "Reconstruction loss: 233.1695333144586, KL divergence: 1.1086568089234745\n",
      "Reconstruction loss: 189.48226191889668, KL divergence: 0.07897923661694373\n",
      "Reconstruction loss: 270.6937043544623, KL divergence: 1.1968137919811541\n",
      "Reconstruction loss: 247.39157712151578, KL divergence: 0.39385636772988675\n",
      "Reconstruction loss: 131.5094648102284, KL divergence: 0.13935358673491072\n",
      "Reconstruction loss: 357.214995992788, KL divergence: 3.9965457986918644\n",
      "Reconstruction loss: 255.72650569275612, KL divergence: 0.6419142583463644\n",
      "Reconstruction loss: 191.4709022088839, KL divergence: 0.05903883804170185\n",
      "Reconstruction loss: 194.22039356852775, KL divergence: 0.049184269874768705\n",
      "Reconstruction loss: 154.43064370389433, KL divergence: 0.03690329898718714\n",
      "Reconstruction loss: 204.74804739645722, KL divergence: 0.04451636787754065\n",
      "Reconstruction loss: 173.53947237249218, KL divergence: 0.024858346914369667\n",
      "Reconstruction loss: 166.6372772839817, KL divergence: 0.03690329898718714\n",
      "Reconstruction loss: 171.75693480436885, KL divergence: 0.06039293534968898\n",
      "Reconstruction loss: 240.41637513556384, KL divergence: 0.09497515649109317\n",
      "Reconstruction loss: 198.98479013551466, KL divergence: 0.053253130838419926\n",
      "Reconstruction loss: 167.88944658571123, KL divergence: 0.03519868356274858\n",
      "Reconstruction loss: 263.1885365428245, KL divergence: 1.6131326500842957\n",
      "Reconstruction loss: 263.2650713701229, KL divergence: 0.304317273853902\n",
      "Reconstruction loss: 192.87556782301778, KL divergence: 0.27613288661034674\n",
      "Reconstruction loss: 239.10835159562413, KL divergence: 0.09785329473730892\n",
      "Reconstruction loss: 230.83820032206552, KL divergence: 0.3634674061091428\n",
      "Reconstruction loss: 302.4535005554572, KL divergence: 0.06905766967866572\n",
      "Reconstruction loss: 211.80687552890052, KL divergence: 0.05569352300604219\n",
      "Reconstruction loss: 267.41003762107357, KL divergence: 1.2948035665985091\n",
      "Reconstruction loss: 164.39827117713114, KL divergence: 0.03806195249696959\n",
      "Reconstruction loss: 192.5560248930018, KL divergence: 0.025439649715023938\n",
      "Reconstruction loss: 229.2427437160652, KL divergence: 0.19072554814179443\n",
      "Reconstruction loss: 278.30537391956466, KL divergence: 1.1407844372140663\n",
      "Reconstruction loss: 212.56911694296474, KL divergence: 0.01944147739851876\n",
      "Reconstruction loss: 210.1857624561323, KL divergence: 0.02783658396780897\n",
      "Reconstruction loss: 139.92763248843076, KL divergence: 0.04024017288140758\n",
      "Reconstruction loss: 207.01733757494088, KL divergence: 0.04543348063475949\n",
      "Reconstruction loss: 209.89942080735057, KL divergence: 0.026798475548671208\n",
      "Reconstruction loss: 233.88187706358966, KL divergence: 0.232648626082904\n",
      "Reconstruction loss: 173.97173258099252, KL divergence: 0.03806195249696959\n",
      "Reconstruction loss: 268.3368857409687, KL divergence: 0.4897207711002889\n",
      "Reconstruction loss: 221.4286577055982, KL divergence: 0.06635046808772238\n",
      "Reconstruction loss: 250.25851864279846, KL divergence: 0.07486953101759519\n",
      "Reconstruction loss: 173.7426854244681, KL divergence: 0.027927556501290862\n",
      "Reconstruction loss: 188.11338067287494, KL divergence: 0.03064272550791708\n",
      "Reconstruction loss: 224.69113270905365, KL divergence: 0.24850317828131463\n",
      "Reconstruction loss: 291.70480536380137, KL divergence: 0.17605019796768334\n",
      "Reconstruction loss: 180.4898920063494, KL divergence: 0.020489562243757364\n",
      "Reconstruction loss: 202.22935791617203, KL divergence: 0.03484758837776608\n",
      "Reconstruction loss: 186.38433546298626, KL divergence: 0.023597259749484123\n",
      "Reconstruction loss: 245.0352976691311, KL divergence: 0.3410773044231355\n",
      "Reconstruction loss: 243.16353239823837, KL divergence: 0.5309749389169551\n",
      "Reconstruction loss: 170.40757298120985, KL divergence: 0.03969008935461649\n",
      "Reconstruction loss: 236.42158030120794, KL divergence: 0.05514130515997806\n",
      "Reconstruction loss: 175.23561229054, KL divergence: 0.041464089131805826\n",
      "Reconstruction loss: 152.59340519547078, KL divergence: 0.039253130379394596\n",
      "Reconstruction loss: 264.76649261795046, KL divergence: 0.4828417087234155\n",
      "Reconstruction loss: 155.02706972270624, KL divergence: 0.08944874123834928\n",
      "Reconstruction loss: 156.74290478611832, KL divergence: 0.039253130379394596\n",
      "Reconstruction loss: 181.77405176075132, KL divergence: 0.11882943250473454\n",
      "Reconstruction loss: 219.02944058105854, KL divergence: 0.030162554313833356\n",
      "Reconstruction loss: 182.4267930784272, KL divergence: 0.039253130379394596\n",
      "Reconstruction loss: 198.80480188488747, KL divergence: 0.020639401734219964\n",
      "Reconstruction loss: 188.90683879086953, KL divergence: 0.06944590965826408\n",
      "Reconstruction loss: 186.09039199425325, KL divergence: 0.039253130379394596\n",
      "Reconstruction loss: 252.29051507808106, KL divergence: 0.8359209307433237\n",
      "Reconstruction loss: 256.35533001503984, KL divergence: 0.6409999507534704\n",
      "Reconstruction loss: 152.27439019953175, KL divergence: 0.039253130379394596\n",
      "Reconstruction loss: 307.5719897635686, KL divergence: 0.7191644521425471\n",
      "Reconstruction loss: 159.73220399279222, KL divergence: 0.039253130379394596\n",
      "Reconstruction loss: 230.5149726824364, KL divergence: 0.020516413558582414\n",
      "Reconstruction loss: 157.99303310714546, KL divergence: 0.12947080554275686\n",
      "Reconstruction loss: 230.38783953516918, KL divergence: 1.324989109356255\n",
      "Reconstruction loss: 192.50446477261687, KL divergence: 0.04003431729015999\n",
      "Reconstruction loss: 148.39745266437578, KL divergence: 0.042147662724627555\n",
      "Reconstruction loss: 278.41609950391876, KL divergence: 0.692506383187208\n",
      "Reconstruction loss: 254.95326655926056, KL divergence: 0.6214672607859116\n",
      "Reconstruction loss: 166.72576904497143, KL divergence: 0.039253130379394596\n",
      "Reconstruction loss: 216.17414402582182, KL divergence: 0.03589780952930105\n",
      "Reconstruction loss: 261.32289247477553, KL divergence: 0.7993858267475547\n",
      "Reconstruction loss: 139.44803414681564, KL divergence: 0.10971930116380835\n",
      "Reconstruction loss: 257.2401071157673, KL divergence: 0.039002524820241635\n",
      "Reconstruction loss: 246.50647465761568, KL divergence: 0.2376131869074522\n",
      "Reconstruction loss: 188.49195475676424, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 208.06862339084762, KL divergence: 0.020241458261527734\n",
      "Reconstruction loss: 262.4893638037643, KL divergence: 0.25998107236032325\n",
      "Reconstruction loss: 208.76993937221513, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 181.80792829540906, KL divergence: 0.0559349059644797\n",
      "Reconstruction loss: 189.81920588346526, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 242.67484554836972, KL divergence: 0.5527782024783388\n",
      "Reconstruction loss: 207.19721923193276, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 262.1620875104751, KL divergence: 0.09724217754867903\n",
      "Reconstruction loss: 130.85664019701278, KL divergence: 0.08129238889263035\n",
      "Reconstruction loss: 223.24984679687884, KL divergence: 0.35056224744692993\n",
      "Reconstruction loss: 153.30802351945346, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 183.34457489204829, KL divergence: 0.019713153581848886\n",
      "Reconstruction loss: 181.54223310236648, KL divergence: 0.01949535261972679\n",
      "Reconstruction loss: 142.68639649396553, KL divergence: 0.04729596737189612\n",
      "Reconstruction loss: 271.2099781616649, KL divergence: 0.1296149613493091\n",
      "Reconstruction loss: 245.6796251072215, KL divergence: 0.22056804047623657\n",
      "Reconstruction loss: 253.1684987789158, KL divergence: 0.19205764702674372\n",
      "Reconstruction loss: 326.3113052487277, KL divergence: 0.8391488064328484\n",
      "Reconstruction loss: 189.36395390826743, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 165.1033065543842, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 173.51208714227874, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 182.40171709470633, KL divergence: 0.02325880380986739\n",
      "Reconstruction loss: 254.1181706272289, KL divergence: 0.5825826776663416\n",
      "Reconstruction loss: 183.1501690135649, KL divergence: 0.0424279159957941\n",
      "Reconstruction loss: 163.46956691632818, KL divergence: 0.04870142456386106\n",
      "Reconstruction loss: 223.21718453599794, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 224.72368022428878, KL divergence: 0.24595331152202465\n",
      "Reconstruction loss: 262.1079823588913, KL divergence: 0.14405221590711675\n",
      "Reconstruction loss: 175.19587681665573, KL divergence: 0.040524804995813846\n",
      "Reconstruction loss: 255.05598189088494, KL divergence: 0.027165817913698764\n",
      "Reconstruction loss: 184.1719381253618, KL divergence: 0.020540485075843606\n",
      "Reconstruction loss: 192.3504889520197, KL divergence: 0.02764743978132972\n",
      "Reconstruction loss: 183.11861418612796, KL divergence: 0.10391617484650756\n",
      "Reconstruction loss: 167.02468155399518, KL divergence: 0.06279292665825376\n",
      "Reconstruction loss: 120.15389133319962, KL divergence: 0.07418523278133382\n",
      "Reconstruction loss: 152.20113119399988, KL divergence: 0.18066894854117088\n",
      "Reconstruction loss: 185.95851218630276, KL divergence: 0.046998852761330645\n",
      "Reconstruction loss: 194.79384648647485, KL divergence: 0.023296212852498388\n",
      "Reconstruction loss: 202.5733921832913, KL divergence: 0.03631015435784768\n",
      "Reconstruction loss: 190.0721579270001, KL divergence: 0.04452836842344554\n",
      "Reconstruction loss: 184.5938016732984, KL divergence: 0.020996154002494438\n",
      "Reconstruction loss: 166.9287320408306, KL divergence: 0.04169029508832328\n",
      "Reconstruction loss: 164.26541450953167, KL divergence: 0.07361760875016932\n",
      "Reconstruction loss: 244.16062834311742, KL divergence: 0.0811119798018336\n",
      "Reconstruction loss: 166.15908652798285, KL divergence: 0.04169029508832328\n",
      "Reconstruction loss: 253.91866410874928, KL divergence: 0.10784624046572633\n",
      "Reconstruction loss: 242.6467115265284, KL divergence: 0.06669153588445864\n",
      "Reconstruction loss: 197.60451967927696, KL divergence: 0.08017334621291244\n",
      "Reconstruction loss: 247.32949556551668, KL divergence: 0.021571861083766353\n",
      "Reconstruction loss: 354.7641676364908, KL divergence: 2.009838083696959\n",
      "Reconstruction loss: 220.12800146296013, KL divergence: 0.0841717550678116\n",
      "Reconstruction loss: 186.8553365419717, KL divergence: 0.022863404533789822\n",
      "Reconstruction loss: 172.48237441986072, KL divergence: 0.04169029508832328\n",
      "Reconstruction loss: 175.1461723431517, KL divergence: 0.03250812477960907\n",
      "Reconstruction loss: 181.70423185646717, KL divergence: 0.04169029508832328\n",
      "Reconstruction loss: 262.9065386164769, KL divergence: 0.3060908642120937\n",
      "Reconstruction loss: 187.8267743100481, KL divergence: 0.04169029508832328\n",
      "Reconstruction loss: 302.92422643523935, KL divergence: 0.09496352903065475\n",
      "Reconstruction loss: 156.92647942102462, KL divergence: 0.05874464895742637\n",
      "Reconstruction loss: 296.0251387391585, KL divergence: 0.4244232113456506\n",
      "Reconstruction loss: 211.45015242607462, KL divergence: 0.09000781827996662\n",
      "Reconstruction loss: 290.08169401977455, KL divergence: 1.4289495560336127\n",
      "Reconstruction loss: 247.82822650359248, KL divergence: 0.11381656465760825\n",
      "Reconstruction loss: 327.4803111108975, KL divergence: 2.1942709655088697\n",
      "Reconstruction loss: 201.19938780844248, KL divergence: 0.03510004098520314\n",
      "Reconstruction loss: 196.21890232609576, KL divergence: 0.04282516004510384\n",
      "Reconstruction loss: 219.5215310412009, KL divergence: 0.05734706423696506\n",
      "Reconstruction loss: 172.5004086754479, KL divergence: 0.02564548576941339\n",
      "Reconstruction loss: 151.79450915915737, KL divergence: 0.09268990159325247\n",
      "Reconstruction loss: 243.31506641962957, KL divergence: 0.2049580446057429\n",
      "Reconstruction loss: 246.44237206510837, KL divergence: 0.01963353501755688\n",
      "Reconstruction loss: 215.69161526187145, KL divergence: 0.042579031860100036\n",
      "Reconstruction loss: 215.8242626832025, KL divergence: 0.019854707305416608\n",
      "Reconstruction loss: 217.75438492933563, KL divergence: 0.021322185862080456\n",
      "Reconstruction loss: 225.56830842227487, KL divergence: 0.1000011219454\n",
      "Reconstruction loss: 230.58953238425028, KL divergence: 0.7160199521065078\n",
      "Reconstruction loss: 149.36886975315008, KL divergence: 0.1342055840845141\n",
      "Reconstruction loss: 138.69304210726278, KL divergence: 0.1422731951102178\n",
      "Reconstruction loss: 185.15518835313998, KL divergence: 0.05578659734368041\n",
      "Reconstruction loss: 186.6885999796554, KL divergence: 0.04282516004510384\n",
      "Reconstruction loss: 170.64992398798398, KL divergence: 0.020701350136724028\n",
      "Reconstruction loss: 225.08183592618056, KL divergence: 0.055499455808870835\n",
      "Reconstruction loss: 161.68665577363072, KL divergence: 0.04282516004510384\n",
      "Reconstruction loss: 226.6393733857522, KL divergence: 0.026142360925792696\n",
      "Reconstruction loss: 169.31403308428673, KL divergence: 0.09395078903558524\n",
      "Reconstruction loss: 190.31304761540991, KL divergence: 0.04282516004510384\n",
      "Reconstruction loss: 281.81169537113385, KL divergence: 0.027872526252788354\n",
      "Reconstruction loss: 309.3828964919222, KL divergence: 0.5522626911916455\n",
      "Reconstruction loss: 201.05915514688544, KL divergence: 0.025277441958596047\n",
      "Reconstruction loss: 310.73210269653555, KL divergence: 0.45254363128386216\n",
      "Reconstruction loss: 215.42400747468525, KL divergence: 0.042781398840706975\n",
      "Reconstruction loss: 208.53080036470323, KL divergence: 0.04303780892845388\n",
      "Reconstruction loss: 239.2109606018082, KL divergence: 0.19044060703256854\n",
      "Reconstruction loss: 174.83720032271393, KL divergence: 0.04282516004510384\n",
      "Reconstruction loss: 174.55677114198585, KL divergence: 0.0816307597398071\n",
      "Reconstruction loss: 165.9608361166641, KL divergence: 0.043838192180136626\n",
      "Reconstruction loss: 118.3701980530598, KL divergence: 0.19537637866738317\n",
      "Reconstruction loss: 240.63800437751703, KL divergence: 0.023932345903466823\n",
      "Reconstruction loss: 218.3871453697767, KL divergence: 0.024310114510278813\n",
      "Reconstruction loss: 211.20608714751103, KL divergence: 0.053753247400431636\n",
      "Reconstruction loss: 205.79145886026475, KL divergence: 0.03918123570054716\n",
      "Reconstruction loss: 268.2381625176023, KL divergence: 0.08354635710916058\n",
      "Reconstruction loss: 188.28933059730997, KL divergence: 0.043838192180136626\n",
      "Reconstruction loss: 169.57317260132083, KL divergence: 0.043838192180136626\n",
      "Reconstruction loss: 160.3253660767172, KL divergence: 0.043838192180136626\n",
      "Reconstruction loss: 161.24144750053702, KL divergence: 0.049828687681145234\n",
      "Reconstruction loss: 198.81758871047742, KL divergence: 0.05099838282666441\n",
      "Reconstruction loss: 131.90741515634534, KL divergence: 0.2252393879687869\n",
      "Reconstruction loss: 151.2835430830974, KL divergence: 0.054729744554985305\n",
      "Reconstruction loss: 223.6185732819667, KL divergence: 0.04339570133492521\n",
      "Reconstruction loss: 253.125530047775, KL divergence: 0.03501393646079465\n",
      "Reconstruction loss: 196.44024338787526, KL divergence: 0.18574916454305795\n",
      "Reconstruction loss: 252.82365025696905, KL divergence: 0.17481813683821534\n",
      "Reconstruction loss: 211.73650602537714, KL divergence: 0.019530518745118353\n",
      "Reconstruction loss: 160.32825750964986, KL divergence: 0.043838192180136626\n",
      "Reconstruction loss: 269.40086442132355, KL divergence: 0.4730399512747508\n",
      "Reconstruction loss: 231.46424969162138, KL divergence: 0.20071851764606868\n",
      "Reconstruction loss: 166.4958495930763, KL divergence: 0.043838192180136626\n",
      "Reconstruction loss: 186.6036802728807, KL divergence: 0.04184015695618687\n",
      "Reconstruction loss: 274.4419112539356, KL divergence: 0.5383158253067989\n",
      "Reconstruction loss: 258.36874047111047, KL divergence: 0.20570895941804063\n",
      "Reconstruction loss: 173.83176902314275, KL divergence: 0.043838192180136626\n",
      "Reconstruction loss: 224.40463249951137, KL divergence: 0.6780072721957795\n",
      "Reconstruction loss: 259.0044696781108, KL divergence: 0.13378882113386653\n",
      "Reconstruction loss: 206.9003913221905, KL divergence: 0.019451114693586347\n",
      "Reconstruction loss: 368.0787199661006, KL divergence: 2.352270270395263\n",
      "Reconstruction loss: 262.97995586112637, KL divergence: 0.6335132769334688\n",
      "Reconstruction loss: 207.62705197853307, KL divergence: 0.019087143994729328\n",
      "Reconstruction loss: 159.06886433928287, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 233.26859346897749, KL divergence: 0.12955036547419274\n",
      "Reconstruction loss: 334.0895324758469, KL divergence: 0.0367408906062236\n",
      "Reconstruction loss: 189.98476763369706, KL divergence: 0.06437901934841606\n",
      "Reconstruction loss: 280.8863549643711, KL divergence: 0.5648432164226986\n",
      "Reconstruction loss: 201.99360329986865, KL divergence: 0.3172679703690257\n",
      "Reconstruction loss: 212.82336303720956, KL divergence: 0.01910562079206468\n",
      "Reconstruction loss: 187.4648924684147, KL divergence: 0.04154644550954262\n",
      "Reconstruction loss: 275.62703698887947, KL divergence: 0.5429078055052738\n",
      "Reconstruction loss: 132.63407177424563, KL divergence: 0.05749260935904893\n",
      "Reconstruction loss: 224.51350925963135, KL divergence: 0.21647972379328606\n",
      "Reconstruction loss: 170.7051184475726, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 227.1334270925981, KL divergence: 0.03441781786605508\n",
      "Reconstruction loss: 173.92049837139336, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 216.7497648097363, KL divergence: 0.07714546611749717\n",
      "Reconstruction loss: 221.14457301445825, KL divergence: 0.15371035528131677\n",
      "Reconstruction loss: 217.40494323411957, KL divergence: 0.02093591146604601\n",
      "Reconstruction loss: 126.23440940884204, KL divergence: 0.09484488368395672\n",
      "Reconstruction loss: 206.04761876310175, KL divergence: 0.042144612018641836\n",
      "Reconstruction loss: 206.9550071017435, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 175.3994847258096, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 226.02510606183336, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 172.3512469226651, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 252.52675288675727, KL divergence: 0.3420837492556968\n",
      "Reconstruction loss: 234.6882320255824, KL divergence: 0.14834886247789963\n",
      "Reconstruction loss: 202.87464120807374, KL divergence: 0.03755215454035904\n",
      "Reconstruction loss: 133.7874333782073, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 263.506207878979, KL divergence: 0.21510299586910736\n",
      "Reconstruction loss: 191.1074563593179, KL divergence: 0.08235592896488608\n",
      "Reconstruction loss: 154.49772333798973, KL divergence: 0.04465280257316018\n",
      "Reconstruction loss: 207.3535543149651, KL divergence: 0.02861747668439657\n",
      "Reconstruction loss: 225.21778817872388, KL divergence: 0.01930604827038862\n",
      "Reconstruction loss: 190.16774671549342, KL divergence: 0.026809789030350528\n",
      "Reconstruction loss: 279.90646212573887, KL divergence: 2.3614669947387057\n",
      "Reconstruction loss: 198.80007991563718, KL divergence: 0.042067660350148195\n",
      "Reconstruction loss: 311.3777485656032, KL divergence: 0.8863984049526441\n",
      "Reconstruction loss: 245.99428460040235, KL divergence: 0.6148200427795707\n",
      "Reconstruction loss: 194.00427916455055, KL divergence: 0.08683719163400344\n",
      "Reconstruction loss: 280.5191452931744, KL divergence: 0.03744848364802922\n",
      "Reconstruction loss: 135.1751560527543, KL divergence: 0.09862093868471522\n",
      "Reconstruction loss: 255.10272271062286, KL divergence: 0.7966850703830256\n",
      "Reconstruction loss: 194.92619917053796, KL divergence: 0.09816210039773138\n",
      "Reconstruction loss: 261.5170218331211, KL divergence: 0.7741572021133173\n",
      "Reconstruction loss: 201.84760215772798, KL divergence: 0.0614192816398092\n",
      "Reconstruction loss: 182.07119158947359, KL divergence: 0.045358655292844774\n",
      "Reconstruction loss: 268.50861641197633, KL divergence: 1.0303432867702016\n",
      "Reconstruction loss: 214.46321409131698, KL divergence: 0.026012905063099945\n",
      "Reconstruction loss: 202.04755738664636, KL divergence: 0.03765676262004075\n",
      "Reconstruction loss: 318.65804813906243, KL divergence: 1.1341551154409917\n",
      "Reconstruction loss: 264.7463072295327, KL divergence: 0.44455281562937693\n",
      "Reconstruction loss: 193.53791701641637, KL divergence: 0.024714598099665697\n",
      "Reconstruction loss: 222.808130755895, KL divergence: 0.019431271293715413\n",
      "Reconstruction loss: 170.29275446970772, KL divergence: 0.020430011777714252\n",
      "Reconstruction loss: 141.03125619420302, KL divergence: 0.051624399161985024\n",
      "Reconstruction loss: 223.5081713662966, KL divergence: 0.044550125971025256\n",
      "Reconstruction loss: 191.42926081368222, KL divergence: 0.04188836213659619\n",
      "Reconstruction loss: 144.8967974476015, KL divergence: 0.0775636271821345\n",
      "Reconstruction loss: 204.70912485293053, KL divergence: 0.18593626648126993\n",
      "Reconstruction loss: 231.87477926200316, KL divergence: 0.3273360607924219\n",
      "Reconstruction loss: 227.02289254523345, KL divergence: 0.038688849754071386\n",
      "Reconstruction loss: 224.99790034983917, KL divergence: 0.13268808839320284\n",
      "Reconstruction loss: 285.1235765510879, KL divergence: 0.3681976816048968\n",
      "Reconstruction loss: 336.30103650415333, KL divergence: 0.880731964015808\n",
      "Reconstruction loss: 165.25507787753077, KL divergence: 0.04597866353574598\n",
      "Reconstruction loss: 184.21176958350662, KL divergence: 0.025896250026391898\n",
      "Reconstruction loss: 240.26913839484536, KL divergence: 0.9774382059343258\n",
      "Reconstruction loss: 181.2620785383517, KL divergence: 0.04597866353574598\n",
      "Reconstruction loss: 186.54207325182682, KL divergence: 0.04597866353574598\n",
      "Reconstruction loss: 258.53105164645336, KL divergence: 1.9687514301826434\n",
      "Reconstruction loss: 141.4160382833669, KL divergence: 0.04597866353574598\n",
      "Reconstruction loss: 271.65289322298815, KL divergence: 0.5016715366565312\n",
      "Reconstruction loss: 219.03488584096823, KL divergence: 0.04045631841673891\n",
      "Reconstruction loss: 155.61952036225586, KL divergence: 0.020497672364576836\n",
      "Reconstruction loss: 255.98493928947116, KL divergence: 1.2123179835543485\n",
      "Reconstruction loss: 276.1815112080125, KL divergence: 0.5643463540262046\n",
      "Reconstruction loss: 265.21286784496516, KL divergence: 2.068276519905565\n",
      "Reconstruction loss: 232.0260509170427, KL divergence: 0.5906333219080075\n",
      "Reconstruction loss: 247.96511588900506, KL divergence: 3.0626043802779357\n",
      "Reconstruction loss: 185.79409069421476, KL divergence: 0.14737171860420056\n",
      "Reconstruction loss: 170.02909221557684, KL divergence: 0.053697626719531266\n",
      "Reconstruction loss: 256.7979777140274, KL divergence: 0.8208980868246312\n",
      "Reconstruction loss: 233.69014601873957, KL divergence: 0.2820781139222941\n",
      "Reconstruction loss: 173.78613752627896, KL divergence: 0.060322934142912676\n",
      "Reconstruction loss: 227.60387513710162, KL divergence: 0.28239736299883234\n",
      "Reconstruction loss: 163.59376718977688, KL divergence: 0.019780276805179853\n",
      "Reconstruction loss: 184.08566890807953, KL divergence: 0.07222981758892477\n",
      "Reconstruction loss: 168.5078343925885, KL divergence: 0.04597866353574598\n",
      "Reconstruction loss: 207.54058778968533, KL divergence: 0.15396963176863565\n",
      "Reconstruction loss: 290.5887806483905, KL divergence: 1.838172320032287\n",
      "Reconstruction loss: 199.815231945956, KL divergence: 0.2563543071848859\n",
      "Reconstruction loss: 208.82572994397984, KL divergence: 0.1094819530657094\n",
      "Reconstruction loss: 223.47702470999337, KL divergence: 0.14618899289623916\n",
      "Reconstruction loss: 218.6619349731955, KL divergence: 0.032797501330859236\n",
      "Reconstruction loss: 266.6461143458963, KL divergence: 0.20436557651344534\n",
      "Reconstruction loss: 231.46657077227314, KL divergence: 0.056823525208307524\n",
      "Reconstruction loss: 303.63909539743236, KL divergence: 4.312656259867381\n",
      "Reconstruction loss: 136.83162052746215, KL divergence: 0.04690632542795098\n",
      "Reconstruction loss: 157.30182903110767, KL divergence: 0.053467388955298234\n",
      "Reconstruction loss: 189.63238997712853, KL divergence: 0.04690632542795098\n",
      "Reconstruction loss: 173.08880170999896, KL divergence: 0.0481077038105287\n",
      "Reconstruction loss: 234.97297248063995, KL divergence: 0.11644255920181223\n",
      "Reconstruction loss: 168.1311072995466, KL divergence: 0.01830132711450172\n",
      "Reconstruction loss: 219.47937783778707, KL divergence: 0.0314186705112211\n",
      "Reconstruction loss: 193.49064698715358, KL divergence: 0.25260847474123005\n",
      "Reconstruction loss: 188.32879666977914, KL divergence: 0.019213274207373898\n",
      "Reconstruction loss: 209.73332213878524, KL divergence: 0.021065664579268617\n",
      "Reconstruction loss: 173.6963096578409, KL divergence: 0.03537404409012934\n",
      "Reconstruction loss: 194.49594822384, KL divergence: 0.019019398662418474\n",
      "Reconstruction loss: 116.4928407576003, KL divergence: 0.04675368739203439\n",
      "Reconstruction loss: 237.80647348705162, KL divergence: 2.577087400785241\n",
      "Reconstruction loss: 245.559933951937, KL divergence: 0.23169321041790453\n",
      "Reconstruction loss: 238.47698449061983, KL divergence: 0.18594781810400096\n",
      "Reconstruction loss: 296.9686472399969, KL divergence: 1.7374737966536005\n",
      "Reconstruction loss: 240.45448262596432, KL divergence: 0.4123071223470487\n",
      "Reconstruction loss: 301.38626521519757, KL divergence: 3.4036134331400594\n",
      "Reconstruction loss: 177.71596913092765, KL divergence: 0.043117355799713875\n",
      "Reconstruction loss: 174.02135505895788, KL divergence: 0.04690632542795098\n",
      "Reconstruction loss: 252.89508891780181, KL divergence: 0.18926815406493508\n",
      "Reconstruction loss: 236.70660885332887, KL divergence: 0.1281000877590459\n",
      "Reconstruction loss: 159.71005759428948, KL divergence: 0.04690632542795098\n",
      "Reconstruction loss: 193.5026600897724, KL divergence: 0.0379667374879068\n",
      "Reconstruction loss: 318.5140419724805, KL divergence: 1.377577824784288\n",
      "Reconstruction loss: 172.66178106634266, KL divergence: 0.03538885970201827\n",
      "Reconstruction loss: 193.26544654251327, KL divergence: 0.029841580233591514\n",
      "Reconstruction loss: 253.94630921350046, KL divergence: 2.433828716809515\n",
      "Reconstruction loss: 190.71015877535137, KL divergence: 0.12114839784352088\n",
      "Reconstruction loss: 254.17854950233635, KL divergence: 1.6372898150754467\n",
      "Reconstruction loss: 155.71995283200764, KL divergence: 0.029206147066111854\n",
      "Reconstruction loss: 213.10484485108276, KL divergence: 0.3524988442027382\n",
      "Reconstruction loss: 278.7094726458885, KL divergence: 3.053091025172695\n",
      "Reconstruction loss: 182.28263572361269, KL divergence: 0.028346004309580453\n",
      "Reconstruction loss: 188.92070592621604, KL divergence: 0.032442471664628436\n",
      "Reconstruction loss: 268.547677137948, KL divergence: 0.15225151633609368\n",
      "Reconstruction loss: 155.79714545163932, KL divergence: 0.04784841444349375\n",
      "Reconstruction loss: 225.6559526562959, KL divergence: 0.09217732092122571\n",
      "Reconstruction loss: 186.3950882281581, KL divergence: 0.10231187546046899\n",
      "Reconstruction loss: 269.50800647568167, KL divergence: 2.082651994518704\n",
      "Reconstruction loss: 131.9186664984398, KL divergence: 0.1620192030197608\n",
      "Reconstruction loss: 257.7576360627981, KL divergence: 0.457353868879456\n",
      "Reconstruction loss: 236.047299333062, KL divergence: 0.01821342658960773\n",
      "Reconstruction loss: 249.94360453399813, KL divergence: 0.5261663546690964\n",
      "Reconstruction loss: 176.4951538233302, KL divergence: 0.06469507583508016\n",
      "Reconstruction loss: 193.23550873572816, KL divergence: 0.5393349707855991\n",
      "Reconstruction loss: 216.64498161791818, KL divergence: 0.7877663693887569\n",
      "Reconstruction loss: 193.15776594790964, KL divergence: 0.08973750503988714\n",
      "Reconstruction loss: 243.83667902972329, KL divergence: 1.2577579207777387\n",
      "Reconstruction loss: 233.40104386661716, KL divergence: 0.27611375345682515\n",
      "Reconstruction loss: 203.05453389458995, KL divergence: 0.33017770397478735\n",
      "Reconstruction loss: 146.80128102240474, KL divergence: 0.1258134195999694\n",
      "Reconstruction loss: 226.01388255279443, KL divergence: 0.18600125449980331\n",
      "Reconstruction loss: 127.07668060859964, KL divergence: 0.12600017980518924\n",
      "Reconstruction loss: 255.61691454748683, KL divergence: 0.12897580725653562\n",
      "Reconstruction loss: 200.4899335967482, KL divergence: 0.33013568449746944\n",
      "Reconstruction loss: 196.42906565552263, KL divergence: 0.08218311822016566\n",
      "Reconstruction loss: 253.85818416773196, KL divergence: 0.10786969963056992\n",
      "Reconstruction loss: 198.42643668334824, KL divergence: 0.03241198124355227\n",
      "Reconstruction loss: 177.74096973647505, KL divergence: 0.05340555257219637\n",
      "Reconstruction loss: 228.08172400627348, KL divergence: 0.6220999520570998\n",
      "Reconstruction loss: 223.3462590251459, KL divergence: 0.6125303238490551\n",
      "Reconstruction loss: 327.5594336033935, KL divergence: 6.698543789717304\n",
      "Reconstruction loss: 140.7355903171573, KL divergence: 0.1976263002450227\n",
      "Reconstruction loss: 210.7215677534317, KL divergence: 0.02101780625463756\n",
      "Reconstruction loss: 213.7392431646055, KL divergence: 0.9069072441176576\n",
      "Reconstruction loss: 168.84503697956467, KL divergence: 0.04909344406412569\n",
      "Reconstruction loss: 188.9439127838524, KL divergence: 0.2272293340410796\n",
      "Reconstruction loss: 239.74245581279, KL divergence: 0.38594506372889015\n",
      "Reconstruction loss: 259.3279489932229, KL divergence: 2.2546239501003265\n",
      "Reconstruction loss: 229.4904778295642, KL divergence: 0.04909344406412569\n",
      "Reconstruction loss: 226.65075840747585, KL divergence: 0.04906496363137042\n",
      "Reconstruction loss: 193.38529090885285, KL divergence: 0.029921861061390775\n",
      "Reconstruction loss: 201.2475188416948, KL divergence: 0.05110905076948069\n",
      "Reconstruction loss: 295.197557980356, KL divergence: 0.0825864845669021\n",
      "Reconstruction loss: 151.22572009903112, KL divergence: 0.04909344406412569\n",
      "Reconstruction loss: 164.9523927000393, KL divergence: 0.15405675255443008\n",
      "Reconstruction loss: 170.6419846681589, KL divergence: 0.04909344406412569\n",
      "Reconstruction loss: 228.20751998498469, KL divergence: 1.2796930160451359\n",
      "Reconstruction loss: 237.29287703978514, KL divergence: 0.18172867433081574\n",
      "Reconstruction loss: 151.98505495099346, KL divergence: 0.05627194181687167\n",
      "Reconstruction loss: 204.22702964503884, KL divergence: 0.03815294999010427\n",
      "Reconstruction loss: 177.22625298374498, KL divergence: 0.31769137332446057\n",
      "Reconstruction loss: 219.2810049188707, KL divergence: 0.26666607930647857\n",
      "Reconstruction loss: 205.69812527501455, KL divergence: 0.04909344406412569\n",
      "Reconstruction loss: 172.6980595365385, KL divergence: 0.04909344406412569\n",
      "Reconstruction loss: 191.46637813815133, KL divergence: 0.01858362614238812\n",
      "Reconstruction loss: 209.87086821546652, KL divergence: 0.07797499639257305\n",
      "Reconstruction loss: 276.5542249737766, KL divergence: 0.11423280363472843\n",
      "Reconstruction loss: 202.5818223996663, KL divergence: 0.2585782756837601\n",
      "Reconstruction loss: 157.77648584961514, KL divergence: 0.030668422935492212\n",
      "Reconstruction loss: 229.63173705473875, KL divergence: 0.5797512706931858\n",
      "Reconstruction loss: 278.1456194699899, KL divergence: 2.113616184504852\n",
      "Reconstruction loss: 164.69429952422215, KL divergence: 0.05179718713290332\n",
      "Reconstruction loss: 156.85814446714198, KL divergence: 0.05393375645292259\n",
      "Reconstruction loss: 227.39376634496492, KL divergence: 2.5126196747254026\n",
      "Reconstruction loss: 213.21320221090832, KL divergence: 0.2261702989075841\n",
      "Reconstruction loss: 151.94812899015304, KL divergence: 0.130833658517166\n",
      "Reconstruction loss: 143.56805048456167, KL divergence: 0.06254357388798926\n",
      "Reconstruction loss: 205.60761409402704, KL divergence: 0.14952849407316476\n",
      "Reconstruction loss: 237.14995250730385, KL divergence: 0.36234904154661685\n",
      "Reconstruction loss: 232.85509808394707, KL divergence: 0.3059622735768782\n",
      "Reconstruction loss: 132.02496801161016, KL divergence: 0.062004646065550895\n",
      "Reconstruction loss: 200.33941062968236, KL divergence: 0.15470042792583089\n",
      "Reconstruction loss: 202.5031368288909, KL divergence: 0.04211757525042287\n",
      "Reconstruction loss: 235.29525079558687, KL divergence: 0.04487284728284813\n",
      "Reconstruction loss: 151.92537011506278, KL divergence: 0.07586810188926496\n",
      "Reconstruction loss: 217.04508438306272, KL divergence: 1.397945698359911\n",
      "Reconstruction loss: 209.25680373240132, KL divergence: 0.18941897410976594\n",
      "Reconstruction loss: 194.0810598378532, KL divergence: 0.09424284931324739\n",
      "Reconstruction loss: 296.9255354708945, KL divergence: 3.9649089082058\n",
      "Reconstruction loss: 257.8738192583045, KL divergence: 0.9592735599873436\n",
      "Reconstruction loss: 143.7584473250154, KL divergence: 0.10338034737473173\n",
      "Reconstruction loss: 212.81819903813562, KL divergence: 0.12244397686908431\n",
      "Reconstruction loss: 261.48534399932544, KL divergence: 2.99701892969172\n",
      "Reconstruction loss: 254.58379560942677, KL divergence: 0.33538923755650685\n",
      "Reconstruction loss: 205.06216349098656, KL divergence: 0.04236361562482649\n",
      "Reconstruction loss: 275.95752790489615, KL divergence: 1.4308413871440995\n",
      "Reconstruction loss: 271.2829799154133, KL divergence: 0.9963719830549435\n",
      "Reconstruction loss: 163.82650833256815, KL divergence: 0.09240648512966093\n",
      "Reconstruction loss: 263.2508470136979, KL divergence: 0.5974293127304815\n",
      "Reconstruction loss: 227.26031562378347, KL divergence: 0.12455363090050003\n",
      "Reconstruction loss: 202.3415739028216, KL divergence: 0.01627317703002823\n",
      "Reconstruction loss: 193.71463665964126, KL divergence: 0.020663982794160762\n",
      "Reconstruction loss: 227.18219859429152, KL divergence: 0.022557089921742546\n",
      "Reconstruction loss: 166.19043690657767, KL divergence: 0.0356021382631978\n",
      "Reconstruction loss: 259.35905817096955, KL divergence: 0.10205696244169077\n",
      "Reconstruction loss: 148.74414895890092, KL divergence: 0.14953943745060377\n",
      "Reconstruction loss: 151.02970687462482, KL divergence: 0.05264374956858747\n",
      "Reconstruction loss: 220.45720224316113, KL divergence: 0.055631734659476295\n",
      "Reconstruction loss: 230.6123617747825, KL divergence: 0.09972892260989563\n",
      "Reconstruction loss: 215.95282357471945, KL divergence: 0.021176977182501555\n",
      "Reconstruction loss: 250.4592753122235, KL divergence: 0.3060595117160889\n",
      "Reconstruction loss: 170.8181820696486, KL divergence: 0.01916019554409948\n",
      "Reconstruction loss: 264.87040851508044, KL divergence: 1.2231791905418625\n",
      "Reconstruction loss: 190.7594132670742, KL divergence: 0.06224292721418556\n",
      "Reconstruction loss: 176.47839470583176, KL divergence: 0.037352615016369495\n",
      "Reconstruction loss: 211.47049346764572, KL divergence: 0.023689897256873926\n",
      "Reconstruction loss: 162.76836591823712, KL divergence: 0.05264374956858747\n",
      "Reconstruction loss: 180.10734162959383, KL divergence: 0.12651777495191913\n",
      "Reconstruction loss: 273.9748740694984, KL divergence: 0.8194317398279638\n",
      "Reconstruction loss: 236.9284873900314, KL divergence: 0.021462431989717035\n",
      "Reconstruction loss: 277.35292331351076, KL divergence: 1.0124191451688191\n",
      "Reconstruction loss: 208.98944321922704, KL divergence: 0.05264374956858747\n",
      "Reconstruction loss: 142.94444501832533, KL divergence: 0.12021339856187735\n",
      "Reconstruction loss: 204.8346407869322, KL divergence: 0.04190603715644542\n",
      "Reconstruction loss: 217.03530939616903, KL divergence: 0.34262557973673746\n",
      "Reconstruction loss: 200.9687282205192, KL divergence: 0.05264374956858747\n",
      "Reconstruction loss: 180.71603697838646, KL divergence: 0.05264374956858747\n",
      "Reconstruction loss: 164.6367414378349, KL divergence: 0.05264374956858747\n",
      "Reconstruction loss: 228.39932784013988, KL divergence: 1.484099780555861\n",
      "Reconstruction loss: 149.36809066476837, KL divergence: 0.05264374956858747\n",
      "Reconstruction loss: 225.98746212034865, KL divergence: 0.031141452853605922\n",
      "Reconstruction loss: 252.77568364234457, KL divergence: 0.08659029040894545\n",
      "Reconstruction loss: 210.20410660997123, KL divergence: 0.9090050787636113\n",
      "Reconstruction loss: 164.79845997649596, KL divergence: 0.06736267928530376\n",
      "Reconstruction loss: 238.20620022683698, KL divergence: 0.04477854882683818\n",
      "Reconstruction loss: 282.9517505609782, KL divergence: 0.7329401183055015\n",
      "Reconstruction loss: 161.59341223248637, KL divergence: 0.17549940281847048\n",
      "Reconstruction loss: 227.79743133255948, KL divergence: 0.01858690621552045\n",
      "Reconstruction loss: 233.4464451313904, KL divergence: 0.9324994320267779\n",
      "Reconstruction loss: 159.11178386909455, KL divergence: 0.05475934314108938\n",
      "Reconstruction loss: 204.2473752150773, KL divergence: 0.11445866745242073\n",
      "Reconstruction loss: 224.54416537334447, KL divergence: 0.10665049334909232\n",
      "Reconstruction loss: 179.76398405583444, KL divergence: 0.04171365140793848\n",
      "Reconstruction loss: 188.79895900505358, KL divergence: 0.05475934314108938\n",
      "Reconstruction loss: 152.6774662161933, KL divergence: 0.05475934314108938\n",
      "Reconstruction loss: 154.55318873163688, KL divergence: 0.0880061535242474\n",
      "Reconstruction loss: 206.9318292922886, KL divergence: 0.09122800306876672\n",
      "Reconstruction loss: 216.5423712373264, KL divergence: 0.01995476851469874\n",
      "Reconstruction loss: 189.96692498937716, KL divergence: 0.026449371210689077\n",
      "Reconstruction loss: 176.25252168537213, KL divergence: 0.04136505716288463\n",
      "Reconstruction loss: 223.87686561722123, KL divergence: 0.101058661083399\n",
      "Reconstruction loss: 264.26601135685263, KL divergence: 0.07982022352487644\n",
      "Reconstruction loss: 273.52553655874783, KL divergence: 2.3022674941122183\n",
      "Reconstruction loss: 225.0624829155616, KL divergence: 0.033402339983194085\n",
      "Reconstruction loss: 204.80723032071887, KL divergence: 0.061135232711902465\n",
      "Reconstruction loss: 155.94375943893502, KL divergence: 0.1305686708725361\n",
      "Reconstruction loss: 221.44561273961108, KL divergence: 0.23048120374205527\n",
      "Reconstruction loss: 301.9865929134697, KL divergence: 0.06260163509322275\n",
      "Reconstruction loss: 166.9216478169244, KL divergence: 0.08633367277045756\n",
      "Reconstruction loss: 299.18486968852477, KL divergence: 0.04781147803051872\n",
      "Reconstruction loss: 225.1725580437128, KL divergence: 0.018030272784889334\n",
      "Reconstruction loss: 277.12187809970806, KL divergence: 0.44225232549766863\n",
      "Reconstruction loss: 176.25279845482567, KL divergence: 0.05475934314108938\n",
      "Reconstruction loss: 262.4723710213331, KL divergence: 0.016642735070943493\n",
      "Reconstruction loss: 201.71299202435657, KL divergence: 0.018165770756209787\n",
      "Reconstruction loss: 192.11241945768256, KL divergence: 0.05475934314108938\n",
      "Reconstruction loss: 164.58526406072872, KL divergence: 0.18491332066773275\n",
      "Reconstruction loss: 242.1954533574972, KL divergence: 0.02357452676151517\n",
      "Reconstruction loss: 284.80324093465373, KL divergence: 0.4981408111075587\n",
      "Reconstruction loss: 283.38000432314703, KL divergence: 0.44856815674903183\n",
      "Reconstruction loss: 184.14677414259182, KL divergence: 0.06322425097427459\n",
      "Reconstruction loss: 257.9814620828334, KL divergence: 0.3066927429765086\n",
      "Reconstruction loss: 154.1969645642018, KL divergence: 0.0858260720723445\n",
      "Reconstruction loss: 154.4835864353752, KL divergence: 0.09481713868659353\n",
      "Reconstruction loss: 227.05104245979447, KL divergence: 0.019756320682794815\n",
      "Reconstruction loss: 158.42857344468663, KL divergence: 0.057020906982826414\n",
      "Reconstruction loss: 226.72753006007397, KL divergence: 0.07292817713686212\n",
      "Reconstruction loss: 140.48549122865612, KL divergence: 0.07118513548038319\n",
      "Reconstruction loss: 167.31812311326354, KL divergence: 0.1326957983398872\n",
      "Reconstruction loss: 159.0451244799547, KL divergence: 0.05665357001743432\n",
      "Reconstruction loss: 201.1780744860339, KL divergence: 0.05665357001743432\n",
      "Reconstruction loss: 186.52935371243825, KL divergence: 0.27756031898255185\n",
      "Reconstruction loss: 193.31928398090793, KL divergence: 0.022080211647914127\n",
      "Reconstruction loss: 224.15288839607615, KL divergence: 0.01861046852578091\n",
      "Reconstruction loss: 195.17815214268805, KL divergence: 0.07517543352268513\n",
      "Reconstruction loss: 145.88509894060797, KL divergence: 0.24247744199498122\n",
      "Reconstruction loss: 235.39457699875828, KL divergence: 0.05665357001743432\n",
      "Reconstruction loss: 187.877061066904, KL divergence: 0.0446863589723242\n",
      "Reconstruction loss: 310.37497029367427, KL divergence: 0.018255073349184747\n",
      "Reconstruction loss: 150.89718608773038, KL divergence: 0.2117841262372241\n",
      "Reconstruction loss: 191.29501962481254, KL divergence: 0.0332524712037075\n",
      "Reconstruction loss: 243.37700858024454, KL divergence: 1.5605245373415202\n",
      "Reconstruction loss: 278.2517037835269, KL divergence: 0.7376303683723026\n",
      "Reconstruction loss: 262.1976807337352, KL divergence: 0.2130248664689557\n",
      "Reconstruction loss: 169.7828438664063, KL divergence: 0.09759562539534716\n",
      "Reconstruction loss: 201.35327924664028, KL divergence: 0.024239660672986052\n",
      "Reconstruction loss: 262.70096977906974, KL divergence: 0.10987206304188318\n",
      "Reconstruction loss: 164.46858817594477, KL divergence: 0.0651962834310379\n",
      "Reconstruction loss: 193.23040313906705, KL divergence: 0.06954865967278956\n",
      "Reconstruction loss: 357.64270550623155, KL divergence: 0.41960520885588415\n",
      "Reconstruction loss: 210.66473864986523, KL divergence: 0.021654698633991887\n",
      "Reconstruction loss: 272.8692224976699, KL divergence: 0.025214059415272827\n",
      "Reconstruction loss: 231.58220885207825, KL divergence: 0.061875490193588234\n",
      "Reconstruction loss: 159.0605091744415, KL divergence: 0.09227539325598894\n",
      "Reconstruction loss: 204.21930080996904, KL divergence: 0.05833945928708789\n",
      "Reconstruction loss: 175.71238965901594, KL divergence: 0.07216989659946571\n",
      "Reconstruction loss: 146.9045442277038, KL divergence: 0.12778382550364514\n",
      "Reconstruction loss: 209.302412825994, KL divergence: 0.05833945928708789\n",
      "Reconstruction loss: 181.13636536029276, KL divergence: 0.0653332994583341\n",
      "Reconstruction loss: 193.32446440820428, KL divergence: 0.05233047311785238\n",
      "Reconstruction loss: 140.90499573037079, KL divergence: 0.13870720962537686\n",
      "Reconstruction loss: 154.05338104890603, KL divergence: 0.07954241875545615\n",
      "Reconstruction loss: 127.06290340590576, KL divergence: 0.08622607031663798\n",
      "Reconstruction loss: 158.7716083237688, KL divergence: 0.05833945928708789\n",
      "Reconstruction loss: 215.55816828743332, KL divergence: 0.05833945928708789\n",
      "Reconstruction loss: 200.75414772442355, KL divergence: 0.018069003573235987\n",
      "Reconstruction loss: 277.34569769352913, KL divergence: 0.217744871114457\n",
      "Reconstruction loss: 115.70036480472828, KL divergence: 0.1663159410901539\n",
      "Reconstruction loss: 280.73746746035727, KL divergence: 0.08245532092260688\n",
      "Reconstruction loss: 250.74237649236633, KL divergence: 0.326029155671369\n",
      "Reconstruction loss: 136.946280425017, KL divergence: 0.1869470282140937\n",
      "Reconstruction loss: 196.6741136108509, KL divergence: 0.05833945928708789\n",
      "Reconstruction loss: 196.5121170020379, KL divergence: 0.05729010268423956\n",
      "Reconstruction loss: 203.8752709843947, KL divergence: 0.018059391550933113\n",
      "Reconstruction loss: 256.9138170444521, KL divergence: 1.9366942646250043\n",
      "Reconstruction loss: 279.5670048055059, KL divergence: 2.510813572588941\n",
      "Reconstruction loss: 308.7823980714253, KL divergence: 0.6702876197834748\n",
      "Reconstruction loss: 242.99124039114554, KL divergence: 0.4602251894510709\n",
      "Reconstruction loss: 163.12534241067365, KL divergence: 0.05833945928708789\n",
      "Reconstruction loss: 292.4291274215816, KL divergence: 0.5245107933050445\n",
      "Reconstruction loss: 222.0386904298455, KL divergence: 0.07745568445020096\n",
      "Reconstruction loss: 250.35437867417073, KL divergence: 0.05833945928708789\n",
      "Reconstruction loss: 192.32245473521255, KL divergence: 0.07560022732715604\n",
      "Reconstruction loss: 195.9878792991243, KL divergence: 0.059752146409112916\n",
      "Reconstruction loss: 188.70219879248182, KL divergence: 0.059752146409112916\n",
      "Reconstruction loss: 252.84855595102172, KL divergence: 0.09908911638772078\n",
      "Reconstruction loss: 160.24358876042504, KL divergence: 0.2025088750136234\n",
      "Reconstruction loss: 258.58936215362377, KL divergence: 0.021078343856957837\n",
      "Reconstruction loss: 232.77048826900412, KL divergence: 0.09301161362389043\n",
      "Reconstruction loss: 239.40975428847392, KL divergence: 0.17443237419983892\n",
      "Reconstruction loss: 249.23340790783357, KL divergence: 0.052770453012456475\n",
      "Reconstruction loss: 220.15916734323037, KL divergence: 0.021436041153315266\n",
      "Reconstruction loss: 222.76598765347015, KL divergence: 0.059752146409112916\n",
      "Reconstruction loss: 166.19032671492747, KL divergence: 0.06637036352136039\n",
      "Reconstruction loss: 175.9946610987443, KL divergence: 0.08037517619515283\n",
      "Reconstruction loss: 220.49582821334786, KL divergence: 0.016547879855746073\n",
      "Reconstruction loss: 236.74670865860566, KL divergence: 0.6515224789146605\n",
      "Reconstruction loss: 187.68967564213875, KL divergence: 0.059752146409112916\n",
      "Reconstruction loss: 277.66073661584653, KL divergence: 0.8477357269373852\n",
      "Reconstruction loss: 285.6823979200241, KL divergence: 0.3488779475706218\n",
      "Reconstruction loss: 327.48224950409593, KL divergence: 0.6243388626635258\n",
      "Reconstruction loss: 174.74512993286, KL divergence: 0.06389239285223719\n",
      "Reconstruction loss: 223.56640801907866, KL divergence: 0.01773097727440237\n",
      "Reconstruction loss: 269.7210948848678, KL divergence: 0.019072182711590113\n",
      "Reconstruction loss: 293.8582875992299, KL divergence: 0.07846891684124108\n",
      "Reconstruction loss: 233.36441131639262, KL divergence: 0.05035876241972842\n",
      "Reconstruction loss: 178.55545883397792, KL divergence: 0.10237864495170701\n",
      "Reconstruction loss: 246.54857646243147, KL divergence: 0.02276412367785574\n",
      "Reconstruction loss: 209.97401891585884, KL divergence: 0.019578857506255654\n",
      "Reconstruction loss: 128.6390965024285, KL divergence: 0.06748969200928961\n",
      "Reconstruction loss: 216.22918728136855, KL divergence: 0.059752146409112916\n",
      "Reconstruction loss: 232.0583534060878, KL divergence: 0.05334507777781644\n",
      "Reconstruction loss: 192.60396965287222, KL divergence: 0.05075273003026115\n",
      "Reconstruction loss: 162.5622402003837, KL divergence: 0.059752146409112916\n",
      "Reconstruction loss: 286.84581143220873, KL divergence: 0.048791728770484155\n",
      "Reconstruction loss: 142.53703694464537, KL divergence: 0.24663782294191988\n",
      "Reconstruction loss: 205.03367172329263, KL divergence: 0.017989602387075643\n",
      "Reconstruction loss: 187.91309542321207, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 159.9757691250402, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 239.4009853923826, KL divergence: 2.1317963059262626\n",
      "Reconstruction loss: 218.38037607273571, KL divergence: 0.019880023678655045\n",
      "Reconstruction loss: 224.5129861967577, KL divergence: 0.017838047755188102\n",
      "Reconstruction loss: 248.02361484744102, KL divergence: 0.04640347427372832\n",
      "Reconstruction loss: 279.85466130549275, KL divergence: 0.0639263826546389\n",
      "Reconstruction loss: 182.1923065258246, KL divergence: 0.0676749522204933\n",
      "Reconstruction loss: 196.0324083893403, KL divergence: 0.07022909482344292\n",
      "Reconstruction loss: 187.2335722454743, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 304.235009118225, KL divergence: 0.09859213711525877\n",
      "Reconstruction loss: 198.0911455848071, KL divergence: 0.08872517343491143\n",
      "Reconstruction loss: 206.4544531297089, KL divergence: 0.06105759206613137\n",
      "Reconstruction loss: 232.91394110945225, KL divergence: 0.05077782742956244\n",
      "Reconstruction loss: 230.75197567756777, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 258.53013362381586, KL divergence: 1.1508416184492698\n",
      "Reconstruction loss: 249.77079897926154, KL divergence: 0.6456304483115927\n",
      "Reconstruction loss: 200.12295160782782, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 187.31973164258002, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 135.66095343998973, KL divergence: 0.08756751748059721\n",
      "Reconstruction loss: 171.1094747424561, KL divergence: 0.11942675692382299\n",
      "Reconstruction loss: 243.34161933967084, KL divergence: 1.8572405173945112\n",
      "Reconstruction loss: 187.33858493478186, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 243.19819858873888, KL divergence: 0.4134588782708825\n",
      "Reconstruction loss: 211.1055505505566, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 273.2940855632095, KL divergence: 0.18137873051917203\n",
      "Reconstruction loss: 137.27916066699578, KL divergence: 0.08257833789356395\n",
      "Reconstruction loss: 188.03114031628616, KL divergence: 0.06075333740819677\n",
      "Reconstruction loss: 155.5008125857263, KL divergence: 0.12809029845037234\n",
      "Reconstruction loss: 152.84461310877452, KL divergence: 0.11976170853037466\n",
      "Reconstruction loss: 198.33418700975807, KL divergence: 0.017288130172668448\n",
      "Reconstruction loss: 149.0045163967865, KL divergence: 0.10288285555987864\n",
      "Reconstruction loss: 139.11645708585667, KL divergence: 0.08216548671161455\n",
      "Reconstruction loss: 270.7660427498457, KL divergence: 0.4114842234250732\n",
      "Reconstruction loss: 235.9334956377111, KL divergence: 0.017225689370005404\n",
      "Reconstruction loss: 273.0050835057049, KL divergence: 0.022852448156501004\n",
      "Reconstruction loss: 210.08921599777784, KL divergence: 0.07266375643263706\n",
      "Reconstruction loss: 172.20246381939683, KL divergence: 0.06138085165707341\n",
      "Reconstruction loss: 162.7172198421207, KL divergence: 0.07479247628512498\n",
      "Reconstruction loss: 206.79092277053076, KL divergence: 0.023333861893714336\n",
      "Reconstruction loss: 178.0152504706482, KL divergence: 0.06138085165707341\n",
      "Reconstruction loss: 178.301794558141, KL divergence: 0.06138085165707341\n",
      "Reconstruction loss: 273.7224700643604, KL divergence: 0.2826154613775225\n",
      "Reconstruction loss: 234.76312103841485, KL divergence: 0.1898023325095587\n",
      "Reconstruction loss: 179.02604720088283, KL divergence: 0.10085685043345166\n",
      "Reconstruction loss: 155.37932752623124, KL divergence: 0.06213669122942361\n",
      "Reconstruction loss: 282.2057047138427, KL divergence: 0.032117447044155645\n",
      "Reconstruction loss: 184.65547303800216, KL divergence: 0.06138085165707341\n",
      "Reconstruction loss: 189.36962217618904, KL divergence: 0.24224620497992672\n",
      "Reconstruction loss: 167.71639849199624, KL divergence: 0.06829669317556664\n",
      "Reconstruction loss: 205.31807287286523, KL divergence: 0.11538826199542696\n",
      "Reconstruction loss: 169.47173998784612, KL divergence: 0.06138085165707341\n",
      "Reconstruction loss: 247.01280767571075, KL divergence: 0.11525062310042661\n",
      "Reconstruction loss: 198.9483905573669, KL divergence: 0.026866438840702722\n",
      "Reconstruction loss: 191.90020584944241, KL divergence: 0.09423461733821342\n",
      "Reconstruction loss: 186.2054061051502, KL divergence: 0.06881162215404829\n",
      "Reconstruction loss: 194.03079723444043, KL divergence: 0.022021427145674954\n",
      "Reconstruction loss: 173.05550208374453, KL divergence: 0.06445797107097284\n",
      "Reconstruction loss: 216.69106697737107, KL divergence: 0.10992851313028057\n",
      "Reconstruction loss: 230.01858037127468, KL divergence: 1.7204456083337005\n",
      "Reconstruction loss: 158.8110665272215, KL divergence: 0.06441049911119162\n",
      "Reconstruction loss: 180.86339829911256, KL divergence: 0.02308316372186786\n",
      "Reconstruction loss: 241.03169979121435, KL divergence: 0.021018430933926968\n",
      "Reconstruction loss: 169.83176422279288, KL divergence: 0.062122129002269544\n",
      "Reconstruction loss: 218.94615804610476, KL divergence: 0.0341173468883294\n",
      "Reconstruction loss: 239.53004778994853, KL divergence: 0.7815201115238491\n",
      "Reconstruction loss: 185.5645932256958, KL divergence: 0.07267198472014774\n",
      "Reconstruction loss: 246.87548808194418, KL divergence: 0.23645969149939128\n",
      "Reconstruction loss: 163.66316073460854, KL divergence: 0.09164484668340639\n",
      "Reconstruction loss: 351.11277555240184, KL divergence: 1.5793851981272742\n",
      "Reconstruction loss: 308.5559561949448, KL divergence: 1.3581387391334845\n",
      "Reconstruction loss: 225.47333180868807, KL divergence: 0.05604705378087693\n",
      "Reconstruction loss: 186.50838917238877, KL divergence: 0.062122129002269544\n",
      "Reconstruction loss: 208.80713764838515, KL divergence: 0.021324138584653096\n",
      "Reconstruction loss: 166.9141967056362, KL divergence: 0.0705112124943914\n",
      "Reconstruction loss: 135.31070169086718, KL divergence: 0.19290359458691947\n",
      "Reconstruction loss: 216.92564643381763, KL divergence: 0.6999625996731351\n",
      "Reconstruction loss: 346.88478522390596, KL divergence: 2.4260846117260018\n",
      "Reconstruction loss: 185.18721966585554, KL divergence: 0.028352712603801622\n",
      "Reconstruction loss: 190.11981910020657, KL divergence: 0.022259282972448946\n",
      "Reconstruction loss: 229.342936175948, KL divergence: 0.5502495928581801\n",
      "Reconstruction loss: 178.85133401001843, KL divergence: 0.05027694507927993\n",
      "Reconstruction loss: 114.68365987854266, KL divergence: 0.23220693317321067\n",
      "Reconstruction loss: 179.18266206394168, KL divergence: 0.027600408498088425\n",
      "Reconstruction loss: 210.3042139983813, KL divergence: 0.09883980523936137\n",
      "Reconstruction loss: 232.63365286436726, KL divergence: 0.3833040730519034\n",
      "Reconstruction loss: 223.25109555094377, KL divergence: 0.11394401803125387\n",
      "Reconstruction loss: 152.8446180500726, KL divergence: 0.062122129002269544\n",
      "Reconstruction loss: 160.9727706268884, KL divergence: 0.05188910366691013\n",
      "Reconstruction loss: 228.68972478354584, KL divergence: 0.01793987171593503\n",
      "Reconstruction loss: 221.15899426117238, KL divergence: 1.533660180758257\n",
      "Reconstruction loss: 194.40497374872712, KL divergence: 0.1560163932627776\n",
      "Reconstruction loss: 361.30991505611826, KL divergence: 0.1700400624173543\n",
      "Reconstruction loss: 175.06827715331008, KL divergence: 0.02009459697779714\n",
      "Reconstruction loss: 161.9666507086912, KL divergence: 0.050578704618848225\n",
      "Reconstruction loss: 215.97763652874863, KL divergence: 0.018299248578199978\n",
      "Reconstruction loss: 170.22384768406476, KL divergence: 0.06510043632179086\n",
      "Reconstruction loss: 232.0799694936725, KL divergence: 0.017056577654624316\n",
      "Reconstruction loss: 297.94520167688506, KL divergence: 0.4272515305799673\n",
      "Reconstruction loss: 282.79776427639933, KL divergence: 0.7425415964552393\n",
      "Reconstruction loss: 235.54201915275067, KL divergence: 0.7732092457856466\n",
      "Reconstruction loss: 281.26383660454735, KL divergence: 0.21384657819003666\n",
      "Reconstruction loss: 161.1545257713417, KL divergence: 0.17290509576891444\n",
      "Reconstruction loss: 199.87321463658407, KL divergence: 0.027720159475092654\n",
      "Reconstruction loss: 123.49194503333968, KL divergence: 0.06282820876256012\n",
      "Reconstruction loss: 227.4128097841371, KL divergence: 0.40701488752775056\n",
      "Reconstruction loss: 133.01084734127733, KL divergence: 0.15335097683343824\n",
      "Reconstruction loss: 182.67880714171258, KL divergence: 0.10647884247687761\n",
      "Reconstruction loss: 161.115878419635, KL divergence: 0.16574471220554648\n",
      "Reconstruction loss: 176.74403856659293, KL divergence: 0.06282820876256012\n",
      "Reconstruction loss: 147.6571975110415, KL divergence: 0.07558256920260797\n",
      "Reconstruction loss: 236.67954585814257, KL divergence: 0.018346047632943285\n",
      "Reconstruction loss: 236.01865950755325, KL divergence: 1.9630858789536862\n",
      "Reconstruction loss: 196.40257427587372, KL divergence: 0.024418559447968058\n",
      "Reconstruction loss: 198.02660141497046, KL divergence: 0.08802381045810337\n",
      "Reconstruction loss: 162.65384578088012, KL divergence: 0.06282820876256012\n",
      "Reconstruction loss: 242.97570358064365, KL divergence: 0.09079138846774093\n",
      "Reconstruction loss: 196.96134788124021, KL divergence: 0.08892664435711345\n",
      "Reconstruction loss: 222.85822079903565, KL divergence: 0.5106320299294356\n",
      "Reconstruction loss: 198.65676306508544, KL divergence: 0.07190336637562661\n",
      "Reconstruction loss: 230.49154067777806, KL divergence: 0.056974674839075645\n",
      "Reconstruction loss: 287.8529794962727, KL divergence: 0.13115015439703004\n",
      "Reconstruction loss: 214.85393784325515, KL divergence: 0.07839615642976738\n",
      "Reconstruction loss: 249.64325846628648, KL divergence: 0.8228335261568553\n",
      "Reconstruction loss: 172.09023965160978, KL divergence: 0.09741258368426287\n",
      "Reconstruction loss: 199.9435021151756, KL divergence: 0.01645514999507658\n",
      "Reconstruction loss: 245.1127651572953, KL divergence: 0.6000627430721448\n",
      "Reconstruction loss: 243.36887902491148, KL divergence: 0.20200026430562595\n",
      "Reconstruction loss: 175.9337436630678, KL divergence: 0.0637917254116\n",
      "Reconstruction loss: 149.54009984755714, KL divergence: 0.08096156221140294\n",
      "Reconstruction loss: 206.0197359261546, KL divergence: 0.2101925066700902\n",
      "Reconstruction loss: 155.24004638188424, KL divergence: 0.10013023117283187\n",
      "Reconstruction loss: 229.38941558425523, KL divergence: 0.41588025210385504\n",
      "Reconstruction loss: 167.73932474634913, KL divergence: 0.02420679782578955\n",
      "Reconstruction loss: 283.1446735309643, KL divergence: 2.4565851983830167\n",
      "Reconstruction loss: 277.80028530861944, KL divergence: 2.700933843110155\n",
      "Reconstruction loss: 241.69041866212336, KL divergence: 0.2656896279554\n",
      "Reconstruction loss: 186.24142078712774, KL divergence: 0.0893587173437756\n",
      "Reconstruction loss: 196.71725876716823, KL divergence: 0.3535385456428213\n",
      "Reconstruction loss: 185.7908220113849, KL divergence: 0.03138360325847195\n",
      "Reconstruction loss: 202.16892208429198, KL divergence: 0.2186731118126196\n",
      "Reconstruction loss: 139.3395790402674, KL divergence: 0.11309155962623157\n",
      "Reconstruction loss: 200.30908309545788, KL divergence: 0.017011593865984054\n",
      "Reconstruction loss: 206.76478207621528, KL divergence: 0.13346881098378344\n",
      "Reconstruction loss: 207.2687669629943, KL divergence: 0.0637917254116\n",
      "Reconstruction loss: 162.30481790935366, KL divergence: 0.0637917254116\n",
      "Reconstruction loss: 185.88952205264445, KL divergence: 0.13491390751139348\n",
      "Reconstruction loss: 184.484670278912, KL divergence: 0.01684025762509761\n",
      "Reconstruction loss: 183.2910842727132, KL divergence: 0.02145264305422756\n",
      "Reconstruction loss: 168.15911550751275, KL divergence: 0.016662629344370627\n",
      "Reconstruction loss: 200.51424581987362, KL divergence: 0.12759732331756685\n",
      "Reconstruction loss: 122.93157171410857, KL divergence: 0.08106641828288674\n",
      "Reconstruction loss: 174.06056133914078, KL divergence: 0.07593110499214079\n",
      "Reconstruction loss: 245.51313129594533, KL divergence: 0.7437462130340757\n",
      "Reconstruction loss: 174.63865292831747, KL divergence: 0.0637917254116\n",
      "Reconstruction loss: 244.40774215903718, KL divergence: 1.0391486826291625\n",
      "Reconstruction loss: 173.35469166259338, KL divergence: 0.03083900722876981\n",
      "Reconstruction loss: 207.3843737980054, KL divergence: 0.0652134030230403\n",
      "Reconstruction loss: 224.2394679258266, KL divergence: 0.03209862053942647\n",
      "Reconstruction loss: 266.3474977157043, KL divergence: 0.5735818376684992\n",
      "Reconstruction loss: 207.5720738044697, KL divergence: 0.5666956638317888\n",
      "Reconstruction loss: 163.65807843692716, KL divergence: 0.03649536575887152\n",
      "Reconstruction loss: 190.9721765318451, KL divergence: 0.0652134030230403\n",
      "Reconstruction loss: 276.47085705841323, KL divergence: 2.98153863980009\n",
      "Reconstruction loss: 260.9884152408601, KL divergence: 1.5028210253406065\n",
      "Reconstruction loss: 217.38319411091572, KL divergence: 0.4320096581850417\n",
      "Reconstruction loss: 124.17683429234046, KL divergence: 0.0652134030230403\n",
      "Reconstruction loss: 150.307398078676, KL divergence: 0.09091477023844874\n",
      "Reconstruction loss: 152.97346434610768, KL divergence: 0.04519423986082988\n",
      "Reconstruction loss: 174.0920436884744, KL divergence: 0.0790208172221547\n",
      "Reconstruction loss: 198.75790226905116, KL divergence: 0.17720849649319237\n",
      "Reconstruction loss: 238.4307313894713, KL divergence: 2.0980827927453873\n",
      "Reconstruction loss: 196.51965496620568, KL divergence: 0.02605619276205401\n",
      "Reconstruction loss: 200.97335638695864, KL divergence: 0.13867542616273137\n",
      "Reconstruction loss: 148.05245986821183, KL divergence: 0.054341074389043564\n",
      "Reconstruction loss: 260.21573248909465, KL divergence: 0.2057423856447072\n",
      "Reconstruction loss: 182.83225838236794, KL divergence: 0.06021508176764434\n",
      "Reconstruction loss: 256.46099739563795, KL divergence: 2.419615682478297\n",
      "Reconstruction loss: 182.75669642816638, KL divergence: 0.01989125400820707\n",
      "Reconstruction loss: 184.24666835578023, KL divergence: 0.016252043006853956\n",
      "Reconstruction loss: 150.00257371319577, KL divergence: 0.02712449089325214\n",
      "Reconstruction loss: 200.2625232320264, KL divergence: 0.37893312671014545\n",
      "Reconstruction loss: 156.7005910952936, KL divergence: 0.0567561093695364\n",
      "Reconstruction loss: 128.91869694687628, KL divergence: 0.07487538550188594\n",
      "Reconstruction loss: 167.9502550293226, KL divergence: 0.022791574768256606\n",
      "Reconstruction loss: 340.25537792778533, KL divergence: 0.8308629427645409\n",
      "Reconstruction loss: 259.7497137983106, KL divergence: 0.0652134030230403\n",
      "Reconstruction loss: 307.0549972080238, KL divergence: 0.9152980267587711\n",
      "Reconstruction loss: 168.49465549197248, KL divergence: 0.08455938502882321\n",
      "Reconstruction loss: 191.72549169483295, KL divergence: 0.3177754005110812\n",
      "Reconstruction loss: 167.8374649772819, KL divergence: 0.02062701739173578\n",
      "Reconstruction loss: 240.35962065048568, KL divergence: 1.0096969109850238\n",
      "Reconstruction loss: 176.8889735572581, KL divergence: 0.01604062792204275\n",
      "Reconstruction loss: 209.94199548760753, KL divergence: 0.0842843888003173\n",
      "Reconstruction loss: 235.06737223505087, KL divergence: 1.3903144293603567\n",
      "Reconstruction loss: 204.49835049746343, KL divergence: 0.3627962309389881\n",
      "Reconstruction loss: 203.89648813744532, KL divergence: 0.02101325172005497\n",
      "Reconstruction loss: 183.19813930980632, KL divergence: 0.016134483497337093\n",
      "Reconstruction loss: 155.36640108632534, KL divergence: 0.02506987452057985\n",
      "Reconstruction loss: 259.64888526101527, KL divergence: 0.7951873950580591\n",
      "Reconstruction loss: 269.6115652471408, KL divergence: 0.6415833724238424\n",
      "Reconstruction loss: 129.86091394431722, KL divergence: 0.07568038920694165\n",
      "Reconstruction loss: 200.50371016858412, KL divergence: 0.598600505913174\n",
      "Reconstruction loss: 202.74324219684252, KL divergence: 2.2057025614932497\n",
      "Reconstruction loss: 288.27403306077264, KL divergence: 0.34827726269880316\n",
      "Reconstruction loss: 147.51121114326816, KL divergence: 0.10271291657735682\n",
      "Reconstruction loss: 188.97613386434597, KL divergence: 0.0666484660423739\n",
      "Reconstruction loss: 208.16731361952733, KL divergence: 0.3581325714285216\n",
      "Reconstruction loss: 204.56190436719973, KL divergence: 0.049611033807192284\n",
      "Reconstruction loss: 221.59654757570448, KL divergence: 2.251663414990222\n",
      "Reconstruction loss: 219.16968440429503, KL divergence: 0.2159016185885722\n",
      "Reconstruction loss: 118.96334051157352, KL divergence: 0.17472892713040566\n",
      "Reconstruction loss: 213.93880417597148, KL divergence: 0.43571939710026913\n",
      "Reconstruction loss: 191.72171727574545, KL divergence: 0.05757729179999277\n",
      "Reconstruction loss: 180.6773910707791, KL divergence: 0.07976927909760861\n",
      "Reconstruction loss: 152.7598070045648, KL divergence: 0.04922346771226799\n",
      "Reconstruction loss: 164.56737795616385, KL divergence: 0.017445141758807015\n",
      "Reconstruction loss: 254.53516344589437, KL divergence: 0.7041263489504674\n",
      "Reconstruction loss: 139.76242709730684, KL divergence: 0.0666484660423739\n",
      "Reconstruction loss: 220.4745525725789, KL divergence: 0.06080533260092935\n",
      "Reconstruction loss: 179.1096251218028, KL divergence: 0.0630033192586294\n",
      "Reconstruction loss: 207.3426455165242, KL divergence: 0.026902649455062766\n",
      "Reconstruction loss: 216.51501182424528, KL divergence: 0.03453930669287808\n",
      "Reconstruction loss: 287.0350473914056, KL divergence: 0.6744096882240123\n",
      "Reconstruction loss: 159.2848095415986, KL divergence: 0.04992619540492793\n",
      "Reconstruction loss: 149.36356216588845, KL divergence: 0.14046414743400965\n",
      "Reconstruction loss: 176.0660355849264, KL divergence: 0.023434893196093654\n",
      "Reconstruction loss: 124.21861297286412, KL divergence: 0.07481203811192477\n",
      "Reconstruction loss: 210.36964633275176, KL divergence: 0.336139051196191\n",
      "Reconstruction loss: 162.04047994485984, KL divergence: 0.12880247131449457\n",
      "Reconstruction loss: 150.85721275486895, KL divergence: 0.06815302155644204\n",
      "Reconstruction loss: 258.97128228949055, KL divergence: 1.0063861958652174\n",
      "Reconstruction loss: 243.81107218619047, KL divergence: 0.2055075537252385\n",
      "Reconstruction loss: 158.0236649522745, KL divergence: 0.06815302155644204\n",
      "Reconstruction loss: 198.11001965214038, KL divergence: 0.0254130080988636\n",
      "Reconstruction loss: 144.31301377196309, KL divergence: 0.12776043676706444\n",
      "Reconstruction loss: 155.16607339573002, KL divergence: 0.10788378749896599\n",
      "Reconstruction loss: 254.56634602149327, KL divergence: 1.3936931464886253\n",
      "Reconstruction loss: 230.84093037615852, KL divergence: 0.06815302155644204\n",
      "Reconstruction loss: 137.41303120564208, KL divergence: 0.06839701058008502\n",
      "Reconstruction loss: 168.39950460323254, KL divergence: 0.03317885824662731\n",
      "Reconstruction loss: 155.98304244005152, KL divergence: 0.12867167771512766\n",
      "Reconstruction loss: 155.6950756451281, KL divergence: 0.052608580273009686\n",
      "Reconstruction loss: 185.95122129327223, KL divergence: 0.06815302155644204\n",
      "Reconstruction loss: 206.0347925636279, KL divergence: 0.619484677252075\n",
      "Reconstruction loss: 220.17257588063444, KL divergence: 0.07028461049922446\n",
      "Reconstruction loss: 198.649299604479, KL divergence: 0.3201949614373443\n",
      "Reconstruction loss: 164.951766858493, KL divergence: 0.03878160283239762\n",
      "Reconstruction loss: 158.39088062547552, KL divergence: 0.11018038802608676\n",
      "Reconstruction loss: 133.48832209966318, KL divergence: 0.10107667795741643\n",
      "Reconstruction loss: 146.50071286908226, KL divergence: 0.01805546261481744\n",
      "Reconstruction loss: 200.15692428384773, KL divergence: 0.14926909472537303\n",
      "Reconstruction loss: 134.0761724832094, KL divergence: 0.06702631209782073\n",
      "Reconstruction loss: 249.11054788878042, KL divergence: 0.5238565246192741\n",
      "Reconstruction loss: 139.31750042134684, KL divergence: 0.07466405956882377\n",
      "Reconstruction loss: 210.95907697370416, KL divergence: 0.06996823172144279\n",
      "Reconstruction loss: 245.79160765742694, KL divergence: 0.38670772992504265\n",
      "Reconstruction loss: 111.0866593547649, KL divergence: 0.09896055261283104\n",
      "Reconstruction loss: 163.38674281355242, KL divergence: 0.08161467203116823\n",
      "Reconstruction loss: 221.68334788893472, KL divergence: 1.8323786250234255\n",
      "Reconstruction loss: 368.89248080012476, KL divergence: 1.470606258690196\n",
      "Reconstruction loss: 260.1764194340972, KL divergence: 0.3323013510810004\n",
      "Reconstruction loss: 233.78770461061254, KL divergence: 0.15954950812032764\n",
      "Reconstruction loss: 224.34894253554288, KL divergence: 0.6196559647089226\n",
      "Reconstruction loss: 154.5816659455397, KL divergence: 0.0597806945826585\n",
      "Reconstruction loss: 197.6651854901331, KL divergence: 1.4047718402276572\n",
      "Reconstruction loss: 215.27666770880765, KL divergence: 0.021738459203922644\n",
      "Reconstruction loss: 291.82085086047226, KL divergence: 2.5973139202492566\n",
      "Reconstruction loss: 286.6983971583081, KL divergence: 1.4852739125998862\n",
      "Reconstruction loss: 160.01305711107477, KL divergence: 0.06996823172144279\n",
      "Reconstruction loss: 253.02444964224335, KL divergence: 1.5801647880379486\n",
      "Reconstruction loss: 278.8535566062356, KL divergence: 0.7256777334915039\n",
      "Reconstruction loss: 216.1828346391992, KL divergence: 0.15110991042058036\n",
      "Reconstruction loss: 190.97351868662747, KL divergence: 0.06107037628465972\n",
      "Reconstruction loss: 224.64416022774176, KL divergence: 0.4791726059222309\n",
      "Reconstruction loss: 188.43634162057313, KL divergence: 0.01778643141942876\n",
      "Reconstruction loss: 133.69540786290526, KL divergence: 0.10676801717174511\n",
      "Reconstruction loss: 278.01951923384746, KL divergence: 0.10771077960738729\n",
      "Reconstruction loss: 202.7138867254816, KL divergence: 0.019155043597386012\n",
      "Reconstruction loss: 250.1639794233228, KL divergence: 0.27932173623090195\n",
      "Reconstruction loss: 238.8894744520735, KL divergence: 0.10115138391833678\n",
      "Reconstruction loss: 265.9727384141521, KL divergence: 1.7171932353008994\n",
      "Reconstruction loss: 179.1502970181382, KL divergence: 0.07123838961504514\n",
      "Reconstruction loss: 217.12862484757414, KL divergence: 0.12858948393950154\n",
      "Reconstruction loss: 292.2480023070751, KL divergence: 1.8319204105231623\n",
      "Reconstruction loss: 131.48092537178485, KL divergence: 0.11263950089575953\n",
      "Reconstruction loss: 237.09390478909108, KL divergence: 0.09945444675008608\n",
      "Reconstruction loss: 155.03314638836494, KL divergence: 0.060110892449849296\n",
      "Reconstruction loss: 282.9156676848893, KL divergence: 2.437238178704132\n",
      "Reconstruction loss: 277.26402435674873, KL divergence: 0.015831694009008912\n",
      "Reconstruction loss: 221.36152963082452, KL divergence: 0.30568342790061526\n",
      "Reconstruction loss: 170.44209359045908, KL divergence: 0.018322416746100467\n",
      "Reconstruction loss: 203.0677910372433, KL divergence: 0.08853209262386563\n",
      "Reconstruction loss: 134.15237875519887, KL divergence: 0.0780247548325379\n",
      "Reconstruction loss: 293.17139665324004, KL divergence: 1.5986616088761827\n",
      "Reconstruction loss: 103.7965751674287, KL divergence: 0.07819203520691509\n",
      "Reconstruction loss: 270.98457076342584, KL divergence: 1.8666708983459857\n",
      "Reconstruction loss: 246.50684013240533, KL divergence: 0.24575242859257768\n",
      "Reconstruction loss: 207.1596633729766, KL divergence: 0.09576347052329226\n",
      "Reconstruction loss: 145.873598524869, KL divergence: 0.07162371976299603\n",
      "Reconstruction loss: 187.79491189007513, KL divergence: 0.03716081553453532\n",
      "Reconstruction loss: 242.90960919943024, KL divergence: 0.15073282516180514\n",
      "Reconstruction loss: 157.7097643315023, KL divergence: 0.10509401091942078\n",
      "Reconstruction loss: 179.26757006379057, KL divergence: 0.06957270281061184\n",
      "Reconstruction loss: 210.05575799435917, KL divergence: 0.07162371976299603\n",
      "Reconstruction loss: 165.676465091835, KL divergence: 0.0272820429650798\n",
      "Reconstruction loss: 167.4905464150471, KL divergence: 0.038417388594196133\n",
      "Reconstruction loss: 247.06410862192376, KL divergence: 0.5017678078865977\n",
      "Reconstruction loss: 141.26251340322182, KL divergence: 0.13718213505126653\n",
      "Reconstruction loss: 240.02338718636278, KL divergence: 1.0385330488536095\n",
      "Reconstruction loss: 175.01093459436416, KL divergence: 0.01747136972749136\n",
      "Reconstruction loss: 175.23007166989896, KL divergence: 0.07162371976299603\n",
      "Reconstruction loss: 251.11401881033208, KL divergence: 0.521018672326426\n",
      "Reconstruction loss: 179.04947334653767, KL divergence: 0.015440918156756966\n",
      "Reconstruction loss: 122.40323413897842, KL divergence: 0.0852568619451266\n",
      "Reconstruction loss: 346.99455915258375, KL divergence: 0.5219132277416764\n",
      "Reconstruction loss: 246.5852883534792, KL divergence: 0.24078818421910364\n",
      "Reconstruction loss: 180.44126716767954, KL divergence: 0.037813401766849164\n",
      "Reconstruction loss: 257.05418945969836, KL divergence: 0.5622315299605172\n",
      "Reconstruction loss: 148.90654225502888, KL divergence: 0.07299057274729048\n",
      "Reconstruction loss: 140.04956979591992, KL divergence: 0.09010363265147353\n",
      "Reconstruction loss: 202.50550905014762, KL divergence: 0.10965149381362765\n",
      "Reconstruction loss: 215.76714841448614, KL divergence: 1.0137385942459054\n",
      "Reconstruction loss: 278.81716599584774, KL divergence: 3.0255299464516354\n",
      "Reconstruction loss: 152.99889504999194, KL divergence: 0.10516764132507717\n",
      "Reconstruction loss: 209.65352357018978, KL divergence: 0.3091488108992845\n",
      "Reconstruction loss: 213.39624063188444, KL divergence: 0.09403974515089769\n",
      "Reconstruction loss: 261.72577392016456, KL divergence: 0.47172117412423975\n",
      "Reconstruction loss: 158.35003886660107, KL divergence: 0.08258718328993608\n",
      "Reconstruction loss: 247.0053309956081, KL divergence: 0.1881948562354102\n",
      "Reconstruction loss: 275.139696442417, KL divergence: 1.9433471262123656\n",
      "Reconstruction loss: 202.5569083288754, KL divergence: 0.15941972324530762\n",
      "Reconstruction loss: 178.77458693042954, KL divergence: 0.07683758441783733\n",
      "Reconstruction loss: 191.50581773057883, KL divergence: 0.0895326519165282\n",
      "Reconstruction loss: 123.85243303498133, KL divergence: 0.2050448250875619\n",
      "Reconstruction loss: 149.16947742316523, KL divergence: 0.0983102185804226\n",
      "Reconstruction loss: 231.85318471499983, KL divergence: 0.3347221902231993\n",
      "Reconstruction loss: 225.20242481237852, KL divergence: 0.522282103085473\n",
      "Reconstruction loss: 161.4725251197473, KL divergence: 0.07299057274729048\n",
      "Reconstruction loss: 165.52719089156608, KL divergence: 0.07299057274729048\n",
      "Reconstruction loss: 233.39302302562993, KL divergence: 0.519729867678258\n",
      "Reconstruction loss: 177.69760933210088, KL divergence: 0.019115953415355325\n",
      "Reconstruction loss: 207.3480503957295, KL divergence: 0.04754998548360734\n",
      "Reconstruction loss: 211.43858236072157, KL divergence: 0.01560779605189394\n",
      "Reconstruction loss: 167.24186736812112, KL divergence: 0.07299057274729048\n",
      "Reconstruction loss: 207.39550181040372, KL divergence: 0.8203281958172332\n",
      "Reconstruction loss: 170.32578913766446, KL divergence: 0.015179213983386453\n",
      "Reconstruction loss: 245.11942173855772, KL divergence: 0.5856498277430111\n",
      "Reconstruction loss: 225.84854440883828, KL divergence: 0.0159629271574393\n",
      "Reconstruction loss: 114.51063145601923, KL divergence: 0.09083175151004624\n",
      "Reconstruction loss: 135.947215296546, KL divergence: 0.09118608563558939\n",
      "Reconstruction loss: 239.23432817100857, KL divergence: 0.8697846758831475\n",
      "Reconstruction loss: 183.50174795381733, KL divergence: 0.02718850562162467\n",
      "Reconstruction loss: 315.45692088437477, KL divergence: 2.2162726073050325\n",
      "Reconstruction loss: 146.16167319209265, KL divergence: 0.1295203848851057\n",
      "Reconstruction loss: 267.1592684932442, KL divergence: 1.4991915614692568\n",
      "Reconstruction loss: 174.00960462246505, KL divergence: 0.019689707878031593\n",
      "Reconstruction loss: 233.7924492824045, KL divergence: 0.0741228851273445\n",
      "Reconstruction loss: 258.9108281053938, KL divergence: 0.5242271195117971\n",
      "Reconstruction loss: 207.24553028804843, KL divergence: 1.2244444497593214\n",
      "Reconstruction loss: 191.09787852819844, KL divergence: 0.012426280197464168\n",
      "Reconstruction loss: 266.68578050896446, KL divergence: 1.2940140191649552\n",
      "Reconstruction loss: 160.12495949471764, KL divergence: 0.01548757107376697\n",
      "Reconstruction loss: 215.88095093123056, KL divergence: 0.29125833697361614\n",
      "Reconstruction loss: 280.79114255759305, KL divergence: 0.7071374518122536\n",
      "Reconstruction loss: 292.09463501693523, KL divergence: 2.1412526930903866\n",
      "Reconstruction loss: 115.071587482418, KL divergence: 0.09988586672539207\n",
      "Reconstruction loss: 182.0230808980762, KL divergence: 0.030594063538631366\n",
      "Reconstruction loss: 192.09849146228495, KL divergence: 0.23668712735681963\n",
      "Reconstruction loss: 200.45773667413496, KL divergence: 0.0741228851273445\n",
      "Reconstruction loss: 255.99775401436887, KL divergence: 0.4205747875127937\n",
      "Reconstruction loss: 244.34542364423348, KL divergence: 0.17312068616999188\n",
      "Reconstruction loss: 238.65025853103685, KL divergence: 0.14612908713902872\n",
      "Reconstruction loss: 208.4170043207959, KL divergence: 0.09687030997327362\n",
      "Reconstruction loss: 198.7265255290029, KL divergence: 0.04939316688499312\n",
      "Reconstruction loss: 201.14476421166836, KL divergence: 0.04634452544266565\n",
      "Reconstruction loss: 222.39434614755237, KL divergence: 0.2965381886140526\n",
      "Reconstruction loss: 259.7600303959913, KL divergence: 0.9777305389548406\n",
      "Reconstruction loss: 178.76964135323797, KL divergence: 0.0741228851273445\n",
      "Reconstruction loss: 186.60207255475422, KL divergence: 0.06451353245789465\n",
      "Reconstruction loss: 203.36744529145494, KL divergence: 0.2928152347578332\n",
      "Reconstruction loss: 284.6985976636869, KL divergence: 1.8763848713053597\n",
      "Reconstruction loss: 219.45883673704856, KL divergence: 0.0161802348865564\n",
      "Reconstruction loss: 202.32972702286003, KL divergence: 0.027169235278359294\n",
      "Reconstruction loss: 173.19661951107997, KL divergence: 0.07506069294742679\n",
      "Reconstruction loss: 193.10051257055778, KL divergence: 0.07506069294742679\n",
      "Reconstruction loss: 174.1586226059019, KL divergence: 0.016312683280689355\n",
      "Reconstruction loss: 194.521905724872, KL divergence: 0.014737646087996548\n",
      "Reconstruction loss: 216.189036043269, KL divergence: 0.8390488081807649\n",
      "Reconstruction loss: 194.81514414457007, KL divergence: 0.014587646255222297\n",
      "Reconstruction loss: 253.60377533384934, KL divergence: 0.9342327324466344\n",
      "Reconstruction loss: 185.9876003889623, KL divergence: 0.028103819087119286\n",
      "Reconstruction loss: 230.07623092541476, KL divergence: 0.49904402170695084\n",
      "Reconstruction loss: 207.56745221572783, KL divergence: 0.017599277058022333\n",
      "Reconstruction loss: 173.7588340952988, KL divergence: 0.15212191230220834\n",
      "Reconstruction loss: 245.296751102503, KL divergence: 1.3573678620418597\n",
      "Reconstruction loss: 173.88266216598933, KL divergence: 0.07506069294742679\n",
      "Reconstruction loss: 253.93314818486823, KL divergence: 1.7880214703494148\n",
      "Reconstruction loss: 165.58662587669505, KL divergence: 0.06868508834873965\n",
      "Reconstruction loss: 197.02154581708703, KL divergence: 0.07270457829844384\n",
      "Reconstruction loss: 223.24096456202503, KL divergence: 3.4148880569040365\n",
      "Reconstruction loss: 223.55180260969885, KL divergence: 0.1024960215921294\n",
      "Reconstruction loss: 219.27750525088368, KL divergence: 0.7412589137381507\n",
      "Reconstruction loss: 188.24526074836598, KL divergence: 0.22286349807904499\n",
      "Reconstruction loss: 257.62893187023326, KL divergence: 0.02545469360449637\n",
      "Reconstruction loss: 236.91548345265758, KL divergence: 2.490108941403566\n",
      "Reconstruction loss: 188.67164828668714, KL divergence: 0.11142514428737477\n",
      "Reconstruction loss: 184.42297608108657, KL divergence: 0.07190124269157505\n",
      "Reconstruction loss: 174.98496767604811, KL divergence: 0.07506069294742679\n",
      "Reconstruction loss: 162.5428519045899, KL divergence: 0.08098088948032939\n",
      "Reconstruction loss: 130.66763141495778, KL divergence: 0.09413097587150954\n",
      "Reconstruction loss: 222.74721876621587, KL divergence: 0.27453877738473587\n",
      "Reconstruction loss: 280.30278105719094, KL divergence: 2.193869275858302\n",
      "Reconstruction loss: 248.25110647738302, KL divergence: 1.6478450025659035\n",
      "Reconstruction loss: 182.48608338626786, KL divergence: 0.06124879930896737\n",
      "Reconstruction loss: 195.255940828337, KL divergence: 0.07615231526586291\n",
      "Reconstruction loss: 193.39681487861145, KL divergence: 0.03887918662060991\n",
      "Reconstruction loss: 155.76649202473024, KL divergence: 0.15714654642183828\n",
      "Reconstruction loss: 288.1927786898728, KL divergence: 2.401328351836233\n",
      "Reconstruction loss: 168.80699154371132, KL divergence: 0.06514372750368502\n",
      "Reconstruction loss: 234.18696918772005, KL divergence: 0.5682525081450065\n",
      "Reconstruction loss: 259.55378733056136, KL divergence: 0.6407236904630624\n",
      "Reconstruction loss: 135.3580425420049, KL divergence: 0.10666784210441932\n",
      "Reconstruction loss: 145.52498711933083, KL divergence: 0.09000423426087995\n",
      "Reconstruction loss: 220.45101064004285, KL divergence: 0.05351084431477232\n",
      "Reconstruction loss: 253.68004134281406, KL divergence: 0.5143923404618476\n",
      "Reconstruction loss: 176.67627459670308, KL divergence: 0.07615231526586291\n",
      "Reconstruction loss: 145.12484383650985, KL divergence: 0.16117935031981467\n",
      "Reconstruction loss: 148.25936870840326, KL divergence: 0.07615231526586291\n",
      "Reconstruction loss: 227.90070518652288, KL divergence: 0.02011301772364721\n",
      "Reconstruction loss: 193.12419510944477, KL divergence: 0.026879466107719685\n",
      "Reconstruction loss: 174.35659116861638, KL divergence: 0.018281798304565278\n",
      "Reconstruction loss: 172.8480876023291, KL divergence: 0.014712509848687327\n",
      "Reconstruction loss: 186.0331885630539, KL divergence: 0.07615231526586291\n",
      "Reconstruction loss: 192.08472299089493, KL divergence: 0.015589266323825124\n",
      "Reconstruction loss: 172.8687322827089, KL divergence: 0.020343740255094167\n",
      "Reconstruction loss: 206.4173804162461, KL divergence: 1.555018579281703\n",
      "Reconstruction loss: 152.02187818114817, KL divergence: 0.12714202048023165\n",
      "Reconstruction loss: 134.04498887281804, KL divergence: 0.07615231526586291\n",
      "Reconstruction loss: 144.8280430368736, KL divergence: 0.11814613979941818\n",
      "Reconstruction loss: 187.38548034420347, KL divergence: 0.07615231526586291\n",
      "Reconstruction loss: 241.2182002040392, KL divergence: 0.2827638894578755\n",
      "Reconstruction loss: 200.7766502937224, KL divergence: 0.07615231526586291\n",
      "Reconstruction loss: 157.05095018283873, KL divergence: 0.0731451174645143\n",
      "Reconstruction loss: 200.72320348399137, KL divergence: 2.227330625727016\n",
      "Reconstruction loss: 141.09883239695756, KL divergence: 0.14928658391885508\n",
      "Reconstruction loss: 157.04896604258568, KL divergence: 0.07758562175181999\n",
      "Reconstruction loss: 231.96463072635282, KL divergence: 0.05454379730372855\n",
      "Reconstruction loss: 186.02671371472047, KL divergence: 0.14550060988411295\n",
      "Reconstruction loss: 154.94716668889254, KL divergence: 0.08727017656370084\n",
      "Reconstruction loss: 208.37524733566613, KL divergence: 0.06085552124738858\n",
      "Reconstruction loss: 276.56440421629895, KL divergence: 1.3974255883680722\n",
      "Reconstruction loss: 272.1490355170178, KL divergence: 1.6868432568936582\n",
      "Reconstruction loss: 230.50783433195198, KL divergence: 0.02183833322026546\n",
      "Reconstruction loss: 150.2940996737945, KL divergence: 0.18926969284492717\n",
      "Reconstruction loss: 285.652678044464, KL divergence: 5.5502283793810445\n",
      "Reconstruction loss: 223.19454002303587, KL divergence: 1.6781515985770254\n",
      "Reconstruction loss: 181.81680466627483, KL divergence: 0.1011386232255625\n",
      "Reconstruction loss: 150.67801913858494, KL divergence: 0.09014989457626443\n",
      "Reconstruction loss: 163.08911441763985, KL divergence: 0.12409181777005779\n",
      "Reconstruction loss: 150.19579161786817, KL divergence: 0.07758562175181999\n",
      "Reconstruction loss: 248.74716557278995, KL divergence: 0.0678472357583112\n",
      "Reconstruction loss: 251.95403069042118, KL divergence: 0.18364396458572008\n",
      "Reconstruction loss: 118.3910994465751, KL divergence: 0.08129875307864454\n",
      "Reconstruction loss: 235.2991850962959, KL divergence: 0.25571770225326546\n",
      "Reconstruction loss: 230.1598772522997, KL divergence: 0.2832937153813967\n",
      "Reconstruction loss: 173.1451190040865, KL divergence: 0.031042394935085593\n",
      "Reconstruction loss: 157.55748940379422, KL divergence: 0.07758562175181999\n",
      "Reconstruction loss: 219.09673604706603, KL divergence: 0.029554813220921905\n",
      "Reconstruction loss: 251.28820556707498, KL divergence: 0.4409132044442546\n",
      "Reconstruction loss: 226.23642017584186, KL divergence: 0.1958795801429004\n",
      "Reconstruction loss: 238.97737077738878, KL divergence: 0.052162098207968965\n",
      "Reconstruction loss: 246.33338162281515, KL divergence: 0.34025891325320695\n",
      "Reconstruction loss: 157.44470558545567, KL divergence: 0.054248555894807315\n",
      "Reconstruction loss: 141.24972597331416, KL divergence: 0.1403517651907924\n",
      "Reconstruction loss: 238.91293067843793, KL divergence: 0.06679262005240466\n",
      "Reconstruction loss: 189.60822306187865, KL divergence: 0.05545629585831291\n",
      "Reconstruction loss: 223.31328670808747, KL divergence: 0.1568209331298009\n",
      "Reconstruction loss: 195.62387470305174, KL divergence: 0.04233337586067015\n",
      "Reconstruction loss: 115.63011379029994, KL divergence: 0.08266084490173303\n",
      "Reconstruction loss: 181.6019695786408, KL divergence: 0.09181557513817845\n",
      "Reconstruction loss: 241.9008252805317, KL divergence: 0.9776603217369932\n",
      "Reconstruction loss: 182.9618945946185, KL divergence: 0.13753883505751546\n",
      "Reconstruction loss: 148.9632395599041, KL divergence: 0.2460412317280381\n",
      "Reconstruction loss: 166.90749572983498, KL divergence: 0.13368141280465634\n",
      "Reconstruction loss: 156.0450191185298, KL divergence: 0.09305408535387566\n",
      "Reconstruction loss: 215.16793717763287, KL divergence: 0.3815539267826769\n",
      "Reconstruction loss: 275.54232032727543, KL divergence: 1.9115527618462844\n",
      "Reconstruction loss: 253.73568991455332, KL divergence: 0.46477137988875034\n",
      "Reconstruction loss: 202.63461368760133, KL divergence: 2.0024502313835324\n",
      "Reconstruction loss: 189.62008074991667, KL divergence: 0.0913838522736104\n",
      "Reconstruction loss: 264.96533972184284, KL divergence: 4.634060234334477\n",
      "Reconstruction loss: 289.2745957545143, KL divergence: 4.91732093278716\n",
      "Reconstruction loss: 235.88022561835535, KL divergence: 1.1303858027521585\n",
      "Reconstruction loss: 227.99478271900333, KL divergence: 0.26047981635338\n",
      "Reconstruction loss: 129.861369206137, KL divergence: 0.13150425165654495\n",
      "Reconstruction loss: 158.37850329125467, KL divergence: 0.14148814343865468\n",
      "Reconstruction loss: 250.56186634738174, KL divergence: 0.11451124057721807\n",
      "Reconstruction loss: 199.9918629455093, KL divergence: 0.04137775499722951\n",
      "Reconstruction loss: 146.0770018529414, KL divergence: 0.1280546868899033\n",
      "Reconstruction loss: 190.73813926465624, KL divergence: 0.0802480337269027\n",
      "Reconstruction loss: 294.3327866378656, KL divergence: 0.17996898346562773\n",
      "Reconstruction loss: 154.33812161412447, KL divergence: 0.0856740720225348\n",
      "Reconstruction loss: 281.22792188774076, KL divergence: 0.7165958796385119\n",
      "Reconstruction loss: 170.83825493258848, KL divergence: 0.07176604515987184\n",
      "Reconstruction loss: 184.12881658920924, KL divergence: 0.07916496726596872\n",
      "Reconstruction loss: 173.5045164370033, KL divergence: 0.08965781524561595\n",
      "Reconstruction loss: 199.11152402629304, KL divergence: 0.02191671783010185\n",
      "Reconstruction loss: 209.55003771714178, KL divergence: 0.06805427531123209\n",
      "Reconstruction loss: 153.00664564309042, KL divergence: 0.19001195048449843\n",
      "Reconstruction loss: 255.13741988110957, KL divergence: 1.6362293946202735\n",
      "Reconstruction loss: 134.6528567973733, KL divergence: 0.21391193031720807\n",
      "Reconstruction loss: 253.41942604080253, KL divergence: 2.2990195005865046\n",
      "Reconstruction loss: 178.37902750939526, KL divergence: 0.019730649716768356\n",
      "Reconstruction loss: 211.08742391359786, KL divergence: 0.059076487516233345\n",
      "Reconstruction loss: 209.69309584118275, KL divergence: 0.16665700095436697\n",
      "Reconstruction loss: 206.51495961163428, KL divergence: 0.07174194853843552\n",
      "Reconstruction loss: 144.12849951404098, KL divergence: 0.15252051104439424\n",
      "Reconstruction loss: 141.45696301562413, KL divergence: 0.211219159248641\n",
      "Reconstruction loss: 184.22054076884038, KL divergence: 0.0907358161816334\n",
      "Reconstruction loss: 182.80219833119116, KL divergence: 0.021188014132027033\n",
      "Reconstruction loss: 211.2313123200148, KL divergence: 0.031064825549769748\n",
      "Reconstruction loss: 208.2991834916366, KL divergence: 0.01777264864544087\n",
      "Reconstruction loss: 254.10253347926377, KL divergence: 0.08147587014543056\n",
      "Reconstruction loss: 193.28008449889956, KL divergence: 0.12914517948729748\n",
      "Reconstruction loss: 253.0699226970171, KL divergence: 0.06707159729849632\n",
      "Reconstruction loss: 297.1395226076634, KL divergence: 3.6131307655886835\n",
      "Reconstruction loss: 201.35881487070026, KL divergence: 0.08101275647139483\n",
      "Reconstruction loss: 187.38915754321073, KL divergence: 0.1705212455151489\n",
      "Reconstruction loss: 245.4029896530983, KL divergence: 1.5253738567652433\n",
      "Reconstruction loss: 173.1296463527607, KL divergence: 0.08082737865267936\n",
      "Reconstruction loss: 177.56018594901417, KL divergence: 0.06610685702604868\n",
      "Reconstruction loss: 212.31576282942507, KL divergence: 0.5023814348990863\n",
      "Reconstruction loss: 269.00255537741225, KL divergence: 0.7380147502363108\n",
      "Reconstruction loss: 230.39318651921656, KL divergence: 0.1814291157945016\n",
      "Reconstruction loss: 214.4796635218023, KL divergence: 0.4185685377918676\n",
      "Reconstruction loss: 142.90176007331667, KL divergence: 0.18329755784248397\n",
      "Reconstruction loss: 237.8485535118378, KL divergence: 0.014339147501119698\n",
      "Reconstruction loss: 249.97002212051885, KL divergence: 0.5176285010873138\n",
      "Reconstruction loss: 255.18550509237855, KL divergence: 0.12710725879720436\n",
      "Reconstruction loss: 194.98825134658827, KL divergence: 0.13012498681749707\n",
      "Reconstruction loss: 191.37560190473144, KL divergence: 0.5863052499658296\n",
      "Reconstruction loss: 190.15512319598375, KL divergence: 0.017606955611337183\n",
      "Reconstruction loss: 180.1141956629277, KL divergence: 0.020199239608574948\n",
      "Reconstruction loss: 140.58263648417278, KL divergence: 0.2883669794619279\n",
      "Reconstruction loss: 217.43944205132465, KL divergence: 0.09835708223224099\n",
      "Reconstruction loss: 238.79145345397816, KL divergence: 1.3423102346311722\n",
      "Reconstruction loss: 215.60051482744595, KL divergence: 0.09671755926109954\n",
      "Reconstruction loss: 159.63886502890833, KL divergence: 0.08289709941043771\n",
      "Reconstruction loss: 232.40476524717604, KL divergence: 4.825812308102435\n",
      "Reconstruction loss: 232.97949781412865, KL divergence: 1.0078194145095063\n",
      "Reconstruction loss: 238.42830132685975, KL divergence: 0.3819915595538079\n",
      "Reconstruction loss: 142.0997961683821, KL divergence: 0.1511729622462884\n",
      "Reconstruction loss: 245.19778117324182, KL divergence: 0.045521513312655304\n",
      "Reconstruction loss: 151.87421047366468, KL divergence: 0.09086542899311001\n",
      "Reconstruction loss: 154.92568907586528, KL divergence: 0.1676984757853453\n",
      "Reconstruction loss: 204.9258936421784, KL divergence: 0.13575248628762993\n",
      "Reconstruction loss: 267.9768117150238, KL divergence: 2.590765641869079\n",
      "Reconstruction loss: 189.74462089355532, KL divergence: 0.01511927776767158\n",
      "Reconstruction loss: 221.24315858818733, KL divergence: 0.2715910114290658\n",
      "Reconstruction loss: 270.97803511708554, KL divergence: 0.8125684154344623\n",
      "Reconstruction loss: 180.1162263017435, KL divergence: 0.06863357231333395\n",
      "Reconstruction loss: 222.86438655228352, KL divergence: 0.1222870052265535\n",
      "Reconstruction loss: 224.14897605240722, KL divergence: 0.014999635846729975\n",
      "Reconstruction loss: 178.04231341636822, KL divergence: 0.08289709941043771\n",
      "Reconstruction loss: 265.21282071519147, KL divergence: 0.38607508092775417\n",
      "Reconstruction loss: 254.76491201567234, KL divergence: 1.1970300673892518\n",
      "Reconstruction loss: 216.73314294926058, KL divergence: 2.8583680470185473\n",
      "Reconstruction loss: 258.97592211434255, KL divergence: 1.721952116895264\n",
      "Reconstruction loss: 258.0233245581742, KL divergence: 3.030423011696061\n",
      "Reconstruction loss: 232.94665606750365, KL divergence: 0.01631317418392292\n",
      "Reconstruction loss: 194.2825818628117, KL divergence: 0.2037879711416511\n",
      "Reconstruction loss: 272.249517446077, KL divergence: 0.8595765071968551\n",
      "Reconstruction loss: 198.76841419807465, KL divergence: 0.08482644549433033\n",
      "Reconstruction loss: 233.0008737286601, KL divergence: 0.015310764788083797\n",
      "Reconstruction loss: 189.12238141097947, KL divergence: 0.04080745716214573\n",
      "Reconstruction loss: 257.24389616327664, KL divergence: 0.07344699969105073\n",
      "Reconstruction loss: 178.17488619484908, KL divergence: 0.1095517802977331\n",
      "Reconstruction loss: 213.84565051833061, KL divergence: 0.017483364970826976\n",
      "Reconstruction loss: 194.4843193621903, KL divergence: 0.03904611119902063\n",
      "Reconstruction loss: 143.6442148694578, KL divergence: 0.14788002047109344\n",
      "Reconstruction loss: 223.1375298435281, KL divergence: 0.08482644549433033\n",
      "Reconstruction loss: 232.62296133071104, KL divergence: 0.3368152935690308\n",
      "Reconstruction loss: 235.90575251502784, KL divergence: 0.029266857707121674\n",
      "Reconstruction loss: 234.510552162798, KL divergence: 0.8046230277167743\n",
      "Reconstruction loss: 214.32792858250434, KL divergence: 0.08432999494878568\n",
      "Reconstruction loss: 215.70457839101124, KL divergence: 0.05192345952196625\n",
      "Reconstruction loss: 233.96963195231666, KL divergence: 0.18142277374073545\n",
      "Reconstruction loss: 186.55229124901444, KL divergence: 0.04033043623262095\n",
      "Reconstruction loss: 194.1641485877378, KL divergence: 0.08482644549433033\n",
      "Reconstruction loss: 188.6333713273496, KL divergence: 0.6791364030185894\n",
      "Reconstruction loss: 185.1600620470693, KL divergence: 0.08472097016789043\n",
      "Reconstruction loss: 202.6665067381935, KL divergence: 0.10471362358810982\n",
      "Reconstruction loss: 235.0034446990547, KL divergence: 1.689640116843686\n",
      "Reconstruction loss: 187.94955976886934, KL divergence: 0.29342698590955574\n",
      "Reconstruction loss: 180.225746695434, KL divergence: 0.08482644549433033\n",
      "Reconstruction loss: 143.4111067466074, KL divergence: 0.10069746634320131\n",
      "Reconstruction loss: 262.37394685132256, KL divergence: 4.306097602390997\n",
      "Reconstruction loss: 222.8094512002295, KL divergence: 0.016692530641234493\n",
      "Reconstruction loss: 200.70314704573232, KL divergence: 0.07133449674902825\n",
      "Reconstruction loss: 161.85193237076237, KL divergence: 0.11586278580853576\n",
      "Reconstruction loss: 116.5807772849428, KL divergence: 0.20190202502360582\n",
      "Reconstruction loss: 212.6598063215817, KL divergence: 0.021530682378888\n",
      "Reconstruction loss: 371.4891642815662, KL divergence: 0.03288447938695854\n",
      "Reconstruction loss: 193.32097970885934, KL divergence: 0.12530612435390798\n",
      "Reconstruction loss: 261.73644748235256, KL divergence: 1.5489496073701758\n",
      "Reconstruction loss: 151.64237321968076, KL divergence: 0.0913509811448523\n",
      "Reconstruction loss: 236.89648395476615, KL divergence: 0.018706772137940753\n",
      "Reconstruction loss: 206.07558939764232, KL divergence: 0.03719778066718005\n",
      "Reconstruction loss: 272.1265906727024, KL divergence: 5.25874818908944\n",
      "Reconstruction loss: 273.2140299983081, KL divergence: 4.511922378226701\n",
      "Reconstruction loss: 159.7056766199026, KL divergence: 0.2149541928940154\n",
      "Reconstruction loss: 223.36076774842599, KL divergence: 0.03557477012237187\n",
      "Reconstruction loss: 176.6081988298472, KL divergence: 0.16008116869751066\n",
      "Reconstruction loss: 204.8633954935558, KL divergence: 0.1002866373825862\n",
      "Reconstruction loss: 207.64688887218432, KL divergence: 0.08680889508144046\n",
      "Reconstruction loss: 252.67976985742268, KL divergence: 0.6329349154857729\n",
      "Reconstruction loss: 195.58321675772567, KL divergence: 0.03206703260057231\n",
      "Reconstruction loss: 269.2984943438488, KL divergence: 2.0478566043316144\n",
      "Reconstruction loss: 220.41840344525156, KL divergence: 0.015788514080153748\n",
      "Reconstruction loss: 299.20022331389896, KL divergence: 5.850839968731474\n",
      "Reconstruction loss: 238.18479703905405, KL divergence: 0.033539016537930844\n",
      "Reconstruction loss: 178.7910425369531, KL divergence: 0.1111118750093994\n",
      "Reconstruction loss: 278.05342266567664, KL divergence: 0.3283624454600875\n",
      "Reconstruction loss: 199.98456566969907, KL divergence: 0.19615626476484943\n",
      "Reconstruction loss: 145.24139395637553, KL divergence: 0.3499303296373408\n",
      "Reconstruction loss: 182.7282206992311, KL divergence: 0.058156161495632186\n",
      "Reconstruction loss: 227.51321427056388, KL divergence: 0.1273716856496523\n",
      "Reconstruction loss: 183.96130029455293, KL divergence: 3.5622765556803793\n",
      "Reconstruction loss: 158.06775223019935, KL divergence: 0.25123337579485316\n",
      "Reconstruction loss: 157.71186702878128, KL divergence: 0.1652488183460723\n",
      "Reconstruction loss: 252.59251480477437, KL divergence: 3.34036279835803\n",
      "Reconstruction loss: 146.51509434894967, KL divergence: 0.14871012962549707\n",
      "Reconstruction loss: 276.930907269396, KL divergence: 0.05725396488187595\n",
      "Reconstruction loss: 201.04821477085267, KL divergence: 0.016667649088722924\n",
      "Reconstruction loss: 132.16529058517816, KL divergence: 0.09633795967276088\n",
      "Reconstruction loss: 200.01799271729234, KL divergence: 0.04890866199491689\n",
      "Reconstruction loss: 133.02979610485758, KL divergence: 0.13541747527736148\n",
      "Reconstruction loss: 187.83416577517892, KL divergence: 0.020669046563772997\n",
      "Reconstruction loss: 273.6505355766485, KL divergence: 0.828944642831188\n",
      "Reconstruction loss: 255.09203593772142, KL divergence: 0.19513926796298275\n",
      "Reconstruction loss: 308.78679384786915, KL divergence: 0.7351529944175812\n",
      "Reconstruction loss: 265.96322571234214, KL divergence: 0.05553707269065705\n",
      "Reconstruction loss: 168.26316434826617, KL divergence: 0.21115152237285884\n",
      "Reconstruction loss: 180.05066706758976, KL divergence: 0.01478257552819745\n",
      "Reconstruction loss: 274.2230324406375, KL divergence: 4.619045164277528\n",
      "Reconstruction loss: 211.03779048671066, KL divergence: 3.513165133540886\n",
      "Reconstruction loss: 242.078492834547, KL divergence: 0.9941598363040958\n",
      "Reconstruction loss: 127.2313507487902, KL divergence: 0.2319865287451146\n",
      "Reconstruction loss: 192.06508687792513, KL divergence: 0.09082839465431802\n",
      "Reconstruction loss: 263.38961433507234, KL divergence: 0.9712311220650298\n",
      "Reconstruction loss: 206.74254309384216, KL divergence: 0.5047431381826228\n",
      "Reconstruction loss: 170.86121982110492, KL divergence: 0.09987903841940982\n",
      "Reconstruction loss: 144.5970029154676, KL divergence: 0.16276211546194874\n",
      "Reconstruction loss: 166.54535016887309, KL divergence: 0.19040905621547694\n",
      "Reconstruction loss: 230.62343860843941, KL divergence: 0.08885595877155444\n",
      "Reconstruction loss: 124.37519243039748, KL divergence: 0.23952451884078146\n",
      "Reconstruction loss: 152.86933927719764, KL divergence: 0.3522981419569009\n",
      "Reconstruction loss: 118.32626196921012, KL divergence: 0.2217455059130572\n",
      "Reconstruction loss: 168.03001986511896, KL divergence: 0.11054392089518972\n",
      "Reconstruction loss: 216.53854300868667, KL divergence: 0.08885595877155444\n",
      "Reconstruction loss: 234.03058056506646, KL divergence: 0.10018305959622281\n",
      "Reconstruction loss: 197.6662226471435, KL divergence: 0.1755470473777554\n",
      "Reconstruction loss: 214.56622937333952, KL divergence: 0.20604441198434997\n",
      "Reconstruction loss: 247.3005792099632, KL divergence: 4.963292895932847\n",
      "Reconstruction loss: 215.8345206588433, KL divergence: 0.09722791732548342\n",
      "Reconstruction loss: 185.7800534850426, KL divergence: 0.01587582722763864\n",
      "Reconstruction loss: 191.0425343966934, KL divergence: 0.018011081146556918\n",
      "Reconstruction loss: 194.20163663910216, KL divergence: 0.13709148905989865\n",
      "Reconstruction loss: 273.17469894292026, KL divergence: 0.3667487969392133\n",
      "Reconstruction loss: 219.19477397832122, KL divergence: 0.7607505109488211\n",
      "Reconstruction loss: 208.09523643698822, KL divergence: 0.24211905346998325\n",
      "Reconstruction loss: 229.17391897846403, KL divergence: 0.055969425619153634\n",
      "Reconstruction loss: 182.85267316541797, KL divergence: 0.10293258911246861\n",
      "Reconstruction loss: 152.03630613272514, KL divergence: 0.479702254564963\n",
      "Reconstruction loss: 230.83844946785467, KL divergence: 0.24156868400047177\n",
      "Reconstruction loss: 215.7407175176645, KL divergence: 0.04113099536088749\n",
      "Reconstruction loss: 154.9084231129036, KL divergence: 0.29205182512417693\n",
      "Reconstruction loss: 193.9740325875081, KL divergence: 0.10756110746714487\n",
      "Reconstruction loss: 145.63557113946882, KL divergence: 0.14206611550744336\n",
      "Reconstruction loss: 242.49784943072555, KL divergence: 0.015256099484804941\n",
      "Reconstruction loss: 216.1964695230533, KL divergence: 0.6090608925772139\n",
      "Reconstruction loss: 245.67525488240744, KL divergence: 0.16593984865037414\n",
      "Reconstruction loss: 324.36933490533545, KL divergence: 8.099554587319021\n",
      "Reconstruction loss: 174.61769457507486, KL divergence: 0.02497581954696476\n",
      "Reconstruction loss: 153.04837316595496, KL divergence: 0.09106789351849742\n",
      "Reconstruction loss: 149.88065262111073, KL divergence: 0.20578778817405047\n",
      "Reconstruction loss: 148.91196266485343, KL divergence: 0.29704740059347606\n",
      "Reconstruction loss: 215.28068661379234, KL divergence: 0.7987260289766407\n",
      "Reconstruction loss: 290.51469476556264, KL divergence: 0.3504568516758677\n",
      "Reconstruction loss: 188.40241542343608, KL divergence: 0.2031720116041652\n",
      "Reconstruction loss: 176.67140477077933, KL divergence: 0.18965498821224136\n",
      "Reconstruction loss: 200.92904883415247, KL divergence: 0.1804304390352528\n",
      "Reconstruction loss: 189.21951235954344, KL divergence: 0.12204403004556597\n",
      "Reconstruction loss: 244.8936170728264, KL divergence: 3.566939160270719\n",
      "Reconstruction loss: 301.5456957558582, KL divergence: 0.5869117496785075\n",
      "Reconstruction loss: 150.0430549746003, KL divergence: 0.15773925456237825\n",
      "Reconstruction loss: 216.46909460973922, KL divergence: 0.1666834682611935\n",
      "Reconstruction loss: 245.52068206785674, KL divergence: 0.11953480447616943\n",
      "Reconstruction loss: 209.32058918505476, KL divergence: 0.18283797066013135\n",
      "Reconstruction loss: 228.92802937097713, KL divergence: 0.09386394630950279\n",
      "Reconstruction loss: 148.63147345585844, KL divergence: 0.3313888149825337\n",
      "Reconstruction loss: 211.28918958224975, KL divergence: 0.14681085587881787\n",
      "Reconstruction loss: 281.61360121331126, KL divergence: 2.6425130767688025\n",
      "Reconstruction loss: 191.49067832000338, KL divergence: 0.2534508868175644\n",
      "Reconstruction loss: 217.6563401286312, KL divergence: 0.05978751583863523\n",
      "Reconstruction loss: 277.5704109837437, KL divergence: 0.48902301166952306\n",
      "Reconstruction loss: 232.49298258398437, KL divergence: 0.2589019552199999\n",
      "Reconstruction loss: 277.7924386947933, KL divergence: 0.11289213294021316\n",
      "Reconstruction loss: 185.90007417021064, KL divergence: 0.3526596403246441\n",
      "Reconstruction loss: 174.60243403561583, KL divergence: 0.1489462937938012\n",
      "Reconstruction loss: 128.54865685633928, KL divergence: 0.14641230303665542\n",
      "Reconstruction loss: 134.86037123593192, KL divergence: 0.375878136971633\n",
      "Reconstruction loss: 223.47634021596218, KL divergence: 0.7364853702867136\n",
      "Reconstruction loss: 153.28083708981075, KL divergence: 0.41528997010705854\n",
      "Reconstruction loss: 182.2265579706139, KL divergence: 0.21842360054919718\n",
      "Reconstruction loss: 161.39478186280797, KL divergence: 0.20499446433389962\n",
      "Reconstruction loss: 248.82664513447753, KL divergence: 4.255448370686438\n",
      "Reconstruction loss: 271.8484984580618, KL divergence: 0.1995632731406235\n",
      "Reconstruction loss: 144.27381845534194, KL divergence: 0.5106881336185157\n",
      "Reconstruction loss: 201.21014043197476, KL divergence: 0.2312185068636643\n",
      "Reconstruction loss: 201.2551644660542, KL divergence: 0.4766948286891018\n",
      "Reconstruction loss: 198.87992943045157, KL divergence: 0.18344707808764393\n",
      "Reconstruction loss: 275.9271234667818, KL divergence: 0.022619472177724864\n",
      "Reconstruction loss: 218.8723902426881, KL divergence: 0.3181627527855753\n",
      "Reconstruction loss: 293.2578947578412, KL divergence: 0.33325676776156204\n",
      "Reconstruction loss: 201.63984220092908, KL divergence: 0.18703865766758299\n",
      "Reconstruction loss: 221.05824854255917, KL divergence: 0.023642352461274885\n",
      "Reconstruction loss: 203.98541621132685, KL divergence: 0.05017844285038919\n",
      "Reconstruction loss: 241.11823350030747, KL divergence: 0.05443151894544679\n",
      "Reconstruction loss: 185.4127154533686, KL divergence: 0.144848505735852\n",
      "Reconstruction loss: 225.02617418852083, KL divergence: 0.13572674200389495\n",
      "Reconstruction loss: 195.22429429147024, KL divergence: 0.024030344359699973\n",
      "Reconstruction loss: 158.33841563735209, KL divergence: 0.16955051759264933\n",
      "Reconstruction loss: 181.31752923576033, KL divergence: 0.20242363998238355\n",
      "Reconstruction loss: 208.1924881395149, KL divergence: 0.45506291399115206\n",
      "Reconstruction loss: 206.70064438076085, KL divergence: 0.05438011862807007\n",
      "Reconstruction loss: 188.9645717615581, KL divergence: 0.16152172069313747\n",
      "Reconstruction loss: 183.77308811113136, KL divergence: 0.14233810081826131\n",
      "Reconstruction loss: 220.17850327905086, KL divergence: 0.016849093690397554\n",
      "Reconstruction loss: 209.20079921804256, KL divergence: 0.04474715092491738\n",
      "Reconstruction loss: 290.95346718608096, KL divergence: 0.01835194352510916\n",
      "Reconstruction loss: 216.04537013259744, KL divergence: 0.12241499047205856\n",
      "Reconstruction loss: 158.49357933889897, KL divergence: 0.35633259990753957\n",
      "Reconstruction loss: 154.53151029061564, KL divergence: 0.27650879820674473\n",
      "Reconstruction loss: 184.8484576394511, KL divergence: 0.10410629917290082\n",
      "Reconstruction loss: 188.75353238626067, KL divergence: 0.0636225717285479\n",
      "Reconstruction loss: 180.18367662199347, KL divergence: 0.07852321970033271\n",
      "Reconstruction loss: 218.726816381917, KL divergence: 0.4128507452464903\n",
      "Reconstruction loss: 185.8236501306414, KL divergence: 0.09114206559606786\n",
      "Reconstruction loss: 249.20071091774267, KL divergence: 0.6984629585851511\n",
      "Reconstruction loss: 225.0038540622621, KL divergence: 0.3619840971574253\n",
      "Reconstruction loss: 223.7230096822395, KL divergence: 0.016763337217951035\n",
      "Reconstruction loss: 311.4258366959091, KL divergence: 0.663980732360536\n",
      "Reconstruction loss: 135.19569448183552, KL divergence: 0.43711053858314797\n",
      "Reconstruction loss: 241.06085826107955, KL divergence: 0.04430476273709083\n",
      "Reconstruction loss: 186.72917195506972, KL divergence: 0.10377639667025274\n",
      "Reconstruction loss: 145.33432646249187, KL divergence: 0.42018523547522657\n",
      "Reconstruction loss: 212.29428071242017, KL divergence: 0.0837974621118806\n",
      "Reconstruction loss: 269.20365171250336, KL divergence: 0.047288643468048897\n",
      "Reconstruction loss: 183.29293900802264, KL divergence: 0.11617784413909837\n",
      "Reconstruction loss: 141.05363089792826, KL divergence: 0.2179952662129807\n",
      "Reconstruction loss: 225.64936700946845, KL divergence: 0.019269241857840902\n",
      "Reconstruction loss: 289.988144154829, KL divergence: 0.018933547624155944\n",
      "Reconstruction loss: 218.31200456202217, KL divergence: 0.03291564487408871\n",
      "Reconstruction loss: 214.7350131271977, KL divergence: 0.5019121492663577\n",
      "Reconstruction loss: 179.50140617659127, KL divergence: 0.059692949069870715\n",
      "Reconstruction loss: 149.3403096772522, KL divergence: 0.28294640758767714\n",
      "Reconstruction loss: 175.80496198910203, KL divergence: 0.09741406291427546\n",
      "Reconstruction loss: 279.3492216643284, KL divergence: 1.2642420816843685\n",
      "Reconstruction loss: 223.14935369505025, KL divergence: 0.2512158177851521\n",
      "Reconstruction loss: 244.9487840248106, KL divergence: 3.0708680275728346\n",
      "Reconstruction loss: 140.21878506141013, KL divergence: 0.4024646589734966\n",
      "Reconstruction loss: 231.79804015691792, KL divergence: 0.042164142174514196\n",
      "Reconstruction loss: 207.4730432696333, KL divergence: 0.1826262280018085\n",
      "Reconstruction loss: 186.46327813757532, KL divergence: 0.11684139254494147\n",
      "Reconstruction loss: 158.67126319886933, KL divergence: 0.33717246200324763\n",
      "Reconstruction loss: 286.3933915266981, KL divergence: 0.06689299500703222\n",
      "Reconstruction loss: 219.50537441726962, KL divergence: 0.019126414406958003\n",
      "Reconstruction loss: 192.75608976553883, KL divergence: 0.015187039120472201\n",
      "Reconstruction loss: 142.87991277929595, KL divergence: 0.2931399941380541\n",
      "Reconstruction loss: 171.73934828952804, KL divergence: 0.162832802548879\n",
      "Reconstruction loss: 202.00961353642083, KL divergence: 0.18834429962867105\n",
      "Reconstruction loss: 167.9904120797, KL divergence: 0.2093913953750834\n",
      "Reconstruction loss: 202.26105755442651, KL divergence: 0.07224610632318113\n",
      "Reconstruction loss: 124.81975139587757, KL divergence: 0.36724494926299994\n",
      "Reconstruction loss: 243.05534292695597, KL divergence: 2.3093516237246297\n",
      "Reconstruction loss: 239.58077134574864, KL divergence: 0.1110478748069737\n",
      "Reconstruction loss: 209.98483979230673, KL divergence: 0.16506067911851352\n",
      "Reconstruction loss: 184.42000927345995, KL divergence: 0.2187986496819625\n",
      "Reconstruction loss: 283.80745433250524, KL divergence: 0.5024703025246328\n",
      "Reconstruction loss: 194.07914856723616, KL divergence: 0.18602399031727468\n",
      "Reconstruction loss: 224.857996857725, KL divergence: 2.4170713557969186\n",
      "Reconstruction loss: 182.25916713048605, KL divergence: 0.059074865992113146\n",
      "Reconstruction loss: 295.4893148530658, KL divergence: 0.6185481713387679\n",
      "Reconstruction loss: 227.90173997946494, KL divergence: 1.2635523546174396\n",
      "Reconstruction loss: 193.38429010110136, KL divergence: 0.2238100627021951\n",
      "Reconstruction loss: 178.02818287080163, KL divergence: 0.09907127117079961\n",
      "Reconstruction loss: 305.07443736076425, KL divergence: 0.3593795001160608\n",
      "Reconstruction loss: 338.5021215710588, KL divergence: 0.036069950931150896\n",
      "Reconstruction loss: 157.01548165553456, KL divergence: 0.0991325291455637\n",
      "Reconstruction loss: 193.35063936567076, KL divergence: 0.14663161735617236\n",
      "Reconstruction loss: 143.3972773994352, KL divergence: 0.19120783243599732\n",
      "Reconstruction loss: 194.23412734172913, KL divergence: 0.03336066144265476\n",
      "Reconstruction loss: 138.0541204443826, KL divergence: 0.13809012722406627\n",
      "Reconstruction loss: 224.0497309918127, KL divergence: 0.03455901648462678\n",
      "Reconstruction loss: 201.3118825922118, KL divergence: 0.048421574371894205\n",
      "Reconstruction loss: 149.35867582027356, KL divergence: 0.1771879612859733\n",
      "Reconstruction loss: 201.5778566656781, KL divergence: 0.0399931087146555\n",
      "Reconstruction loss: 193.40029471799622, KL divergence: 0.07926855712706454\n",
      "Reconstruction loss: 149.87063824227405, KL divergence: 0.2969445407748214\n",
      "Reconstruction loss: 252.2328086088096, KL divergence: 1.1719727059977276\n",
      "Reconstruction loss: 136.75360460984953, KL divergence: 0.5338533406769929\n",
      "Reconstruction loss: 159.4996296669729, KL divergence: 0.4282364412091435\n",
      "Reconstruction loss: 187.10253064174148, KL divergence: 0.09939413217736931\n",
      "Reconstruction loss: 216.45332506923958, KL divergence: 0.5638726146042135\n",
      "Reconstruction loss: 219.5052209879184, KL divergence: 0.08204621208528273\n",
      "Reconstruction loss: 199.75958048590152, KL divergence: 0.07529557573166074\n",
      "Reconstruction loss: 188.17462335916593, KL divergence: 0.14791549810945948\n",
      "Reconstruction loss: 175.76675303492675, KL divergence: 0.45937642591961947\n",
      "Reconstruction loss: 213.4384679623651, KL divergence: 0.03304762380473597\n",
      "Reconstruction loss: 168.80090968081169, KL divergence: 0.2302008159089044\n",
      "Reconstruction loss: 200.70185036868855, KL divergence: 0.12732034092267847\n",
      "Reconstruction loss: 221.15639808823607, KL divergence: 0.036760707437659745\n",
      "Reconstruction loss: 193.28329447957373, KL divergence: 0.09936657860484632\n",
      "Reconstruction loss: 217.25914195518988, KL divergence: 0.20167611673558283\n",
      "Reconstruction loss: 237.11168993189966, KL divergence: 0.02067553997704208\n",
      "Reconstruction loss: 225.3249633118789, KL divergence: 0.15824019861464417\n",
      "Reconstruction loss: 187.22745512013216, KL divergence: 0.1413748817259276\n",
      "Reconstruction loss: 235.9067722290447, KL divergence: 0.6702240705987015\n",
      "Reconstruction loss: 189.89319177649605, KL divergence: 0.04742452040556899\n",
      "Reconstruction loss: 202.0661232099655, KL divergence: 0.052706635744657004\n",
      "Reconstruction loss: 266.06823994419034, KL divergence: 0.0582685690224804\n",
      "Reconstruction loss: 185.54642211900716, KL divergence: 0.021460366227798455\n",
      "Reconstruction loss: 272.01680441479596, KL divergence: 4.176118051193659\n",
      "Reconstruction loss: 163.6574652000835, KL divergence: 0.12076330432569576\n",
      "Reconstruction loss: 207.32546773270576, KL divergence: 0.03865160311287286\n",
      "Reconstruction loss: 233.98392325855338, KL divergence: 0.2151673954711203\n",
      "Reconstruction loss: 184.32547944756715, KL divergence: 0.12003372862380657\n",
      "Reconstruction loss: 167.24542298533186, KL divergence: 0.11335329405357775\n",
      "Reconstruction loss: 184.81873978149846, KL divergence: 0.05306534529346946\n",
      "Reconstruction loss: 282.94653469827915, KL divergence: 1.7794533073583951\n",
      "Reconstruction loss: 311.2082051392354, KL divergence: 3.0950043808548244\n",
      "Reconstruction loss: 116.4546960067174, KL divergence: 0.2882198117282062\n",
      "Reconstruction loss: 265.9837369865182, KL divergence: 1.3595643326680695\n",
      "Reconstruction loss: 160.09690790996228, KL divergence: 0.16359761424873842\n",
      "Reconstruction loss: 164.0627208622023, KL divergence: 0.01728378271758868\n",
      "Reconstruction loss: 209.57795102997738, KL divergence: 0.017581202918596384\n",
      "Reconstruction loss: 192.4891466954199, KL divergence: 0.324205252571721\n",
      "Reconstruction loss: 216.1166664888479, KL divergence: 3.396492899797284\n",
      "Reconstruction loss: 157.43082453211719, KL divergence: 0.13345469372582652\n",
      "Reconstruction loss: 187.20853212935606, KL divergence: 0.26202541182823386\n",
      "Reconstruction loss: 231.01631037893995, KL divergence: 0.16618312283437203\n",
      "Reconstruction loss: 176.43346485125315, KL divergence: 0.138460871714071\n",
      "Reconstruction loss: 190.4995824024772, KL divergence: 0.10087752483061702\n",
      "Reconstruction loss: 150.100570453401, KL divergence: 0.18976087524222174\n",
      "Reconstruction loss: 226.00878006064553, KL divergence: 0.017630170626468822\n",
      "Reconstruction loss: 216.79288402876878, KL divergence: 0.02153580063762872\n",
      "Reconstruction loss: 243.9493378352944, KL divergence: 0.6821042576915946\n",
      "Reconstruction loss: 157.98694964212277, KL divergence: 0.11466082292092561\n",
      "Reconstruction loss: 145.1370384235724, KL divergence: 0.27979420892397316\n",
      "Reconstruction loss: 185.04841326772907, KL divergence: 0.02115650322488405\n",
      "Reconstruction loss: 224.09549874002306, KL divergence: 1.591827182527283\n",
      "Reconstruction loss: 270.54646383075186, KL divergence: 0.4194843179270992\n",
      "Reconstruction loss: 207.20148734721326, KL divergence: 0.014638945669349701\n",
      "Reconstruction loss: 277.083535025345, KL divergence: 3.300255231356389\n",
      "Reconstruction loss: 220.6054579371059, KL divergence: 0.8928500790895764\n",
      "Reconstruction loss: 193.615871797725, KL divergence: 0.022803816034840663\n",
      "Reconstruction loss: 197.1736163106848, KL divergence: 0.017357389711113558\n",
      "Reconstruction loss: 168.77608776835825, KL divergence: 0.13234605549495326\n",
      "Reconstruction loss: 273.41493025842243, KL divergence: 1.2913341056364438\n",
      "Reconstruction loss: 236.39474357424854, KL divergence: 0.36352463924648104\n",
      "Reconstruction loss: 222.9213944120825, KL divergence: 0.03575396899681327\n",
      "Reconstruction loss: 151.2098708565181, KL divergence: 0.10252350035673996\n",
      "Reconstruction loss: 301.2287594243411, KL divergence: 1.3971118250540084\n",
      "Reconstruction loss: 222.291546817891, KL divergence: 3.8689864557875198\n",
      "Reconstruction loss: 264.5441800795247, KL divergence: 0.5802584701983224\n",
      "Reconstruction loss: 208.09824960188354, KL divergence: 5.59119422608898\n",
      "Reconstruction loss: 137.54184877661388, KL divergence: 0.16459901297353313\n",
      "Reconstruction loss: 252.88986747414324, KL divergence: 1.0770785300292687\n",
      "Reconstruction loss: 162.89985999842057, KL divergence: 0.18815663011441258\n",
      "Reconstruction loss: 230.38996625359397, KL divergence: 0.035034144299692926\n",
      "Reconstruction loss: 221.63755877870966, KL divergence: 0.028729922639290695\n",
      "Reconstruction loss: 173.116988423235, KL divergence: 0.08253811882616674\n",
      "Reconstruction loss: 199.59282805586037, KL divergence: 0.018699531649309542\n",
      "Reconstruction loss: 251.5535090353112, KL divergence: 2.2507597123634477\n",
      "Reconstruction loss: 191.32614236393238, KL divergence: 0.22397566644982425\n",
      "Reconstruction loss: 157.32803021630698, KL divergence: 0.1029315990640996\n",
      "Reconstruction loss: 240.82791542939856, KL divergence: 0.31877850261458657\n",
      "Reconstruction loss: 186.0965258823809, KL divergence: 0.160353211378572\n",
      "Reconstruction loss: 118.72936695329493, KL divergence: 0.2607189478634888\n",
      "Reconstruction loss: 147.0821418250073, KL divergence: 0.4230695714405409\n",
      "Reconstruction loss: 222.89956238670234, KL divergence: 0.2393939331152698\n",
      "Reconstruction loss: 171.58205915017012, KL divergence: 0.038494427452705926\n",
      "Reconstruction loss: 265.198060999106, KL divergence: 0.05915947545153277\n",
      "Reconstruction loss: 130.47027378559466, KL divergence: 0.22316271168210516\n",
      "Reconstruction loss: 166.04282724976525, KL divergence: 0.14104415482953353\n",
      "Reconstruction loss: 128.62267596638006, KL divergence: 0.43646393163158953\n",
      "Reconstruction loss: 246.29006474013133, KL divergence: 0.4013828487668987\n",
      "Reconstruction loss: 153.9409774760346, KL divergence: 0.20767690618106593\n",
      "Reconstruction loss: 366.410817869571, KL divergence: 2.6919873867266295\n",
      "Reconstruction loss: 230.2931361184523, KL divergence: 0.07150504801731594\n",
      "Reconstruction loss: 194.34596105471954, KL divergence: 0.04675924021973549\n",
      "Reconstruction loss: 139.20959733173675, KL divergence: 0.17792346089370825\n",
      "Reconstruction loss: 103.57891369117974, KL divergence: 0.3216389169486178\n",
      "Reconstruction loss: 246.84606196401063, KL divergence: 1.614422147716371\n",
      "Reconstruction loss: 158.90101741830694, KL divergence: 0.28101888881603404\n",
      "Reconstruction loss: 127.1744618104164, KL divergence: 0.43256512477163916\n",
      "Reconstruction loss: 237.64659841485457, KL divergence: 0.5714014923146142\n",
      "Reconstruction loss: 132.00666703643128, KL divergence: 0.457534739181687\n",
      "Reconstruction loss: 134.5159870907275, KL divergence: 0.290080972632301\n",
      "Reconstruction loss: 172.0553451523591, KL divergence: 0.07559720621380756\n",
      "Reconstruction loss: 145.18936499990585, KL divergence: 0.396025802654384\n",
      "Reconstruction loss: 242.92123873935788, KL divergence: 0.7974937185576998\n",
      "Reconstruction loss: 248.4383289923012, KL divergence: 0.5685109299980308\n",
      "Reconstruction loss: 205.69556949984343, KL divergence: 0.26321399279279395\n",
      "Reconstruction loss: 219.88867300778873, KL divergence: 0.39011296725635075\n",
      "Reconstruction loss: 168.10578702242367, KL divergence: 0.08970384889804289\n",
      "Reconstruction loss: 182.1257271036484, KL divergence: 0.14238478675238814\n",
      "Reconstruction loss: 271.00562771523613, KL divergence: 0.3424565626996561\n",
      "Reconstruction loss: 184.76422944110104, KL divergence: 0.12201141625550871\n",
      "Reconstruction loss: 188.38788272908548, KL divergence: 0.10526970166223143\n",
      "Reconstruction loss: 215.00602849771647, KL divergence: 6.728261072579226\n",
      "Reconstruction loss: 107.01799366253401, KL divergence: 0.21092714751184244\n",
      "Reconstruction loss: 316.64449903183106, KL divergence: 1.6810445261263274\n",
      "Reconstruction loss: 162.86439212093123, KL divergence: 0.11652993168854248\n",
      "Reconstruction loss: 159.58974103762998, KL divergence: 0.3133046708650252\n",
      "Reconstruction loss: 306.1120598653603, KL divergence: 1.3071709133151899\n",
      "Reconstruction loss: 258.27776294326367, KL divergence: 0.3972439948693012\n",
      "Reconstruction loss: 174.15903964422944, KL divergence: 0.04309871604190152\n",
      "Reconstruction loss: 241.2610686746529, KL divergence: 0.14149514469568714\n",
      "Reconstruction loss: 164.88503358072836, KL divergence: 0.2226364078909034\n",
      "Reconstruction loss: 185.48093110366116, KL divergence: 3.4962468667888906\n",
      "Reconstruction loss: 151.60581410319085, KL divergence: 0.22067153338804624\n",
      "Reconstruction loss: 295.4911808786419, KL divergence: 0.45948377417821273\n",
      "Reconstruction loss: 118.3454055350177, KL divergence: 0.5315215237915341\n",
      "Reconstruction loss: 237.7804248949231, KL divergence: 2.6542352818463977\n",
      "Reconstruction loss: 251.51752229705667, KL divergence: 1.053935413678393\n",
      "Reconstruction loss: 220.71989034005065, KL divergence: 0.062068092531194996\n",
      "Reconstruction loss: 207.16357384897344, KL divergence: 0.16580157518130273\n",
      "Reconstruction loss: 164.073528321828, KL divergence: 0.1347077869819609\n",
      "Reconstruction loss: 181.06201013487373, KL divergence: 0.13564142904426485\n",
      "Reconstruction loss: 206.87469966702588, KL divergence: 0.8102469335063154\n",
      "Reconstruction loss: 259.9987673585611, KL divergence: 1.0644816890723314\n",
      "Reconstruction loss: 225.40635449452003, KL divergence: 0.03472023765758131\n",
      "Reconstruction loss: 195.20688719922575, KL divergence: 0.11404129607456737\n",
      "Reconstruction loss: 211.90983445003408, KL divergence: 1.9896834162168708\n",
      "Reconstruction loss: 267.47668990800605, KL divergence: 0.3585921583663237\n",
      "Reconstruction loss: 230.00617459483826, KL divergence: 0.2592207700520618\n",
      "Reconstruction loss: 206.92615245451756, KL divergence: 0.022922667567626953\n",
      "Reconstruction loss: 238.59223352743152, KL divergence: 4.558638403425789\n",
      "Reconstruction loss: 192.0082937299895, KL divergence: 0.28802654095181407\n",
      "Reconstruction loss: 195.69920490948462, KL divergence: 0.12173913630795158\n",
      "Reconstruction loss: 277.33918835641254, KL divergence: 0.054810204347844504\n",
      "Reconstruction loss: 201.16881806478006, KL divergence: 0.7439326184988962\n",
      "Reconstruction loss: 199.67435555325744, KL divergence: 0.09623856460852886\n",
      "Reconstruction loss: 197.77146974831598, KL divergence: 0.02760396334990417\n",
      "Reconstruction loss: 221.8806453460461, KL divergence: 0.26414559282162486\n",
      "Reconstruction loss: 201.22149403335015, KL divergence: 0.03949904976172547\n",
      "Reconstruction loss: 222.48754510281088, KL divergence: 0.0345786225802886\n",
      "Reconstruction loss: 191.1319394684802, KL divergence: 0.022598623111413196\n",
      "Reconstruction loss: 134.04506662952133, KL divergence: 0.5025421355593049\n",
      "Reconstruction loss: 182.2301886402008, KL divergence: 0.026889904094777206\n",
      "Reconstruction loss: 241.14899813853046, KL divergence: 3.325491764627791\n",
      "Reconstruction loss: 211.96137784990222, KL divergence: 1.6115618642980336\n",
      "Reconstruction loss: 165.45386017937088, KL divergence: 0.6019547406643282\n",
      "Reconstruction loss: 141.32759105904012, KL divergence: 0.39588067064462534\n",
      "Reconstruction loss: 209.22438710195865, KL divergence: 0.15849842192267688\n",
      "Reconstruction loss: 189.880384298707, KL divergence: 0.046905951271284996\n",
      "Reconstruction loss: 192.50077801113827, KL divergence: 0.14174909817598008\n",
      "Reconstruction loss: 287.558073138586, KL divergence: 6.238463364582335\n",
      "Reconstruction loss: 184.61232546633403, KL divergence: 0.12302052019959459\n",
      "Reconstruction loss: 160.23823133623853, KL divergence: 0.09418779228192115\n",
      "Reconstruction loss: 195.4139468653006, KL divergence: 0.1907082152113016\n",
      "Reconstruction loss: 157.65088580878012, KL divergence: 0.1783786454384768\n",
      "Reconstruction loss: 199.63380636205568, KL divergence: 0.13530018827662743\n",
      "Reconstruction loss: 221.10896369599186, KL divergence: 0.39644075631408177\n",
      "Reconstruction loss: 262.3282073355788, KL divergence: 1.0636062728639475\n",
      "Reconstruction loss: 219.3543707315853, KL divergence: 0.19408992684919563\n",
      "Reconstruction loss: 284.1032086895558, KL divergence: 2.6465172182825096\n",
      "Reconstruction loss: 235.84617916229058, KL divergence: 0.12066911675376352\n",
      "Reconstruction loss: 215.25366703743424, KL divergence: 0.08716826372617215\n",
      "Reconstruction loss: 193.77381755163546, KL divergence: 0.21214282016083869\n",
      "Reconstruction loss: 211.0807912030554, KL divergence: 0.17503424245235327\n",
      "Reconstruction loss: 251.6049613077705, KL divergence: 3.8708895907201097\n",
      "Reconstruction loss: 168.95865290003067, KL divergence: 0.02017817547366091\n",
      "Reconstruction loss: 198.76035637091803, KL divergence: 0.017797723123024578\n",
      "Reconstruction loss: 158.06445838764395, KL divergence: 0.462782980895695\n",
      "Reconstruction loss: 206.5277067921565, KL divergence: 3.2380292688163372\n",
      "Reconstruction loss: 173.29705656220167, KL divergence: 0.43793624026358835\n",
      "Reconstruction loss: 222.36757443690453, KL divergence: 0.054234492765107334\n",
      "Reconstruction loss: 165.6250854349414, KL divergence: 0.2623000736984392\n",
      "Reconstruction loss: 162.96961056789576, KL divergence: 0.5077442870895935\n",
      "Reconstruction loss: 173.7317748784174, KL divergence: 0.05144997544685648\n",
      "Reconstruction loss: 169.40802309367896, KL divergence: 0.08624132966894377\n",
      "Reconstruction loss: 142.6463016760033, KL divergence: 0.1118421721118088\n",
      "Reconstruction loss: 220.6015707748868, KL divergence: 0.3761608383126292\n",
      "Reconstruction loss: 181.3424460918149, KL divergence: 0.01935326108336277\n",
      "Reconstruction loss: 285.08312215009187, KL divergence: 0.4719442357431199\n",
      "Reconstruction loss: 186.23191107518986, KL divergence: 0.04428991829963724\n",
      "Reconstruction loss: 178.43308641733896, KL divergence: 0.1400054492278946\n",
      "Reconstruction loss: 210.25497552040355, KL divergence: 0.18507651678753123\n",
      "Reconstruction loss: 227.34918057614362, KL divergence: 0.019264452606486515\n",
      "Reconstruction loss: 189.80652976596673, KL divergence: 0.15371911268978228\n",
      "Reconstruction loss: 205.88874764445393, KL divergence: 0.28278786048289617\n",
      "Reconstruction loss: 276.51529062814626, KL divergence: 2.450814903865961\n",
      "Reconstruction loss: 213.56214231857615, KL divergence: 0.051161725425493854\n",
      "Reconstruction loss: 273.06916991813796, KL divergence: 4.542672581645714\n",
      "Reconstruction loss: 224.32849624590608, KL divergence: 3.158107401729745\n",
      "Reconstruction loss: 211.47594531789088, KL divergence: 0.3972354672943173\n",
      "Reconstruction loss: 249.770828055028, KL divergence: 0.2062097942333883\n",
      "Reconstruction loss: 212.49717781855878, KL divergence: 0.03947651436483923\n",
      "Reconstruction loss: 160.70302268510602, KL divergence: 0.36666871945356116\n",
      "Reconstruction loss: 165.47352157418985, KL divergence: 0.21217016694471474\n",
      "Reconstruction loss: 148.55379740070254, KL divergence: 0.41835724188666445\n",
      "Reconstruction loss: 206.61265852195774, KL divergence: 0.9849697682427732\n",
      "Reconstruction loss: 219.2442052936614, KL divergence: 0.14082350978642766\n",
      "Reconstruction loss: 167.5882593794484, KL divergence: 0.1306276492491968\n",
      "Reconstruction loss: 310.9736289840365, KL divergence: 0.19807418212269334\n",
      "Reconstruction loss: 154.90380512609238, KL divergence: 0.4110876279475942\n",
      "Reconstruction loss: 183.76082164244667, KL divergence: 0.05193231553331745\n",
      "Reconstruction loss: 233.37743183162564, KL divergence: 1.8919184991513087\n",
      "Reconstruction loss: 239.73539597247736, KL divergence: 0.04268959855601745\n",
      "Reconstruction loss: 250.49406231759912, KL divergence: 0.1713785781902859\n",
      "Reconstruction loss: 171.66462267408946, KL divergence: 0.8902811448252342\n",
      "Reconstruction loss: 173.98538465515358, KL divergence: 0.5379495293307753\n",
      "Reconstruction loss: 207.53829812895378, KL divergence: 0.3775636604111775\n",
      "Reconstruction loss: 223.70325256200152, KL divergence: 0.28424573491596106\n",
      "Reconstruction loss: 148.3099008057024, KL divergence: 0.3410595532511024\n",
      "Reconstruction loss: 186.74380364241023, KL divergence: 0.1464113371108834\n",
      "Reconstruction loss: 206.80224520736658, KL divergence: 1.568016985217774\n",
      "Reconstruction loss: 148.4320271371655, KL divergence: 0.29310065726674867\n",
      "Reconstruction loss: 254.82974196475962, KL divergence: 0.38093983272365445\n",
      "Reconstruction loss: 223.61918688768503, KL divergence: 0.08184853847741236\n",
      "Reconstruction loss: 211.39023125119007, KL divergence: 0.09770418217979798\n",
      "Reconstruction loss: 256.07145012225885, KL divergence: 0.13956656963279124\n",
      "Reconstruction loss: 147.04779082111943, KL divergence: 0.6767236214573811\n",
      "Reconstruction loss: 191.34946485696207, KL divergence: 0.06469040309791119\n",
      "Reconstruction loss: 137.1223417882443, KL divergence: 0.3717651896108301\n",
      "Reconstruction loss: 242.4287911800694, KL divergence: 0.28921593862868805\n",
      "Reconstruction loss: 275.60179459509595, KL divergence: 1.5240875114437902\n",
      "Reconstruction loss: 195.82252396538615, KL divergence: 0.018612676552577867\n",
      "Reconstruction loss: 133.5837884929452, KL divergence: 0.20429818744426687\n",
      "Reconstruction loss: 226.78332638770777, KL divergence: 0.03364603038891334\n",
      "Reconstruction loss: 203.4328178735181, KL divergence: 0.05452532794518811\n",
      "Reconstruction loss: 164.60609165746058, KL divergence: 0.2600645982510385\n",
      "Reconstruction loss: 211.28609087636545, KL divergence: 2.112962419329553\n",
      "Reconstruction loss: 194.75179821548605, KL divergence: 0.06604535858163268\n",
      "Reconstruction loss: 271.5374593960157, KL divergence: 0.022830178537306955\n",
      "Reconstruction loss: 217.9518621372129, KL divergence: 0.06226501625840092\n",
      "Reconstruction loss: 178.21355397225472, KL divergence: 0.02171155471589009\n",
      "Reconstruction loss: 241.85461355864516, KL divergence: 1.4138106173311429\n",
      "Reconstruction loss: 216.7501419859732, KL divergence: 0.1460649515608845\n",
      "Reconstruction loss: 146.9824181339473, KL divergence: 0.9669450477195627\n",
      "Reconstruction loss: 198.44426155539918, KL divergence: 0.15921419419442923\n",
      "Reconstruction loss: 205.9253418921894, KL divergence: 0.11267593451504465\n",
      "Reconstruction loss: 145.48296343285358, KL divergence: 0.31939203301464403\n",
      "Reconstruction loss: 207.16463220112425, KL divergence: 0.018900135998290213\n",
      "Reconstruction loss: 246.11444597893194, KL divergence: 0.13949604206596472\n",
      "Reconstruction loss: 330.31286523878464, KL divergence: 0.13128190284157842\n",
      "Reconstruction loss: 145.31649497489008, KL divergence: 0.31710751286039035\n",
      "Reconstruction loss: 249.03656869924868, KL divergence: 0.33342522739785\n",
      "Reconstruction loss: 115.75344839220847, KL divergence: 0.707166498348701\n",
      "Reconstruction loss: 226.0603153727856, KL divergence: 0.0980048844982524\n",
      "Reconstruction loss: 228.58642770127358, KL divergence: 0.7238502244228309\n",
      "Reconstruction loss: 258.5981599364508, KL divergence: 0.03360057231339425\n",
      "Reconstruction loss: 210.320321316064, KL divergence: 0.11816490303831495\n",
      "Reconstruction loss: 199.99999981345198, KL divergence: 1.0582749350538592\n",
      "Reconstruction loss: 255.77395288979153, KL divergence: 0.05986555009112182\n",
      "Reconstruction loss: 149.18968613837256, KL divergence: 0.7557925125105118\n",
      "Reconstruction loss: 178.62505032866022, KL divergence: 0.15623838734603535\n",
      "Reconstruction loss: 123.35073279972801, KL divergence: 0.37510241125971316\n",
      "Reconstruction loss: 194.91416777314583, KL divergence: 0.8611345972116398\n",
      "Reconstruction loss: 223.18260775684882, KL divergence: 0.023886354728258286\n",
      "Reconstruction loss: 201.89877143007948, KL divergence: 0.03398647406918398\n",
      "Reconstruction loss: 217.6536548357774, KL divergence: 0.5393909287518225\n",
      "Reconstruction loss: 173.5882964247613, KL divergence: 0.09705447497774133\n",
      "Reconstruction loss: 248.65342772499685, KL divergence: 1.4200698687641005\n",
      "Reconstruction loss: 210.0600806537875, KL divergence: 3.5633661106863306\n",
      "Reconstruction loss: 191.91074245712298, KL divergence: 0.08773159825957738\n",
      "Reconstruction loss: 229.6943136728058, KL divergence: 0.29080918535603695\n",
      "Reconstruction loss: 270.2727218629811, KL divergence: 7.983795123184933\n",
      "Reconstruction loss: 116.11323562124863, KL divergence: 0.7493983330041694\n",
      "Reconstruction loss: 182.16629424142423, KL divergence: 0.11816490303831495\n",
      "Reconstruction loss: 254.2126498154829, KL divergence: 0.08199037783854235\n",
      "Reconstruction loss: 116.13424841961512, KL divergence: 0.453927541556301\n",
      "Reconstruction loss: 229.62751803115134, KL divergence: 0.1266670286243452\n",
      "Reconstruction loss: 208.81788642410757, KL divergence: 0.03427142222566126\n",
      "Reconstruction loss: 148.62538889681792, KL divergence: 0.6325322480724995\n",
      "Reconstruction loss: 235.67175929824765, KL divergence: 3.9895873520299494\n",
      "Reconstruction loss: 179.04068259530027, KL divergence: 0.1933855611733412\n",
      "Reconstruction loss: 168.77676680487522, KL divergence: 0.15087070684802822\n",
      "Reconstruction loss: 229.14382850560867, KL divergence: 0.11221521676527924\n",
      "Reconstruction loss: 235.63717613486014, KL divergence: 0.054672938140324145\n",
      "Reconstruction loss: 138.21387901295907, KL divergence: 0.13735256873022667\n",
      "Reconstruction loss: 169.88293434718693, KL divergence: 0.11842982913144412\n",
      "Reconstruction loss: 223.27537951254777, KL divergence: 0.03968449559497389\n",
      "Reconstruction loss: 272.18506731964266, KL divergence: 0.01971216998737224\n",
      "Reconstruction loss: 218.38847072623872, KL divergence: 0.1256749549272677\n",
      "Reconstruction loss: 131.2680796712848, KL divergence: 0.4166629358641223\n",
      "Reconstruction loss: 176.09813616927167, KL divergence: 0.24319364960316842\n",
      "Reconstruction loss: 164.33854936791852, KL divergence: 0.2191116697491367\n",
      "Reconstruction loss: 211.11962497593933, KL divergence: 0.08130822803597465\n",
      "Reconstruction loss: 172.87671466001572, KL divergence: 0.32903291108615296\n",
      "Reconstruction loss: 174.626031360304, KL divergence: 0.13113068906048064\n",
      "Reconstruction loss: 213.01373209366346, KL divergence: 0.12111488240064494\n",
      "Reconstruction loss: 161.5743944064169, KL divergence: 0.4178480164176075\n",
      "Reconstruction loss: 172.12486918524445, KL divergence: 0.24141600249877782\n",
      "Reconstruction loss: 234.76165191219087, KL divergence: 0.12077222089397194\n",
      "Reconstruction loss: 124.85163191079951, KL divergence: 0.9309845804341694\n",
      "Reconstruction loss: 261.6054511051941, KL divergence: 0.26176963720102137\n",
      "Reconstruction loss: 264.7353604134893, KL divergence: 0.3336922796555124\n",
      "Reconstruction loss: 192.92357531935028, KL divergence: 0.035229031141690514\n",
      "Reconstruction loss: 223.31666172711203, KL divergence: 0.018622035076083288\n",
      "Reconstruction loss: 148.3827346800386, KL divergence: 0.12077222089397194\n",
      "Reconstruction loss: 164.21825388556772, KL divergence: 0.19455387080888592\n",
      "Reconstruction loss: 210.3536345032646, KL divergence: 0.19036969534712245\n",
      "Reconstruction loss: 239.25792693402778, KL divergence: 3.9919001106626233\n",
      "Reconstruction loss: 154.18081978071575, KL divergence: 0.3521671147516517\n",
      "Reconstruction loss: 208.6243679076885, KL divergence: 1.1076958192524464\n",
      "Reconstruction loss: 293.30031546148683, KL divergence: 0.06668720152816249\n",
      "Reconstruction loss: 227.7907182440684, KL divergence: 1.1558340258665138\n",
      "Reconstruction loss: 228.11099062384653, KL divergence: 0.13577620089483805\n",
      "Reconstruction loss: 244.8117973287964, KL divergence: 0.49989591343753315\n",
      "Reconstruction loss: 213.92841103664173, KL divergence: 0.12264996908730103\n",
      "Reconstruction loss: 202.8991032292814, KL divergence: 0.16075176375776884\n",
      "Reconstruction loss: 259.5251189123633, KL divergence: 0.05466291668630263\n",
      "Reconstruction loss: 212.82850949315502, KL divergence: 0.055097485856264006\n",
      "Reconstruction loss: 219.553138235229, KL divergence: 0.3761938726014585\n",
      "Reconstruction loss: 348.01276988635715, KL divergence: 0.10743542648936127\n",
      "Reconstruction loss: 260.10837366673354, KL divergence: 0.048001661993307465\n",
      "Reconstruction loss: 221.88548060432973, KL divergence: 0.019561332696170164\n",
      "Reconstruction loss: 266.02962983296885, KL divergence: 0.19660513963294246\n",
      "Reconstruction loss: 188.9128592347633, KL divergence: 0.12264996908730103\n",
      "Reconstruction loss: 267.1344977702331, KL divergence: 0.12264996908730103\n",
      "Reconstruction loss: 174.06556214074726, KL divergence: 0.3239731920176076\n",
      "Reconstruction loss: 127.4858871827421, KL divergence: 0.5935439694848244\n",
      "Reconstruction loss: 181.38106966905252, KL divergence: 0.27068042194798353\n",
      "Reconstruction loss: 254.93460248010862, KL divergence: 0.020200619271875275\n",
      "Reconstruction loss: 312.05578760119624, KL divergence: 0.05217825083939681\n",
      "Reconstruction loss: 180.22661149439008, KL divergence: 0.09799648988713477\n",
      "Reconstruction loss: 197.0181207477386, KL divergence: 0.32337811389276333\n",
      "Reconstruction loss: 242.63934003093073, KL divergence: 6.906644030092696\n",
      "Reconstruction loss: 253.70419811225295, KL divergence: 9.219737180818155\n",
      "Reconstruction loss: 292.4904696527385, KL divergence: 0.9944497940746133\n",
      "Reconstruction loss: 309.8787126441138, KL divergence: 1.3433884112163808\n",
      "Reconstruction loss: 343.11772281387493, KL divergence: 2.4092305616062153\n",
      "Reconstruction loss: 173.716464242171, KL divergence: 0.12264996908730103\n",
      "Reconstruction loss: 211.7330636248168, KL divergence: 2.234714521098124\n",
      "Reconstruction loss: 190.60159329884044, KL divergence: 0.033051986861205196\n",
      "Reconstruction loss: 218.91907824850492, KL divergence: 0.0777737766421927\n",
      "Reconstruction loss: 239.71201157408808, KL divergence: 6.8466622266101576\n",
      "Reconstruction loss: 285.3995197489984, KL divergence: 2.1937499013426685\n",
      "Reconstruction loss: 216.70576575632984, KL divergence: 0.24794300286531196\n",
      "Reconstruction loss: 102.86030521816751, KL divergence: 0.6432185744527158\n",
      "Reconstruction loss: 235.0497071674709, KL divergence: 0.022498973194039484\n",
      "Reconstruction loss: 212.8699334537272, KL divergence: 0.045722676768619686\n",
      "Reconstruction loss: 158.97969926884642, KL divergence: 0.12320350429949362\n",
      "Reconstruction loss: 228.02736964295664, KL divergence: 0.12320350429949362\n",
      "Reconstruction loss: 205.94017503704328, KL divergence: 0.19935227769789388\n",
      "Reconstruction loss: 287.9509043018543, KL divergence: 0.452781625759368\n",
      "Reconstruction loss: 260.22968736226443, KL divergence: 1.0590071697638588\n",
      "Reconstruction loss: 179.22132364581586, KL divergence: 0.12320350429949362\n",
      "Reconstruction loss: 211.30300034135593, KL divergence: 0.04139128732596825\n",
      "Reconstruction loss: 137.31784103669668, KL divergence: 0.12320350429949362\n",
      "Reconstruction loss: 241.97709110998733, KL divergence: 0.4208048636218399\n",
      "Reconstruction loss: 167.59221806042527, KL divergence: 0.5499924768140437\n",
      "Reconstruction loss: 211.91231524072117, KL divergence: 0.12320350429949362\n",
      "Reconstruction loss: 219.1377495514834, KL divergence: 2.779797825908892\n",
      "Reconstruction loss: 227.08769276366712, KL divergence: 0.4648744927888439\n",
      "Reconstruction loss: 255.90238429805947, KL divergence: 0.1130336308067234\n",
      "Reconstruction loss: 262.1774609409723, KL divergence: 0.20130503787169135\n",
      "Reconstruction loss: 252.42447988327217, KL divergence: 0.11867188375511423\n",
      "Reconstruction loss: 180.61977506778317, KL divergence: 0.03187967693797822\n",
      "Reconstruction loss: 230.63692593944398, KL divergence: 0.04821939011319093\n",
      "Reconstruction loss: 226.59685030535564, KL divergence: 0.2495573306135051\n",
      "Reconstruction loss: 119.00136539719317, KL divergence: 0.519813758052531\n",
      "Reconstruction loss: 139.94737079136956, KL divergence: 0.28513680932555036\n",
      "Reconstruction loss: 294.69975018931774, KL divergence: 2.593835909987592\n",
      "Reconstruction loss: 313.9331240603393, KL divergence: 0.07605807277430293\n",
      "Reconstruction loss: 218.7572164879224, KL divergence: 0.48616010961292017\n",
      "Reconstruction loss: 190.1832824819656, KL divergence: 0.12387788506308872\n",
      "Reconstruction loss: 196.20560921976798, KL divergence: 0.135845999969965\n",
      "Reconstruction loss: 150.86147947040484, KL divergence: 0.2267795154395985\n",
      "Reconstruction loss: 176.37372649820878, KL divergence: 0.03353127302359932\n",
      "Reconstruction loss: 191.94020404357948, KL divergence: 0.1283855962472784\n",
      "Reconstruction loss: 179.37612595131156, KL divergence: 0.3872859701289741\n",
      "Reconstruction loss: 193.59035886629172, KL divergence: 0.021949022878257285\n",
      "Reconstruction loss: 211.93856784869473, KL divergence: 0.05482781448113655\n",
      "Reconstruction loss: 190.47241482827502, KL divergence: 0.12400859776171247\n",
      "Reconstruction loss: 107.97951367452507, KL divergence: 0.599256764053459\n",
      "Reconstruction loss: 172.7307752166863, KL divergence: 0.12515982160754963\n",
      "Reconstruction loss: 191.86471604650768, KL divergence: 0.38504454634836155\n",
      "Reconstruction loss: 170.3126508965558, KL divergence: 0.9955174944010987\n",
      "Reconstruction loss: 185.74168549183366, KL divergence: 0.026551357527376973\n",
      "Reconstruction loss: 158.8816545601179, KL divergence: 0.016319656246644942\n",
      "Reconstruction loss: 169.06112998590743, KL divergence: 0.07730402167892642\n",
      "Reconstruction loss: 208.37625061985452, KL divergence: 4.225125390288347\n",
      "Reconstruction loss: 135.6407065132408, KL divergence: 0.2686532924819554\n",
      "Reconstruction loss: 201.7363804843513, KL divergence: 0.12658732516385135\n",
      "Reconstruction loss: 245.44458621170185, KL divergence: 0.08846435990722556\n",
      "Reconstruction loss: 157.3387710932318, KL divergence: 0.09535081438266702\n",
      "Reconstruction loss: 181.40295406199925, KL divergence: 0.1874267841473971\n",
      "Reconstruction loss: 189.01026356035163, KL divergence: 0.03569348822067642\n",
      "Reconstruction loss: 186.43421394154961, KL divergence: 1.898278536724026\n",
      "Reconstruction loss: 287.6217920795771, KL divergence: 1.539346725682066\n",
      "Reconstruction loss: 175.80086863520015, KL divergence: 0.1217426094356141\n",
      "Reconstruction loss: 139.0473618334202, KL divergence: 0.1988480614741887\n",
      "Reconstruction loss: 182.52656912265923, KL divergence: 0.06394128144071615\n",
      "Reconstruction loss: 198.49171778522395, KL divergence: 0.026266324607929936\n",
      "Reconstruction loss: 185.55747268656307, KL divergence: 0.12400859776171247\n",
      "Reconstruction loss: 215.562921333821, KL divergence: 1.3244523555404109\n",
      "Reconstruction loss: 215.1606628609777, KL divergence: 0.26492868478594356\n",
      "Reconstruction loss: 201.2259216762759, KL divergence: 0.08239624982169486\n",
      "Reconstruction loss: 192.13885840658816, KL divergence: 0.021158123079732694\n",
      "Reconstruction loss: 274.02067495529445, KL divergence: 1.2439928819646375\n",
      "Reconstruction loss: 197.93628710368313, KL divergence: 0.020173012671539925\n",
      "Reconstruction loss: 234.4563479037673, KL divergence: 6.414028890373663\n",
      "Reconstruction loss: 158.3587566673437, KL divergence: 0.08576115491636482\n",
      "Reconstruction loss: 137.43223807675926, KL divergence: 0.49423928347021895\n",
      "Reconstruction loss: 267.45046760482825, KL divergence: 0.3460945073338706\n",
      "Reconstruction loss: 162.68480640148442, KL divergence: 0.12466891292513133\n",
      "Reconstruction loss: 245.93474394812156, KL divergence: 0.03588883189033082\n",
      "Reconstruction loss: 200.07805314732258, KL divergence: 0.16853287773700426\n",
      "Reconstruction loss: 203.5766292687375, KL divergence: 0.10852608362136279\n",
      "Reconstruction loss: 253.2029162864807, KL divergence: 0.4099524259350234\n",
      "Reconstruction loss: 219.4933640542634, KL divergence: 0.1356678920271624\n",
      "Reconstruction loss: 163.9972731758231, KL divergence: 0.03892042850372124\n",
      "Reconstruction loss: 266.36959741038476, KL divergence: 0.14129278400948508\n",
      "Reconstruction loss: 180.74272238450084, KL divergence: 0.08284004396475153\n",
      "Reconstruction loss: 184.6789865002334, KL divergence: 0.08464477404414944\n",
      "Reconstruction loss: 229.83072992279472, KL divergence: 0.7666061237544008\n",
      "Reconstruction loss: 187.48791583891796, KL divergence: 0.12466891292513133\n",
      "Reconstruction loss: 199.59400734435508, KL divergence: 0.35439859505510707\n",
      "Reconstruction loss: 189.7174770135594, KL divergence: 0.75673265638335\n",
      "Reconstruction loss: 164.7528389706834, KL divergence: 0.18581096793699853\n",
      "Reconstruction loss: 247.302107793845, KL divergence: 0.1368334162323352\n",
      "Reconstruction loss: 125.23151869692782, KL divergence: 0.5751185418398892\n",
      "Reconstruction loss: 233.91899831678288, KL divergence: 2.9733543717569186\n",
      "Reconstruction loss: 336.71134886277423, KL divergence: 0.8058164206048429\n",
      "Reconstruction loss: 198.06511422936006, KL divergence: 0.36561924361255127\n",
      "Reconstruction loss: 141.97436810166082, KL divergence: 0.1573036002530792\n",
      "Reconstruction loss: 158.13501772425857, KL divergence: 0.12466891292513133\n",
      "Reconstruction loss: 161.56846744274642, KL divergence: 0.018605073817085982\n",
      "Reconstruction loss: 222.2702812164406, KL divergence: 1.3533729992246226\n",
      "Reconstruction loss: 211.74719733987615, KL divergence: 4.447310237659856\n",
      "Reconstruction loss: 183.41633786224344, KL divergence: 0.12599248261369633\n",
      "Reconstruction loss: 271.8602754351291, KL divergence: 0.7323587966439498\n",
      "Reconstruction loss: 178.53515535097873, KL divergence: 0.029050442967557932\n",
      "Reconstruction loss: 211.03068073193043, KL divergence: 0.1584722096755255\n",
      "Reconstruction loss: 158.1991389793092, KL divergence: 0.1854085673514846\n",
      "Reconstruction loss: 366.66977096497686, KL divergence: 3.506462801260671\n",
      "Reconstruction loss: 229.33712053625223, KL divergence: 0.16165131287686108\n",
      "Reconstruction loss: 247.6485786213829, KL divergence: 0.02971821585886547\n",
      "Reconstruction loss: 229.84355560214945, KL divergence: 3.6708586086726505\n",
      "Reconstruction loss: 193.45471509045387, KL divergence: 8.201213020956196\n",
      "Reconstruction loss: 198.0664207758855, KL divergence: 0.2721812172080338\n",
      "Reconstruction loss: 208.7334779975952, KL divergence: 0.2999768748409111\n",
      "Reconstruction loss: 124.35185938869199, KL divergence: 0.231207957131827\n",
      "Reconstruction loss: 217.99063815547976, KL divergence: 0.08802960967146817\n",
      "Reconstruction loss: 155.3331770119717, KL divergence: 0.12599248261369633\n",
      "Reconstruction loss: 149.2534808847501, KL divergence: 0.2575828026491607\n",
      "Reconstruction loss: 180.4165146157211, KL divergence: 0.015599248681839262\n",
      "Reconstruction loss: 149.88281813813592, KL divergence: 0.1485629037201892\n",
      "Reconstruction loss: 226.7470624533721, KL divergence: 0.7941600199494941\n",
      "Reconstruction loss: 229.44138479095568, KL divergence: 0.1606508869979808\n",
      "Reconstruction loss: 113.19103387260728, KL divergence: 0.2515814872260878\n",
      "Reconstruction loss: 170.60900062648682, KL divergence: 0.14560269649402846\n",
      "Reconstruction loss: 112.01794601849107, KL divergence: 0.44117025302603097\n",
      "Reconstruction loss: 164.74921146203616, KL divergence: 0.13940668439937465\n",
      "Reconstruction loss: 175.99919563354788, KL divergence: 6.265729894949103\n",
      "Reconstruction loss: 293.2095000554582, KL divergence: 5.6389967055079655\n",
      "Reconstruction loss: 190.08505648602477, KL divergence: 0.12599248261369633\n",
      "Reconstruction loss: 245.59919131397243, KL divergence: 0.12184998045737172\n",
      "Reconstruction loss: 240.696076950745, KL divergence: 1.2697905357251384\n",
      "Reconstruction loss: 207.73151136776363, KL divergence: 4.2874210550095935\n",
      "Reconstruction loss: 203.41717567078067, KL divergence: 4.025855925887669\n",
      "Reconstruction loss: 178.35893383560273, KL divergence: 0.043875178799909154\n",
      "Reconstruction loss: 216.1843880205426, KL divergence: 0.5023482929126055\n",
      "Reconstruction loss: 250.2539056158493, KL divergence: 0.682817309899016\n",
      "Reconstruction loss: 170.84378796709598, KL divergence: 0.129197648870141\n",
      "Reconstruction loss: 242.40700045352133, KL divergence: 2.486125826743428\n",
      "Reconstruction loss: 245.92117536096748, KL divergence: 2.22682368897566\n",
      "Reconstruction loss: 203.46949601320784, KL divergence: 0.019304412700784868\n",
      "Reconstruction loss: 181.1370037117878, KL divergence: 0.08362347931308045\n",
      "Reconstruction loss: 258.62494517284284, KL divergence: 1.3677020580443697\n",
      "Reconstruction loss: 260.3232005682025, KL divergence: 0.4624856941978078\n",
      "Reconstruction loss: 159.85926876812348, KL divergence: 0.024386786044930386\n",
      "Reconstruction loss: 239.78572532258084, KL divergence: 0.2488331355289095\n",
      "Reconstruction loss: 176.6522422248101, KL divergence: 0.129197648870141\n",
      "Reconstruction loss: 210.1478981569633, KL divergence: 0.05583191879150545\n",
      "Reconstruction loss: 155.9828771182624, KL divergence: 0.15274827923549017\n",
      "Reconstruction loss: 163.67755520727084, KL divergence: 0.129197648870141\n",
      "Reconstruction loss: 213.21106757141638, KL divergence: 1.2396461051128773\n",
      "Reconstruction loss: 118.69128885320502, KL divergence: 0.593012722551266\n",
      "Reconstruction loss: 244.34947515936625, KL divergence: 0.7191205967763721\n",
      "Reconstruction loss: 213.42050725888345, KL divergence: 5.755329712254138\n",
      "Reconstruction loss: 128.64457683383532, KL divergence: 0.129197648870141\n",
      "Reconstruction loss: 129.83901121876974, KL divergence: 0.32411713028291583\n",
      "Reconstruction loss: 223.44347801817156, KL divergence: 0.04226343149465561\n",
      "Reconstruction loss: 303.2617181163947, KL divergence: 2.3538231705356814\n",
      "Reconstruction loss: 286.87044082191125, KL divergence: 0.4829020470966194\n",
      "Reconstruction loss: 233.01559215386754, KL divergence: 0.03146645768363998\n",
      "Reconstruction loss: 235.54021625005765, KL divergence: 3.709133602619808\n",
      "Reconstruction loss: 260.6151395545402, KL divergence: 2.431113133778959\n",
      "Reconstruction loss: 247.27234375028078, KL divergence: 3.9360305448948094\n",
      "Reconstruction loss: 256.2582598809807, KL divergence: 3.411891180221522\n",
      "Reconstruction loss: 243.74826414174248, KL divergence: 0.24908170264479196\n",
      "Reconstruction loss: 219.20295950728058, KL divergence: 0.036017180296997964\n",
      "Reconstruction loss: 161.8420812805717, KL divergence: 0.1330417910567816\n",
      "Reconstruction loss: 137.2321194645561, KL divergence: 0.16976814410586627\n",
      "Reconstruction loss: 121.48638564634557, KL divergence: 0.4024123465566184\n",
      "Reconstruction loss: 195.72669722175846, KL divergence: 0.04315551030371739\n",
      "Reconstruction loss: 264.5201535405083, KL divergence: 0.0404654472335359\n",
      "Reconstruction loss: 183.50685103020652, KL divergence: 0.1330417910567816\n",
      "Reconstruction loss: 210.97395279428434, KL divergence: 0.023902637158371653\n",
      "Reconstruction loss: 168.03805994439693, KL divergence: 0.13989919868274026\n",
      "Reconstruction loss: 299.3090676495004, KL divergence: 0.7935786062943231\n",
      "Reconstruction loss: 270.68369325481984, KL divergence: 0.300429547219141\n",
      "Reconstruction loss: 269.0535782783069, KL divergence: 0.10058834415451928\n",
      "Reconstruction loss: 206.53347503469087, KL divergence: 0.19326547381011705\n",
      "Reconstruction loss: 234.45597015702145, KL divergence: 0.11321234203184549\n",
      "Reconstruction loss: 260.54805198684664, KL divergence: 0.047161550822575116\n",
      "Reconstruction loss: 168.64891116897522, KL divergence: 0.16566307306249628\n",
      "Reconstruction loss: 250.622703472021, KL divergence: 0.9439689195996067\n",
      "Reconstruction loss: 207.25634256813103, KL divergence: 0.08637234099148411\n",
      "Reconstruction loss: 202.90725843995318, KL divergence: 0.025482614015506888\n",
      "Reconstruction loss: 155.62640333499525, KL divergence: 0.3055929769255658\n",
      "Reconstruction loss: 214.83590190147214, KL divergence: 0.3898011146181964\n",
      "Reconstruction loss: 264.9751016983468, KL divergence: 0.04606632178526676\n",
      "Reconstruction loss: 200.02065908088116, KL divergence: 0.060459085722298855\n",
      "Reconstruction loss: 198.44015039196785, KL divergence: 0.1330417910567816\n",
      "Reconstruction loss: 228.55047912948135, KL divergence: 0.020410527330660733\n",
      "Reconstruction loss: 310.2418713335883, KL divergence: 0.24163090418359973\n",
      "Reconstruction loss: 186.86461521531032, KL divergence: 0.23954529448489692\n",
      "Reconstruction loss: 216.39492958911612, KL divergence: 0.09999583069755008\n",
      "Reconstruction loss: 200.700168709188, KL divergence: 6.757894691477438\n",
      "Reconstruction loss: 238.4637636439275, KL divergence: 4.639741292224326\n",
      "Reconstruction loss: 137.69302911438777, KL divergence: 0.386892725346472\n",
      "Reconstruction loss: 173.56110969737733, KL divergence: 0.1330417910567816\n",
      "Reconstruction loss: 271.408373962639, KL divergence: 0.07883435161062369\n",
      "Reconstruction loss: 241.59182832802819, KL divergence: 0.03655725276761751\n",
      "Reconstruction loss: 221.12857353173112, KL divergence: 0.020888739827755853\n",
      "Reconstruction loss: 232.3677198837451, KL divergence: 0.8410815489486501\n",
      "Reconstruction loss: 175.15523406436233, KL divergence: 0.04721704711443492\n",
      "Reconstruction loss: 274.8476554591098, KL divergence: 2.0932411081520117\n",
      "Reconstruction loss: 198.93616546172433, KL divergence: 1.487768250988741\n",
      "Reconstruction loss: 268.9424084651821, KL divergence: 0.038661840725775776\n",
      "Reconstruction loss: 226.1421736736582, KL divergence: 0.836199967111671\n",
      "Reconstruction loss: 224.59825077480454, KL divergence: 0.3009495187983216\n",
      "Reconstruction loss: 191.27094967909443, KL divergence: 0.02379914928235005\n",
      "Reconstruction loss: 195.38215452370835, KL divergence: 0.08591660439471271\n",
      "Reconstruction loss: 223.6861312513696, KL divergence: 2.2036191735615493\n",
      "Reconstruction loss: 258.63875141685867, KL divergence: 0.15902181931328718\n",
      "Reconstruction loss: 406.07076064627904, KL divergence: 1.6523108925435266\n",
      "Reconstruction loss: 225.36027265439512, KL divergence: 0.1737140554532312\n",
      "Reconstruction loss: 207.21986688341556, KL divergence: 0.13600880813848676\n",
      "Reconstruction loss: 242.7362711979298, KL divergence: 0.08587495957789831\n",
      "Reconstruction loss: 218.30457863818225, KL divergence: 0.0330648333205093\n",
      "Reconstruction loss: 237.92245844761948, KL divergence: 5.1855382886301955\n",
      "Reconstruction loss: 305.5118292328261, KL divergence: 0.27057006621430335\n",
      "Reconstruction loss: 126.76342008848138, KL divergence: 0.34316591543527775\n",
      "Reconstruction loss: 178.32775751100047, KL divergence: 0.12911512954204812\n",
      "Reconstruction loss: 260.93182488598256, KL divergence: 0.2570429129167029\n",
      "Reconstruction loss: 177.9111974095183, KL divergence: 0.08342332021475529\n",
      "Reconstruction loss: 215.53190982949008, KL divergence: 0.13600880813848676\n",
      "Reconstruction loss: 165.34706436802102, KL divergence: 0.21362490932787565\n",
      "Reconstruction loss: 314.6328004414984, KL divergence: 3.810292444138266\n",
      "Reconstruction loss: 161.9310225721286, KL divergence: 0.13716776067704228\n",
      "Reconstruction loss: 213.6815719837989, KL divergence: 0.05588334154965435\n",
      "Reconstruction loss: 146.94423954813897, KL divergence: 0.13868908038196337\n",
      "Reconstruction loss: 196.4151527502484, KL divergence: 0.047478593081432374\n",
      "Reconstruction loss: 187.2465157392912, KL divergence: 0.0755590313427732\n",
      "Reconstruction loss: 194.12645702761557, KL divergence: 2.194319448881367\n",
      "Reconstruction loss: 164.4860431623116, KL divergence: 0.1279798907523903\n",
      "Reconstruction loss: 331.6784541664737, KL divergence: 3.2728655702331726\n",
      "Reconstruction loss: 126.34562574825313, KL divergence: 0.5833318320360835\n",
      "Reconstruction loss: 203.3839019939125, KL divergence: 0.13416017988691814\n",
      "Reconstruction loss: 202.91032984077538, KL divergence: 0.1442895966983042\n",
      "Reconstruction loss: 223.1896470002915, KL divergence: 0.06317997566241634\n",
      "Reconstruction loss: 214.58997960252756, KL divergence: 0.1956769841432603\n",
      "Reconstruction loss: 365.25969805237804, KL divergence: 0.4114554518827086\n",
      "Reconstruction loss: 214.93139038457954, KL divergence: 0.033398362827888384\n",
      "Reconstruction loss: 135.20541011134515, KL divergence: 0.5519759158014772\n",
      "Reconstruction loss: 192.71753250990844, KL divergence: 0.13805431356368525\n",
      "Reconstruction loss: 179.39878090735778, KL divergence: 0.04475020011347769\n",
      "Reconstruction loss: 188.48574716397582, KL divergence: 0.030966784963346428\n",
      "Reconstruction loss: 136.35024322134882, KL divergence: 0.31622403387160153\n",
      "Reconstruction loss: 155.09112456724193, KL divergence: 0.42633000403415444\n",
      "Reconstruction loss: 185.40190711680583, KL divergence: 0.2408858120905833\n",
      "Reconstruction loss: 279.69474513421903, KL divergence: 0.14038259921555835\n",
      "Reconstruction loss: 153.2141779161346, KL divergence: 0.03228048563785996\n",
      "Reconstruction loss: 154.54475658212095, KL divergence: 0.11514805243955317\n",
      "Reconstruction loss: 162.58135321887903, KL divergence: 0.187721117910005\n",
      "Reconstruction loss: 179.13198049336827, KL divergence: 0.2182694145797927\n",
      "Reconstruction loss: 224.69136785709122, KL divergence: 0.17483492943163692\n",
      "Reconstruction loss: 133.13345567854284, KL divergence: 0.18040878186558112\n",
      "Reconstruction loss: 169.57549262544512, KL divergence: 0.13805431356368525\n",
      "Reconstruction loss: 171.14363094041965, KL divergence: 0.13805431356368525\n",
      "Reconstruction loss: 376.08795980160426, KL divergence: 0.1603704856574828\n",
      "Reconstruction loss: 165.12372680392338, KL divergence: 0.07614190564215578\n",
      "Reconstruction loss: 231.68644377296883, KL divergence: 0.03792733251723773\n",
      "Reconstruction loss: 192.28939083473688, KL divergence: 0.136870562848748\n",
      "Reconstruction loss: 285.0356410483672, KL divergence: 0.34899612731430646\n",
      "Reconstruction loss: 205.85982831013646, KL divergence: 0.13805431356368525\n",
      "Reconstruction loss: 167.2768608686337, KL divergence: 0.02983488024685338\n",
      "Reconstruction loss: 258.85660287952624, KL divergence: 0.716693730509821\n",
      "Reconstruction loss: 152.88582218441186, KL divergence: 0.31815932446627876\n",
      "Reconstruction loss: 272.71973476674793, KL divergence: 0.14997840144182095\n",
      "Reconstruction loss: 136.4389824597124, KL divergence: 0.5233370985346539\n",
      "Reconstruction loss: 175.58772259400104, KL divergence: 0.1395814384286777\n",
      "Reconstruction loss: 241.71435253063817, KL divergence: 1.361262001022712\n",
      "Reconstruction loss: 219.05751230250286, KL divergence: 5.691684854323462\n",
      "Reconstruction loss: 128.47646946067772, KL divergence: 0.2765764493588434\n",
      "Reconstruction loss: 300.60291719365523, KL divergence: 0.0971571710972674\n",
      "Reconstruction loss: 149.43462052442368, KL divergence: 0.14645562133914086\n",
      "Reconstruction loss: 182.20798871874285, KL divergence: 0.139456233954247\n",
      "Reconstruction loss: 133.66465960922483, KL divergence: 0.23489040987183374\n",
      "Reconstruction loss: 177.10788231068966, KL divergence: 0.1395814384286777\n",
      "Reconstruction loss: 221.30168426908023, KL divergence: 0.1327390373216752\n",
      "Reconstruction loss: 186.27856419156893, KL divergence: 0.02328843892974214\n",
      "Reconstruction loss: 262.61971440761647, KL divergence: 0.08469240209866136\n",
      "Reconstruction loss: 241.6370471244331, KL divergence: 0.021382437481391015\n",
      "Reconstruction loss: 198.2613467834385, KL divergence: 0.18512108644254266\n",
      "Reconstruction loss: 294.2851386521823, KL divergence: 0.04008167253204664\n",
      "Reconstruction loss: 245.8528362405341, KL divergence: 0.15900532491253666\n",
      "Reconstruction loss: 190.0060749046465, KL divergence: 0.022979578250518662\n",
      "Reconstruction loss: 247.16318572166765, KL divergence: 0.27594748784167483\n",
      "Reconstruction loss: 285.9452452492814, KL divergence: 0.021933040775687696\n",
      "Reconstruction loss: 142.9428997438025, KL divergence: 0.4326053296208501\n",
      "Reconstruction loss: 223.37901958980248, KL divergence: 0.1193428189035326\n",
      "Reconstruction loss: 269.4169597685657, KL divergence: 0.8978358180622201\n",
      "Reconstruction loss: 269.9867438162434, KL divergence: 0.1394906552430265\n",
      "Reconstruction loss: 132.79435347243074, KL divergence: 0.4044935105813605\n",
      "Reconstruction loss: 198.23933467245502, KL divergence: 0.45893481118186485\n",
      "Reconstruction loss: 171.06462774951993, KL divergence: 0.060509930565055825\n",
      "Reconstruction loss: 197.79675857225573, KL divergence: 0.11270204168331399\n",
      "Reconstruction loss: 191.37532549773186, KL divergence: 0.03697596560753347\n",
      "Reconstruction loss: 180.12593921442567, KL divergence: 0.18070150290956005\n",
      "Reconstruction loss: 307.3957266557064, KL divergence: 0.06600861947233433\n",
      "Reconstruction loss: 207.56210169447846, KL divergence: 0.05593407624926389\n",
      "Reconstruction loss: 281.7132645826018, KL divergence: 1.1982476753921583\n",
      "Reconstruction loss: 190.9704893430388, KL divergence: 0.18507151383678178\n",
      "Reconstruction loss: 214.56027712637058, KL divergence: 0.4441978724627764\n",
      "Reconstruction loss: 215.24726867609743, KL divergence: 0.31411316946364326\n",
      "Reconstruction loss: 170.72799611867597, KL divergence: 0.14082151304086465\n",
      "Reconstruction loss: 202.1404377209855, KL divergence: 0.31028786052577156\n",
      "Reconstruction loss: 226.18262151131555, KL divergence: 0.6780533465154379\n",
      "Reconstruction loss: 162.53204722878252, KL divergence: 0.16006431811568667\n",
      "Reconstruction loss: 192.06912976574793, KL divergence: 0.022632522445458325\n",
      "Reconstruction loss: 203.7642914133915, KL divergence: 0.0370610327744344\n",
      "Reconstruction loss: 228.97123982752578, KL divergence: 0.13741030683804573\n",
      "Reconstruction loss: 205.9812526296671, KL divergence: 0.04791226762649431\n",
      "Reconstruction loss: 198.6128217447899, KL divergence: 0.02121448857654956\n",
      "Reconstruction loss: 118.5482411597348, KL divergence: 0.4990261593168955\n",
      "Reconstruction loss: 182.16422514189284, KL divergence: 0.03754919917895749\n",
      "Reconstruction loss: 206.81607427422665, KL divergence: 0.031003195054069344\n",
      "Reconstruction loss: 180.42310837106297, KL divergence: 0.024228949475910977\n",
      "Reconstruction loss: 166.70561805914957, KL divergence: 0.14082151304086465\n",
      "Reconstruction loss: 160.74307155576966, KL divergence: 0.07191876676080372\n",
      "Reconstruction loss: 245.41176764489813, KL divergence: 0.1638021728556464\n",
      "Reconstruction loss: 231.4940426219125, KL divergence: 1.4767720996065408\n",
      "Reconstruction loss: 167.13162185790148, KL divergence: 0.059130053785369896\n",
      "Reconstruction loss: 179.05085760273943, KL divergence: 0.023152265331026878\n",
      "Reconstruction loss: 158.53163913110583, KL divergence: 0.16454881822010942\n",
      "Reconstruction loss: 177.4335882611445, KL divergence: 0.17533438293266101\n",
      "Reconstruction loss: 209.46738142355824, KL divergence: 3.059880582705426\n",
      "Reconstruction loss: 218.39641216756962, KL divergence: 0.1634964401451005\n",
      "Reconstruction loss: 202.8408428203047, KL divergence: 0.02290167917140895\n",
      "Reconstruction loss: 288.59755708098544, KL divergence: 2.581353548324505\n",
      "Reconstruction loss: 256.4965372910946, KL divergence: 0.6574419863563093\n",
      "Reconstruction loss: 160.4162219769238, KL divergence: 0.021694105774013372\n",
      "Reconstruction loss: 222.36144531002486, KL divergence: 0.05506603473708127\n",
      "Reconstruction loss: 235.27481614205593, KL divergence: 1.4479067214300998\n",
      "Reconstruction loss: 206.3352447726042, KL divergence: 0.058629501697794406\n",
      "Reconstruction loss: 189.74327205251407, KL divergence: 0.19283937936049161\n",
      "Reconstruction loss: 157.3804082507533, KL divergence: 0.05083349768163936\n",
      "Reconstruction loss: 231.81169383394516, KL divergence: 0.14271898015032325\n",
      "Reconstruction loss: 210.72375520532128, KL divergence: 0.021655735563326006\n",
      "Reconstruction loss: 241.14885921258985, KL divergence: 1.8060966186928096\n",
      "Reconstruction loss: 217.21522781710456, KL divergence: 0.33160082682992165\n",
      "Reconstruction loss: 263.3071787299571, KL divergence: 0.6160474778472436\n",
      "Reconstruction loss: 174.3999887161536, KL divergence: 5.0827939132068485\n",
      "Reconstruction loss: 297.7570257631478, KL divergence: 0.8190319391636435\n",
      "Reconstruction loss: 205.01717524176985, KL divergence: 0.022295541226343252\n",
      "Reconstruction loss: 232.46792266641197, KL divergence: 0.021481472246029476\n",
      "Reconstruction loss: 116.98762214819534, KL divergence: 0.5325308490975933\n",
      "Reconstruction loss: 195.56047812213396, KL divergence: 2.1539730214709634\n",
      "Reconstruction loss: 207.44698193650544, KL divergence: 0.0369926337320553\n",
      "Reconstruction loss: 212.69914882957136, KL divergence: 0.16313047795646962\n",
      "Reconstruction loss: 190.53286486989606, KL divergence: 0.02525738605089023\n",
      "Reconstruction loss: 130.01186832315972, KL divergence: 0.382436749942168\n",
      "Reconstruction loss: 238.99668261433862, KL divergence: 0.06294362089087635\n",
      "Reconstruction loss: 267.9002444727954, KL divergence: 0.5688218062776569\n",
      "Reconstruction loss: 163.71985147085707, KL divergence: 0.022092402981672976\n",
      "Reconstruction loss: 244.92379634401223, KL divergence: 0.050514526823927985\n",
      "Reconstruction loss: 219.33725289142345, KL divergence: 0.31091191729484613\n",
      "Reconstruction loss: 263.6443888017729, KL divergence: 0.055483098684712406\n",
      "Reconstruction loss: 222.75183189217617, KL divergence: 0.03270494323559486\n",
      "Reconstruction loss: 304.8101179110331, KL divergence: 0.12528162277535493\n",
      "Reconstruction loss: 172.36072056351054, KL divergence: 0.5981570510118356\n",
      "Reconstruction loss: 162.66054748905427, KL divergence: 0.20190413753625286\n",
      "Reconstruction loss: 223.12786675348198, KL divergence: 0.027466645313269977\n",
      "Reconstruction loss: 181.52311095163566, KL divergence: 0.17795572719841174\n",
      "Reconstruction loss: 178.2845197268863, KL divergence: 0.09841121384344809\n",
      "Reconstruction loss: 157.31089420801595, KL divergence: 0.17754262384491276\n",
      "Reconstruction loss: 118.40254382912033, KL divergence: 0.4734944862915313\n",
      "Reconstruction loss: 163.03343689509313, KL divergence: 0.1979807574356265\n",
      "Reconstruction loss: 133.18982238682082, KL divergence: 0.1802958482906752\n",
      "Reconstruction loss: 191.16492261495972, KL divergence: 0.04412274268045674\n",
      "Reconstruction loss: 148.3212302914987, KL divergence: 0.2681100681355722\n",
      "Reconstruction loss: 207.65874318950063, KL divergence: 0.023284702247263755\n",
      "Reconstruction loss: 281.2010749906425, KL divergence: 0.03152163865100538\n",
      "Reconstruction loss: 263.73171984242794, KL divergence: 0.021775566100287302\n",
      "Reconstruction loss: 228.94143921973165, KL divergence: 1.3635305684814796\n",
      "Reconstruction loss: 168.58818437090423, KL divergence: 0.3507121769413666\n",
      "Reconstruction loss: 184.06916595796156, KL divergence: 0.2219578182238206\n",
      "Reconstruction loss: 213.7838642243213, KL divergence: 1.4713518223132995\n",
      "Reconstruction loss: 159.29847715004001, KL divergence: 0.09348336106483185\n",
      "Reconstruction loss: 288.14257391075694, KL divergence: 3.596549780561151\n",
      "Reconstruction loss: 204.41308740898288, KL divergence: 0.021965038700105743\n",
      "Reconstruction loss: 139.06220223633352, KL divergence: 0.21927997272080396\n",
      "Reconstruction loss: 197.0134998484619, KL divergence: 8.39452969887013\n",
      "Reconstruction loss: 174.95308721114128, KL divergence: 0.14564984995504004\n",
      "Reconstruction loss: 241.35224253673533, KL divergence: 0.3788004601036138\n",
      "Reconstruction loss: 206.94562632347396, KL divergence: 0.15073387200666682\n",
      "Reconstruction loss: 186.216285067914, KL divergence: 0.19502877811492908\n",
      "Reconstruction loss: 261.84194135479316, KL divergence: 1.157180943489992\n",
      "Reconstruction loss: 170.76556926121202, KL divergence: 0.14564984995504004\n",
      "Reconstruction loss: 177.0204614064661, KL divergence: 0.05108148664826412\n",
      "Reconstruction loss: 192.76584236795196, KL divergence: 0.06274120931005239\n",
      "Reconstruction loss: 217.5320470591552, KL divergence: 0.21470003915216596\n",
      "Reconstruction loss: 230.75481820975875, KL divergence: 6.124623417530674\n",
      "Reconstruction loss: 135.40232909766405, KL divergence: 0.7558173162364099\n",
      "Reconstruction loss: 267.3246143519411, KL divergence: 2.4744691319409062\n",
      "Reconstruction loss: 127.93731119629336, KL divergence: 0.522332558604796\n",
      "Reconstruction loss: 131.01463005182563, KL divergence: 0.4564464160360171\n",
      "Reconstruction loss: 224.1522121840642, KL divergence: 0.06001356032690652\n",
      "Reconstruction loss: 214.62404797835353, KL divergence: 0.018250239853674244\n",
      "Reconstruction loss: 125.96969628117742, KL divergence: 0.3804729097516292\n",
      "Reconstruction loss: 207.486718544731, KL divergence: 0.022865160106738502\n",
      "Reconstruction loss: 198.83328866047265, KL divergence: 0.14019785145354363\n",
      "Reconstruction loss: 177.25134031275678, KL divergence: 0.15659734244861018\n",
      "Reconstruction loss: 164.56209166292552, KL divergence: 0.14950832429180083\n",
      "Reconstruction loss: 253.80464731041087, KL divergence: 0.23802742218091294\n",
      "Reconstruction loss: 242.65020436494711, KL divergence: 0.16379093042065834\n",
      "Reconstruction loss: 193.97992044203704, KL divergence: 0.03139442783816604\n",
      "Reconstruction loss: 148.8413763184501, KL divergence: 0.5764841691778279\n",
      "Reconstruction loss: 176.2104027946127, KL divergence: 0.022685775219609017\n",
      "Reconstruction loss: 160.02594272540188, KL divergence: 0.11976527949889137\n",
      "Reconstruction loss: 210.6840424831257, KL divergence: 0.05099279425442649\n",
      "Reconstruction loss: 200.10502758341477, KL divergence: 0.13690938694917115\n",
      "Reconstruction loss: 256.64683012066337, KL divergence: 3.069874359898324\n",
      "Reconstruction loss: 271.56190849792256, KL divergence: 0.7934607105115652\n",
      "Reconstruction loss: 201.81886162404248, KL divergence: 0.18072181002836968\n",
      "Reconstruction loss: 161.8673098428504, KL divergence: 0.15266473480634402\n",
      "Reconstruction loss: 204.40500275118657, KL divergence: 0.13401942573973252\n",
      "Reconstruction loss: 202.44485848678505, KL divergence: 0.2501727354353468\n",
      "Reconstruction loss: 196.10560583084742, KL divergence: 0.024856706277531004\n",
      "Reconstruction loss: 218.1274181334917, KL divergence: 0.16297352863271386\n",
      "Reconstruction loss: 244.96109050269615, KL divergence: 0.04435141835590323\n",
      "Reconstruction loss: 236.36849960155263, KL divergence: 0.02407755270645251\n",
      "Reconstruction loss: 262.7628234196934, KL divergence: 0.14414920560056854\n",
      "Reconstruction loss: 161.8137821100001, KL divergence: 0.2711872265156853\n",
      "Reconstruction loss: 207.3670819647614, KL divergence: 0.04740746974556864\n",
      "Reconstruction loss: 182.22486105884389, KL divergence: 0.12128196285730009\n",
      "Reconstruction loss: 136.4877951963098, KL divergence: 0.24177906272922456\n",
      "Reconstruction loss: 175.81488594557572, KL divergence: 0.030260476147670634\n",
      "Reconstruction loss: 238.62041254869692, KL divergence: 6.579493748918097\n",
      "Reconstruction loss: 151.0967341702915, KL divergence: 0.5369303384957618\n",
      "Reconstruction loss: 195.78503463684294, KL divergence: 0.14071322516083312\n",
      "Reconstruction loss: 231.87584563293711, KL divergence: 1.1972777684050806\n",
      "Reconstruction loss: 161.16994196100575, KL divergence: 0.10429031588163185\n",
      "Reconstruction loss: 273.7713062362939, KL divergence: 0.32911994479504114\n",
      "Reconstruction loss: 244.06269515051207, KL divergence: 0.024011611009980338\n",
      "Reconstruction loss: 247.72710501276958, KL divergence: 0.4544999799429053\n",
      "Reconstruction loss: 192.12466393663533, KL divergence: 0.4545245887224281\n",
      "Reconstruction loss: 142.7606727969163, KL divergence: 0.3101524305079601\n",
      "Reconstruction loss: 184.00895795499335, KL divergence: 4.979930657217595\n",
      "Reconstruction loss: 138.06816452483167, KL divergence: 1.0632953462891566\n",
      "Reconstruction loss: 220.23480849179163, KL divergence: 0.027091351307752976\n",
      "Reconstruction loss: 152.64349223466178, KL divergence: 0.26606113128017145\n",
      "Reconstruction loss: 229.1795722547576, KL divergence: 0.10380401300424813\n",
      "Reconstruction loss: 166.24455395861452, KL divergence: 0.5279385902746101\n",
      "Reconstruction loss: 139.64122328419856, KL divergence: 1.0702656637326027\n",
      "Reconstruction loss: 203.74832417606981, KL divergence: 0.15306152898859532\n",
      "Reconstruction loss: 185.814301083802, KL divergence: 0.20560153023955546\n",
      "Reconstruction loss: 261.1740763878586, KL divergence: 0.1513848120723964\n",
      "Reconstruction loss: 116.31919815888531, KL divergence: 0.4769854796886698\n",
      "Reconstruction loss: 259.79191779947763, KL divergence: 0.03489300940908924\n",
      "Reconstruction loss: 186.340302209385, KL divergence: 0.6785523576430411\n",
      "Reconstruction loss: 147.2105219550599, KL divergence: 0.2994859907711104\n",
      "Reconstruction loss: 171.49700932804, KL divergence: 0.2509997638817003\n",
      "Reconstruction loss: 142.71674498658098, KL divergence: 0.8211714484063262\n",
      "Reconstruction loss: 151.81075693477337, KL divergence: 0.2702982627378888\n",
      "Reconstruction loss: 246.12344390692652, KL divergence: 0.462016504811848\n",
      "Reconstruction loss: 104.14755315722105, KL divergence: 1.1269989181726507\n",
      "Reconstruction loss: 158.26412565697456, KL divergence: 0.6232068429711517\n",
      "Reconstruction loss: 278.9312986652978, KL divergence: 1.7339179973172767\n",
      "Reconstruction loss: 259.1692277044501, KL divergence: 0.033624603169573775\n",
      "Reconstruction loss: 191.50942856920244, KL divergence: 0.14927281559294153\n",
      "Reconstruction loss: 170.04540766550292, KL divergence: 0.08251263607048204\n",
      "Reconstruction loss: 181.94521291361366, KL divergence: 0.0941483005023005\n",
      "Reconstruction loss: 212.44509634294184, KL divergence: 0.08940314087227919\n",
      "Reconstruction loss: 128.89899693921583, KL divergence: 0.6315995416400679\n",
      "Reconstruction loss: 235.76034805069875, KL divergence: 5.458302244112879\n",
      "Reconstruction loss: 233.8483747600881, KL divergence: 0.11501082209844044\n",
      "Reconstruction loss: 206.90422183785904, KL divergence: 0.3873074414934687\n",
      "Reconstruction loss: 266.1186713605547, KL divergence: 0.024153633325892876\n",
      "Reconstruction loss: 296.06479648408606, KL divergence: 3.7348053109552257\n",
      "Reconstruction loss: 215.9016142321857, KL divergence: 0.1647893403942609\n",
      "Reconstruction loss: 145.20087712110134, KL divergence: 0.7009558826872037\n",
      "Reconstruction loss: 380.6134691604701, KL divergence: 2.829870132335291\n",
      "Reconstruction loss: 174.74550989039534, KL divergence: 0.29949110133990237\n",
      "Reconstruction loss: 266.77305077374245, KL divergence: 0.44820597585598754\n",
      "Reconstruction loss: 250.52978530033104, KL divergence: 0.4291031134619699\n",
      "Reconstruction loss: 225.8268826795338, KL divergence: 0.4503878920143492\n",
      "Reconstruction loss: 243.30505502860876, KL divergence: 0.046807311612228175\n",
      "Reconstruction loss: 238.13256757110824, KL divergence: 2.564343240248462\n",
      "Reconstruction loss: 201.26906715880853, KL divergence: 0.029710059452954685\n",
      "Reconstruction loss: 319.77764807680927, KL divergence: 0.19154731814524545\n",
      "Reconstruction loss: 258.74462183748244, KL divergence: 0.06132963347010001\n",
      "Reconstruction loss: 208.92170295473903, KL divergence: 0.24981952330651597\n",
      "Reconstruction loss: 229.88619891985746, KL divergence: 0.2753702426650985\n",
      "Reconstruction loss: 174.0973036215025, KL divergence: 0.10999777159189461\n",
      "Reconstruction loss: 217.7707262116323, KL divergence: 0.34646497443633073\n",
      "Reconstruction loss: 202.3570434020677, KL divergence: 0.06525827943816381\n",
      "Reconstruction loss: 294.5396528041718, KL divergence: 1.264335539416506\n",
      "Reconstruction loss: 198.6803242409686, KL divergence: 0.4288479746177232\n",
      "Reconstruction loss: 292.121292599183, KL divergence: 0.2849390285734278\n",
      "Reconstruction loss: 178.1119119322093, KL divergence: 0.07368841713256225\n",
      "Reconstruction loss: 216.93452333744276, KL divergence: 0.023901944966644106\n",
      "Reconstruction loss: 205.7289205428277, KL divergence: 0.062457377640344314\n",
      "Reconstruction loss: 234.51368351050021, KL divergence: 0.17815102226762553\n",
      "Reconstruction loss: 257.98231409556865, KL divergence: 0.03367974017325642\n",
      "Reconstruction loss: 190.09908525610277, KL divergence: 0.22952214255843\n",
      "Reconstruction loss: 133.98392215007522, KL divergence: 0.7022440590214177\n",
      "Reconstruction loss: 164.46408393111568, KL divergence: 0.6508492053920669\n",
      "Reconstruction loss: 192.05230245644174, KL divergence: 0.4314756806580873\n",
      "Reconstruction loss: 264.21522990854504, KL divergence: 2.034421732251751\n",
      "Reconstruction loss: 183.14469683792828, KL divergence: 0.32003555148739093\n",
      "Reconstruction loss: 224.14168981972392, KL divergence: 0.03077782431656534\n",
      "Reconstruction loss: 135.40646642457833, KL divergence: 0.7471494864056301\n",
      "Reconstruction loss: 198.1833728621208, KL divergence: 0.189721972828229\n",
      "Reconstruction loss: 145.51248632192375, KL divergence: 0.5949472766933019\n",
      "Reconstruction loss: 209.33926898509895, KL divergence: 1.0601394990152038\n",
      "Reconstruction loss: 164.8593326355732, KL divergence: 0.910615975787102\n",
      "Reconstruction loss: 181.23253674030843, KL divergence: 0.18484887264702193\n",
      "Reconstruction loss: 193.2176373262211, KL divergence: 0.26759602696733054\n",
      "Reconstruction loss: 276.3832754147164, KL divergence: 1.3932308198295171\n",
      "Reconstruction loss: 134.71322840116738, KL divergence: 1.0838392320311812\n",
      "Reconstruction loss: 243.02496880055688, KL divergence: 0.48788202150832644\n",
      "Reconstruction loss: 187.91857913901185, KL divergence: 0.3612479794084386\n",
      "Reconstruction loss: 194.56706384483442, KL divergence: 0.5780584491296907\n",
      "Reconstruction loss: 200.8350552701735, KL divergence: 0.20226173261934416\n",
      "Reconstruction loss: 227.56003017908915, KL divergence: 1.743458559718163\n",
      "Reconstruction loss: 248.0355412112968, KL divergence: 0.0315266927862381\n",
      "Reconstruction loss: 182.85275095689803, KL divergence: 0.6885579270883574\n",
      "Reconstruction loss: 167.00919229600592, KL divergence: 0.34538089483182044\n",
      "Reconstruction loss: 219.09526795143324, KL divergence: 0.42136468287510176\n",
      "Reconstruction loss: 174.07715293368832, KL divergence: 0.695435975418166\n",
      "Reconstruction loss: 221.55714255383236, KL divergence: 0.1004975195295808\n",
      "Reconstruction loss: 131.48019301266163, KL divergence: 1.0506702631492268\n",
      "Reconstruction loss: 224.89144128011793, KL divergence: 0.02437927110414051\n",
      "Reconstruction loss: 177.13381868029586, KL divergence: 0.170427744930081\n",
      "Reconstruction loss: 221.09048146660524, KL divergence: 4.771669298864726\n",
      "Reconstruction loss: 129.8367695014558, KL divergence: 0.3855674969669549\n",
      "Reconstruction loss: 266.4808710353952, KL divergence: 1.5448834163240133\n",
      "Reconstruction loss: 197.4465808660347, KL divergence: 0.07773210989738066\n",
      "Reconstruction loss: 283.1258802330752, KL divergence: 0.040476584968011375\n",
      "Reconstruction loss: 148.3888194603272, KL divergence: 0.6391382908765562\n",
      "Reconstruction loss: 160.7523688371701, KL divergence: 0.2504132997273792\n",
      "Reconstruction loss: 186.97055962237357, KL divergence: 0.16486136549257846\n",
      "Reconstruction loss: 175.21114519348876, KL divergence: 0.04851782302411245\n",
      "Reconstruction loss: 147.35763068062803, KL divergence: 0.2539919575318961\n",
      "Reconstruction loss: 189.85890953828542, KL divergence: 2.403732352923631\n",
      "Reconstruction loss: 207.39353801129647, KL divergence: 0.23660132616833252\n",
      "Reconstruction loss: 134.7595634789074, KL divergence: 0.5647402978841682\n",
      "Reconstruction loss: 146.91829409131628, KL divergence: 0.7929925691767055\n",
      "Reconstruction loss: 269.8293086063217, KL divergence: 0.06976104080164652\n",
      "Reconstruction loss: 192.12254752845465, KL divergence: 0.04621850867154459\n",
      "Reconstruction loss: 149.04320125904013, KL divergence: 0.48263848999734443\n",
      "Reconstruction loss: 157.82857387371303, KL divergence: 0.23996038796603586\n",
      "Reconstruction loss: 125.91558550033557, KL divergence: 0.7085708189928445\n",
      "Reconstruction loss: 135.66430112956795, KL divergence: 0.9287385382778311\n",
      "Reconstruction loss: 192.82731152611223, KL divergence: 0.36470878561267017\n",
      "Reconstruction loss: 240.79155013316782, KL divergence: 1.966308399870832\n",
      "Reconstruction loss: 206.31325867825078, KL divergence: 0.2932503201131079\n",
      "Reconstruction loss: 207.63429302283754, KL divergence: 0.02361332259883453\n",
      "Reconstruction loss: 191.98143685648617, KL divergence: 0.20719450272650658\n",
      "Reconstruction loss: 228.46086691287266, KL divergence: 0.6055417404042309\n",
      "Reconstruction loss: 117.3321737681027, KL divergence: 1.0452180531894575\n",
      "Reconstruction loss: 214.08543052010074, KL divergence: 0.23213407189370555\n",
      "Reconstruction loss: 158.6441894211186, KL divergence: 0.3670792539713449\n",
      "Reconstruction loss: 210.3540228613586, KL divergence: 0.41300673878753275\n",
      "Reconstruction loss: 118.48096710208043, KL divergence: 0.7959793316647406\n",
      "Reconstruction loss: 191.67634161039473, KL divergence: 0.040778378531155846\n",
      "Reconstruction loss: 236.48792394218054, KL divergence: 0.40508455934965487\n",
      "Reconstruction loss: 263.85993930300924, KL divergence: 0.2066256226431909\n",
      "Reconstruction loss: 312.91572634139953, KL divergence: 1.4242493286853328\n",
      "Reconstruction loss: 185.31098897937, KL divergence: 0.2691948187463024\n",
      "Reconstruction loss: 261.58834580940277, KL divergence: 0.04230887081163176\n",
      "Reconstruction loss: 249.31700844271973, KL divergence: 5.574151857697043\n",
      "Reconstruction loss: 172.98828221156862, KL divergence: 0.10061958707142793\n",
      "Reconstruction loss: 164.49849387503605, KL divergence: 0.5250601665195855\n",
      "Reconstruction loss: 200.07911260239257, KL divergence: 0.047760126404142456\n",
      "Reconstruction loss: 226.150245026658, KL divergence: 0.04663254211338613\n",
      "Reconstruction loss: 154.07133689380044, KL divergence: 0.3757183901486365\n",
      "Reconstruction loss: 253.70611722336238, KL divergence: 0.05258196352993938\n",
      "Reconstruction loss: 176.50473595559504, KL divergence: 0.5825889048208039\n",
      "Reconstruction loss: 218.46725845920398, KL divergence: 10.04536597282368\n",
      "Reconstruction loss: 120.55725986141302, KL divergence: 0.515222852802891\n",
      "Reconstruction loss: 203.40561052895475, KL divergence: 0.029504812553691362\n",
      "Reconstruction loss: 176.46264983067243, KL divergence: 0.18541494595890928\n",
      "Reconstruction loss: 223.7234147874036, KL divergence: 4.73783753208372\n",
      "Reconstruction loss: 279.54426963883566, KL divergence: 0.23267643603409843\n",
      "Reconstruction loss: 193.7074714683998, KL divergence: 0.512907800773188\n",
      "Reconstruction loss: 303.67012878786477, KL divergence: 0.02525407436436261\n",
      "Reconstruction loss: 198.61465010242694, KL divergence: 0.2845299718756488\n",
      "Reconstruction loss: 178.02524239285668, KL divergence: 0.17147637855756231\n",
      "Reconstruction loss: 168.60622610752068, KL divergence: 0.03129238011811969\n",
      "Reconstruction loss: 229.34291018773098, KL divergence: 0.16507985608761994\n",
      "Reconstruction loss: 230.50050683595538, KL divergence: 0.4130545475478288\n",
      "Reconstruction loss: 227.89236031191942, KL divergence: 0.06140376634366124\n",
      "Reconstruction loss: 213.58678405779332, KL divergence: 1.132975003581887\n",
      "Reconstruction loss: 173.81507830625253, KL divergence: 0.2672815165907568\n",
      "Reconstruction loss: 278.76615351106875, KL divergence: 1.0725089036729831\n",
      "Reconstruction loss: 273.8953386037497, KL divergence: 0.03725799985295575\n",
      "Reconstruction loss: 250.00707706620358, KL divergence: 1.4868000576746474\n",
      "Reconstruction loss: 226.96698537625753, KL divergence: 4.507035429286396\n",
      "Reconstruction loss: 259.6790568996792, KL divergence: 0.19663614370468419\n",
      "Reconstruction loss: 231.2339970082192, KL divergence: 0.17326025105603304\n",
      "Reconstruction loss: 272.0585652639541, KL divergence: 3.4037793615089047\n",
      "Reconstruction loss: 169.06554681440926, KL divergence: 0.11893911434963184\n",
      "Reconstruction loss: 262.1927728511056, KL divergence: 0.37165700343502817\n",
      "Reconstruction loss: 199.58113529473263, KL divergence: 0.11815138846262335\n",
      "Reconstruction loss: 267.7638201336068, KL divergence: 0.13446497754975228\n",
      "Reconstruction loss: 148.60561002493253, KL divergence: 0.12219065861287176\n",
      "Reconstruction loss: 197.26952897932853, KL divergence: 0.235608460853314\n",
      "Reconstruction loss: 273.15406261442024, KL divergence: 0.1389767401381226\n",
      "Reconstruction loss: 182.91296961352668, KL divergence: 0.07042443638610552\n",
      "Reconstruction loss: 177.14705076999826, KL divergence: 0.28398632595692225\n",
      "Reconstruction loss: 188.35348652512624, KL divergence: 0.2242742059672994\n",
      "Reconstruction loss: 165.77182344445134, KL divergence: 1.0627862422240488\n",
      "Reconstruction loss: 258.2191585884687, KL divergence: 0.6111229205412942\n",
      "Reconstruction loss: 248.3850449724082, KL divergence: 1.634620144826167\n",
      "Reconstruction loss: 230.36472998288045, KL divergence: 0.9120235661994384\n",
      "Reconstruction loss: 186.0297872929704, KL divergence: 0.17171562312494426\n",
      "Reconstruction loss: 168.16461525983192, KL divergence: 0.3205489443536194\n",
      "Reconstruction loss: 188.85264275330235, KL divergence: 0.06382953404100816\n",
      "Reconstruction loss: 263.17797516260333, KL divergence: 0.0980972710494391\n",
      "Reconstruction loss: 160.6723763688248, KL divergence: 0.1858920520114774\n",
      "Reconstruction loss: 208.44646169177315, KL divergence: 0.1766673109695569\n",
      "Reconstruction loss: 196.1461389843218, KL divergence: 0.06066859681181219\n",
      "Reconstruction loss: 198.26418929190612, KL divergence: 0.16462394740448444\n",
      "Reconstruction loss: 188.92147479755408, KL divergence: 0.45425190662022996\n",
      "Reconstruction loss: 160.9376480463498, KL divergence: 0.5630904379806341\n",
      "Reconstruction loss: 110.3347848351236, KL divergence: 0.8765758568305528\n",
      "Reconstruction loss: 230.278992787308, KL divergence: 0.9203901440422368\n",
      "Reconstruction loss: 155.12418344293246, KL divergence: 0.04625823328375006\n",
      "Reconstruction loss: 217.52870606985644, KL divergence: 0.03673519330377034\n",
      "Reconstruction loss: 290.3170979780909, KL divergence: 0.02556329772641991\n",
      "Reconstruction loss: 241.83529358344538, KL divergence: 5.897219492376817\n",
      "Reconstruction loss: 127.96706775719213, KL divergence: 0.4674923580717126\n",
      "Reconstruction loss: 217.69599085410954, KL divergence: 7.8992003255595105\n",
      "Reconstruction loss: 238.42483457644346, KL divergence: 0.025348009700456975\n",
      "Reconstruction loss: 176.97334297400852, KL divergence: 0.2630269583864107\n",
      "Reconstruction loss: 155.39127463428167, KL divergence: 0.1627449257800827\n",
      "Reconstruction loss: 185.47566627875966, KL divergence: 0.20441659697043618\n",
      "Reconstruction loss: 211.02415195608194, KL divergence: 0.5033107426006742\n",
      "Reconstruction loss: 179.76720890200323, KL divergence: 0.025905601945411916\n",
      "Reconstruction loss: 214.67886994541635, KL divergence: 0.023476144540633515\n",
      "Reconstruction loss: 238.16623135696705, KL divergence: 0.1389122873873248\n",
      "Reconstruction loss: 176.2076459896515, KL divergence: 0.1627449257800827\n",
      "Reconstruction loss: 184.8744572566697, KL divergence: 0.17794869780525796\n",
      "Reconstruction loss: 214.49683024159205, KL divergence: 0.18493786610958868\n",
      "Reconstruction loss: 271.0992630528927, KL divergence: 0.2529494399605706\n",
      "Reconstruction loss: 241.78386256648486, KL divergence: 0.41684372310829515\n",
      "Reconstruction loss: 219.93161558884128, KL divergence: 1.0719299988259343\n",
      "Reconstruction loss: 193.63719585482883, KL divergence: 0.03387889346450612\n",
      "Reconstruction loss: 155.26266919481142, KL divergence: 0.38307563074814416\n",
      "Reconstruction loss: 238.34549248369106, KL divergence: 0.024680238912645536\n",
      "Reconstruction loss: 230.52022962197788, KL divergence: 0.0849721894216331\n",
      "Reconstruction loss: 154.77092268628945, KL divergence: 0.5025611873857456\n",
      "Reconstruction loss: 201.92489577581637, KL divergence: 0.07899052556107061\n",
      "Reconstruction loss: 204.0384001300483, KL divergence: 0.06107038948454507\n",
      "Reconstruction loss: 128.92933698705713, KL divergence: 0.38265161479727766\n",
      "Reconstruction loss: 118.35855559734382, KL divergence: 0.9752419180021488\n",
      "Reconstruction loss: 167.57694607443375, KL divergence: 0.2779872524278582\n",
      "Reconstruction loss: 125.52526499834312, KL divergence: 0.5311178330530848\n",
      "Reconstruction loss: 122.04531153327444, KL divergence: 1.035196878626318\n",
      "Reconstruction loss: 145.01377368743803, KL divergence: 0.4945596728142246\n",
      "Reconstruction loss: 224.68215254357415, KL divergence: 0.03178390965866912\n",
      "Reconstruction loss: 269.8339264595703, KL divergence: 0.03554009233717209\n",
      "Reconstruction loss: 196.21027981304582, KL divergence: 0.25831364690263775\n",
      "Reconstruction loss: 241.45599213659835, KL divergence: 0.026373967690165978\n",
      "Reconstruction loss: 165.13552230938961, KL divergence: 0.06047559687507226\n",
      "Reconstruction loss: 186.64753956334573, KL divergence: 0.14248187649927396\n",
      "Reconstruction loss: 217.99849717794902, KL divergence: 0.04404476310412808\n",
      "Reconstruction loss: 169.27682511819356, KL divergence: 0.11270983078227631\n",
      "Reconstruction loss: 242.28321965993595, KL divergence: 0.4497220390005702\n",
      "Reconstruction loss: 226.1315406186024, KL divergence: 0.04342759856982814\n",
      "Reconstruction loss: 219.30437681501067, KL divergence: 0.07644240402516461\n",
      "Reconstruction loss: 122.78082102655458, KL divergence: 0.8778670508032257\n",
      "Reconstruction loss: 183.22259261934812, KL divergence: 0.05218463140343488\n",
      "Reconstruction loss: 222.47538213512155, KL divergence: 1.499925348363118\n",
      "Reconstruction loss: 167.96637793272174, KL divergence: 0.3049792325267888\n",
      "Reconstruction loss: 257.1345808698163, KL divergence: 0.7361584289579634\n",
      "Reconstruction loss: 192.23448980669107, KL divergence: 0.10049462960313194\n",
      "Reconstruction loss: 222.35206955997185, KL divergence: 0.5932725082487567\n",
      "Reconstruction loss: 254.52214310728644, KL divergence: 0.030531618073050548\n",
      "Reconstruction loss: 181.60818272187663, KL divergence: 0.2156854549707079\n",
      "Reconstruction loss: 199.4602935456183, KL divergence: 0.1285283982323342\n",
      "Reconstruction loss: 183.6598149105087, KL divergence: 0.04317896540113819\n",
      "Reconstruction loss: 162.17346496076627, KL divergence: 0.4349046889991382\n",
      "Reconstruction loss: 180.20875186647805, KL divergence: 0.16433628259500993\n",
      "Reconstruction loss: 141.93995168405917, KL divergence: 0.2634650577019473\n",
      "Reconstruction loss: 219.7968458320051, KL divergence: 0.29807142472787784\n",
      "Reconstruction loss: 195.4654010781563, KL divergence: 0.06114682341083483\n",
      "Reconstruction loss: 215.78695618119013, KL divergence: 0.02839855083009102\n",
      "Reconstruction loss: 182.23913003889118, KL divergence: 0.16433628259500993\n",
      "Reconstruction loss: 232.54778201712688, KL divergence: 5.002338333409718\n",
      "Reconstruction loss: 139.01626852817964, KL divergence: 0.3265915931477295\n",
      "Reconstruction loss: 216.85729111102398, KL divergence: 0.16567078373605693\n",
      "Reconstruction loss: 245.5462039916291, KL divergence: 0.48288199715311125\n",
      "Reconstruction loss: 196.94846877416694, KL divergence: 0.07140477770584391\n",
      "Reconstruction loss: 277.0690975457168, KL divergence: 0.0267272132384343\n",
      "Reconstruction loss: 178.32562504093949, KL divergence: 0.40286807807049324\n",
      "Reconstruction loss: 236.38684250604172, KL divergence: 0.04937938793547003\n",
      "Reconstruction loss: 124.14735724798975, KL divergence: 0.2988085943829525\n",
      "Reconstruction loss: 194.89534020695754, KL divergence: 3.163676704495358\n",
      "Reconstruction loss: 247.0341675031805, KL divergence: 7.655993448047335\n",
      "Reconstruction loss: 262.4055926960889, KL divergence: 3.6787788455527717\n",
      "Reconstruction loss: 171.37629582839054, KL divergence: 0.06286442798062464\n",
      "Reconstruction loss: 199.727831753494, KL divergence: 0.04925170266970902\n",
      "Reconstruction loss: 212.5873300633026, KL divergence: 0.025954144946491398\n",
      "Reconstruction loss: 250.574672745664, KL divergence: 0.5437898134946871\n",
      "Reconstruction loss: 291.49324072204763, KL divergence: 0.1572973955976426\n",
      "Reconstruction loss: 170.26982281045736, KL divergence: 0.2154508653882306\n",
      "Reconstruction loss: 191.15797018340345, KL divergence: 5.989185484834533\n",
      "Reconstruction loss: 145.42553329613213, KL divergence: 0.21029602523430058\n",
      "Reconstruction loss: 202.56799000539525, KL divergence: 0.11811779678934442\n",
      "Reconstruction loss: 173.4187985861451, KL divergence: 0.16567078373605693\n",
      "Reconstruction loss: 244.59945658275353, KL divergence: 0.18654421261626603\n",
      "Reconstruction loss: 283.91349611815116, KL divergence: 0.07412706238122696\n",
      "Reconstruction loss: 216.14170156791033, KL divergence: 0.02935280054296263\n",
      "Reconstruction loss: 204.3143743011022, KL divergence: 0.17226581657729356\n",
      "Reconstruction loss: 169.03591414453348, KL divergence: 0.16567078373605693\n",
      "Reconstruction loss: 159.46689858768204, KL divergence: 0.2946125566830937\n",
      "Reconstruction loss: 218.84743091625376, KL divergence: 0.07854002582431557\n",
      "Reconstruction loss: 141.8362467287526, KL divergence: 0.9290082349351084\n",
      "Reconstruction loss: 163.93258344981356, KL divergence: 0.07070633862473424\n",
      "Reconstruction loss: 166.67702197696667, KL divergence: 0.24144134048124288\n",
      "Reconstruction loss: 232.4696092850467, KL divergence: 0.07370466206927723\n",
      "Reconstruction loss: 216.18374285764327, KL divergence: 0.1575866522045304\n",
      "Reconstruction loss: 266.0770189025229, KL divergence: 0.2569325373575613\n",
      "Reconstruction loss: 241.16298939319248, KL divergence: 0.043471815937715486\n",
      "Reconstruction loss: 235.97448812718716, KL divergence: 0.7284623437675086\n",
      "Reconstruction loss: 167.6748708125087, KL divergence: 0.09616859142320783\n",
      "Reconstruction loss: 211.66968226617573, KL divergence: 0.037618421011927916\n",
      "Reconstruction loss: 221.47635103792186, KL divergence: 3.0713046982566206\n",
      "Reconstruction loss: 172.34779978664716, KL divergence: 0.07453062564244556\n",
      "Reconstruction loss: 241.4538460849785, KL divergence: 0.08049831123518841\n",
      "Reconstruction loss: 112.12670961423851, KL divergence: 1.2531914705645164\n",
      "Reconstruction loss: 185.91746093197722, KL divergence: 0.09777140052576161\n",
      "Reconstruction loss: 173.72880297927475, KL divergence: 9.883647293248295\n",
      "Reconstruction loss: 308.112083571329, KL divergence: 0.15453471720542988\n",
      "Reconstruction loss: 225.6762701938275, KL divergence: 0.16727188479736266\n",
      "Reconstruction loss: 189.42737910650348, KL divergence: 0.06906851750865917\n",
      "Reconstruction loss: 249.76439628584853, KL divergence: 0.3045141401155855\n",
      "Reconstruction loss: 170.29045426183086, KL divergence: 0.2111518471682834\n",
      "Reconstruction loss: 257.64592260241295, KL divergence: 0.02924438895416015\n",
      "Reconstruction loss: 136.3224071351748, KL divergence: 0.24593086366954042\n",
      "Reconstruction loss: 289.38801240938596, KL divergence: 0.0623115933761349\n",
      "Reconstruction loss: 202.39364622528137, KL divergence: 0.01941156108262232\n",
      "Reconstruction loss: 238.31891119650857, KL divergence: 0.9092400190595818\n",
      "Reconstruction loss: 211.44740651186564, KL divergence: 0.03797257824928557\n",
      "Reconstruction loss: 217.72009043180333, KL divergence: 0.08094753765414997\n",
      "Reconstruction loss: 245.33509612933142, KL divergence: 0.03225369641692194\n",
      "Reconstruction loss: 199.85015008062518, KL divergence: 0.02808164969218291\n",
      "Reconstruction loss: 239.46080602479685, KL divergence: 0.10778355029757591\n",
      "Reconstruction loss: 367.3024620679472, KL divergence: 0.7847914635473958\n",
      "Reconstruction loss: 127.55083958163038, KL divergence: 1.0123843012169356\n",
      "Reconstruction loss: 233.8611394570696, KL divergence: 0.05342179289944815\n",
      "Reconstruction loss: 242.2966130949668, KL divergence: 1.2290454757075107\n",
      "Reconstruction loss: 169.6545277419234, KL divergence: 0.22577190184891355\n",
      "Reconstruction loss: 236.37573250346492, KL divergence: 0.6459451160110848\n",
      "Reconstruction loss: 193.36250290679294, KL divergence: 0.05000675674848576\n",
      "Reconstruction loss: 239.7429604116565, KL divergence: 0.15805965435991476\n",
      "Reconstruction loss: 200.10773450222518, KL divergence: 0.34315706437205784\n",
      "Reconstruction loss: 199.73062430218178, KL divergence: 0.16983459380315058\n",
      "Reconstruction loss: 245.6913245034508, KL divergence: 0.03685985903675282\n",
      "Reconstruction loss: 250.63923233517508, KL divergence: 0.059568303095612785\n",
      "Reconstruction loss: 322.4026164854223, KL divergence: 0.8562218791343403\n",
      "Reconstruction loss: 182.57781650661585, KL divergence: 0.16983459380315058\n",
      "Reconstruction loss: 412.9874842445396, KL divergence: 0.03944984682492425\n",
      "Reconstruction loss: 182.63335264012593, KL divergence: 4.142279573954222\n",
      "Reconstruction loss: 125.07298034473091, KL divergence: 0.7894789036271403\n",
      "Reconstruction loss: 199.02362463463078, KL divergence: 0.09609688810501754\n",
      "Reconstruction loss: 192.4323467572135, KL divergence: 0.1107400676797895\n",
      "Reconstruction loss: 207.5446137238424, KL divergence: 0.02877968090825389\n",
      "Reconstruction loss: 217.03983574789027, KL divergence: 0.032789916137212494\n",
      "Reconstruction loss: 178.7197765005928, KL divergence: 2.4257238655976088\n",
      "Reconstruction loss: 156.057978227717, KL divergence: 0.22797054813197676\n",
      "Reconstruction loss: 120.61086500398292, KL divergence: 0.683338471933984\n",
      "Reconstruction loss: 277.33545487760404, KL divergence: 0.5989885918770794\n",
      "Reconstruction loss: 139.08546775845957, KL divergence: 0.16983459380315058\n",
      "Reconstruction loss: 223.0245774414343, KL divergence: 3.70765162382975\n",
      "Reconstruction loss: 211.92362484858762, KL divergence: 0.5697388109640477\n",
      "Reconstruction loss: 154.63523657529072, KL divergence: 0.5368415563859988\n",
      "Reconstruction loss: 351.2151165175868, KL divergence: 1.4143834136343716\n",
      "Reconstruction loss: 161.08440314480544, KL divergence: 0.16983459380315058\n",
      "Reconstruction loss: 176.45372847747979, KL divergence: 0.3205984796132546\n",
      "Reconstruction loss: 165.3655715026281, KL divergence: 0.4070731950126595\n",
      "Reconstruction loss: 229.84949652879678, KL divergence: 1.0739930691037185\n",
      "Reconstruction loss: 179.16066699508542, KL divergence: 0.42736658923092197\n",
      "Reconstruction loss: 276.6097018515001, KL divergence: 0.13263821409435866\n",
      "Reconstruction loss: 231.08733894727226, KL divergence: 2.2931860951348204\n",
      "Reconstruction loss: 164.7582493637384, KL divergence: 0.1726511952513231\n",
      "Reconstruction loss: 132.45730681713655, KL divergence: 0.6261486254924942\n",
      "Reconstruction loss: 123.35915008569305, KL divergence: 0.6964231437002693\n",
      "Reconstruction loss: 258.74641211286036, KL divergence: 0.24703614173474808\n",
      "Reconstruction loss: 283.75305951795167, KL divergence: 0.40407821634920416\n",
      "Reconstruction loss: 169.03898577649463, KL divergence: 5.591009621704654\n",
      "Reconstruction loss: 172.47217240667098, KL divergence: 0.16094271478341515\n",
      "Reconstruction loss: 231.4921418404487, KL divergence: 0.03228894988081038\n",
      "Reconstruction loss: 256.45951749633036, KL divergence: 0.3541437475019571\n",
      "Reconstruction loss: 273.81005346036125, KL divergence: 0.9891877925693636\n",
      "Reconstruction loss: 193.55130363563674, KL divergence: 0.4266836858964872\n",
      "Reconstruction loss: 202.4866322689515, KL divergence: 0.05850932243807533\n",
      "Reconstruction loss: 138.16754416169036, KL divergence: 0.5510653144093296\n",
      "Reconstruction loss: 174.2616609440794, KL divergence: 0.37391442131647845\n",
      "Reconstruction loss: 207.1986297550124, KL divergence: 0.028627227064590755\n",
      "Reconstruction loss: 205.44228949355704, KL divergence: 0.09110338076837377\n",
      "Reconstruction loss: 199.58236544663885, KL divergence: 0.17071969671356507\n",
      "Reconstruction loss: 120.88939717865418, KL divergence: 0.8712320469806052\n",
      "Reconstruction loss: 127.97900286251577, KL divergence: 0.4637558931634502\n",
      "Reconstruction loss: 252.63392500701397, KL divergence: 4.0395018985941755\n",
      "Reconstruction loss: 182.6886538700588, KL divergence: 0.057928426812422995\n",
      "Reconstruction loss: 188.12201272007496, KL divergence: 0.17152015254434533\n",
      "Reconstruction loss: 239.56276459995382, KL divergence: 0.200291367461713\n",
      "Reconstruction loss: 273.46830987406594, KL divergence: 0.9685851654184634\n",
      "Reconstruction loss: 191.80922932796483, KL divergence: 1.993893413238582\n",
      "Reconstruction loss: 210.88956344313468, KL divergence: 0.0920315694184024\n",
      "Reconstruction loss: 230.36863245509886, KL divergence: 0.8453338418303926\n",
      "Reconstruction loss: 134.33331147423604, KL divergence: 0.3002228958375341\n",
      "Reconstruction loss: 189.49414168557482, KL divergence: 0.36363051866298424\n",
      "Reconstruction loss: 264.04036496639833, KL divergence: 0.1642478235781914\n",
      "Reconstruction loss: 326.3416350108598, KL divergence: 0.06474319659118671\n",
      "Reconstruction loss: 172.76947822450632, KL divergence: 0.17223180644728908\n",
      "Reconstruction loss: 216.2093908741602, KL divergence: 0.042612956082565046\n",
      "Reconstruction loss: 247.14372404307346, KL divergence: 0.09227551397166706\n",
      "Reconstruction loss: 156.5231868577533, KL divergence: 0.4668052579384293\n",
      "Reconstruction loss: 161.67874441560485, KL divergence: 0.27555325157602667\n",
      "Reconstruction loss: 174.98417739101504, KL divergence: 0.18108482643654672\n",
      "Reconstruction loss: 251.2907577185672, KL divergence: 0.04972997588936584\n",
      "Reconstruction loss: 308.0170029046239, KL divergence: 0.5150289455318007\n",
      "Reconstruction loss: 187.58950944799062, KL divergence: 0.17223180644728908\n",
      "Reconstruction loss: 195.31047457360987, KL divergence: 0.047103729297951036\n",
      "Reconstruction loss: 168.07085048892665, KL divergence: 0.2202087957480348\n",
      "Reconstruction loss: 259.2348094773205, KL divergence: 0.3276540856368811\n",
      "Reconstruction loss: 178.51617761121526, KL divergence: 0.12312030015330627\n",
      "Reconstruction loss: 301.99811683221645, KL divergence: 1.1166828998195997\n",
      "Reconstruction loss: 170.27634708043792, KL divergence: 0.3408763887517725\n",
      "Reconstruction loss: 153.2903984427257, KL divergence: 0.17223180644728908\n",
      "Reconstruction loss: 156.108364379945, KL divergence: 0.2979278956145014\n",
      "Reconstruction loss: 189.88902925838124, KL divergence: 5.4151655123347275\n",
      "Reconstruction loss: 186.93413240289425, KL divergence: 0.0702528859338244\n",
      "Reconstruction loss: 161.9062077129247, KL divergence: 0.17710267020542292\n",
      "Reconstruction loss: 125.66360407686703, KL divergence: 0.33277777463274516\n",
      "Reconstruction loss: 199.4489339066567, KL divergence: 0.18308356964216516\n",
      "Reconstruction loss: 140.96614211073904, KL divergence: 0.7116423388086367\n",
      "Reconstruction loss: 218.75681905965286, KL divergence: 0.22594827511673604\n",
      "Reconstruction loss: 224.1500132093823, KL divergence: 2.402611705791895\n",
      "Reconstruction loss: 309.906967065963, KL divergence: 0.16912104974975362\n",
      "Reconstruction loss: 250.34996143985995, KL divergence: 0.06041630527132297\n",
      "Reconstruction loss: 152.831068664589, KL divergence: 0.17223180644728908\n",
      "Reconstruction loss: 215.7753388744926, KL divergence: 0.03063530521521801\n",
      "Reconstruction loss: 148.6770958069065, KL divergence: 0.23709323470659777\n",
      "Reconstruction loss: 213.25855079535648, KL divergence: 0.23150465441816803\n",
      "Reconstruction loss: 215.07668245380773, KL divergence: 0.06185574084181328\n",
      "Reconstruction loss: 204.59713988210314, KL divergence: 0.284751360506906\n",
      "Reconstruction loss: 182.22405639157614, KL divergence: 0.14727362444495318\n",
      "Reconstruction loss: 237.0673416064103, KL divergence: 0.31348400793074005\n",
      "Reconstruction loss: 316.23528834305637, KL divergence: 3.903893031492691\n",
      "Reconstruction loss: 204.70289089662975, KL divergence: 0.2574814052692285\n",
      "Reconstruction loss: 165.15308674226483, KL divergence: 0.17272169069599913\n",
      "Reconstruction loss: 222.68581607618495, KL divergence: 1.2767059450670373\n",
      "Reconstruction loss: 176.73558600315909, KL divergence: 0.08428482297631934\n",
      "Reconstruction loss: 143.35794522876205, KL divergence: 0.3529643024686396\n",
      "Reconstruction loss: 299.14966529492887, KL divergence: 3.6614351112007553\n",
      "Reconstruction loss: 251.32567521435124, KL divergence: 1.575920076574556\n",
      "Reconstruction loss: 238.65750953839543, KL divergence: 0.1909808914753484\n",
      "Reconstruction loss: 121.48323533515901, KL divergence: 0.6059131396618436\n",
      "Reconstruction loss: 229.4164849546463, KL divergence: 0.6201274817333097\n",
      "Reconstruction loss: 249.17273373527053, KL divergence: 0.09003972037363939\n",
      "Reconstruction loss: 144.7351666697273, KL divergence: 0.17589308914038376\n",
      "Reconstruction loss: 192.29690882901062, KL divergence: 0.8473776343856462\n",
      "Reconstruction loss: 226.21163903120964, KL divergence: 0.07682986387322382\n",
      "Reconstruction loss: 277.3461923008558, KL divergence: 0.04028356524065918\n",
      "Reconstruction loss: 213.5003453300218, KL divergence: 0.17657212219283164\n",
      "Reconstruction loss: 282.2799236530083, KL divergence: 0.6845705677947587\n",
      "Reconstruction loss: 238.4941716438549, KL divergence: 0.09250601567692351\n",
      "Reconstruction loss: 166.15530567528043, KL divergence: 0.1276151819098767\n",
      "Reconstruction loss: 295.78320108929245, KL divergence: 0.1692946429759542\n",
      "Reconstruction loss: 153.33706004251977, KL divergence: 0.17272169069599913\n",
      "Reconstruction loss: 206.3289897222881, KL divergence: 0.042675155572330814\n",
      "Reconstruction loss: 245.03346438483828, KL divergence: 0.12397762078156122\n",
      "Reconstruction loss: 193.52010660805945, KL divergence: 0.07247946797239846\n",
      "Reconstruction loss: 253.72264668050832, KL divergence: 0.03077860755194506\n",
      "Reconstruction loss: 172.41858737827187, KL divergence: 0.17079646157247447\n",
      "Reconstruction loss: 207.55187222764243, KL divergence: 0.10516592250188811\n",
      "Reconstruction loss: 202.2409372970895, KL divergence: 0.7215170582756405\n",
      "Reconstruction loss: 213.59894732808885, KL divergence: 0.1986565165194849\n",
      "Reconstruction loss: 161.7117977606697, KL divergence: 0.39475842591576477\n",
      "Reconstruction loss: 199.79872729021497, KL divergence: 0.24514890247452809\n",
      "Reconstruction loss: 303.85025720674935, KL divergence: 2.879352821346849\n",
      "Reconstruction loss: 202.2927124611137, KL divergence: 0.04667166059531941\n",
      "Reconstruction loss: 166.66625716554364, KL divergence: 0.45099684096087206\n",
      "Reconstruction loss: 182.88074828177673, KL divergence: 0.12390211793218231\n",
      "Reconstruction loss: 233.97308591906955, KL divergence: 0.4898608592251409\n",
      "Reconstruction loss: 320.81785596939693, KL divergence: 0.25200990485872754\n",
      "Reconstruction loss: 146.4190961802164, KL divergence: 1.1161143646366725\n",
      "Reconstruction loss: 197.02518588030713, KL divergence: 0.25977459282501997\n",
      "Reconstruction loss: 195.37921599622837, KL divergence: 0.3265682916797334\n",
      "Reconstruction loss: 180.2001617876557, KL divergence: 0.17305022331417363\n",
      "Reconstruction loss: 205.55729870832346, KL divergence: 0.14847248593989137\n",
      "Reconstruction loss: 184.22706316468498, KL divergence: 0.17060958337648308\n",
      "Reconstruction loss: 200.94702695726465, KL divergence: 0.07212494270152392\n",
      "Reconstruction loss: 184.91370223875418, KL divergence: 0.13126004140700331\n",
      "Reconstruction loss: 160.99795690029174, KL divergence: 0.2692422551680661\n",
      "Reconstruction loss: 153.79083087978432, KL divergence: 0.22143495647254335\n",
      "Reconstruction loss: 183.13600951573278, KL divergence: 0.3361044352778846\n",
      "Reconstruction loss: 164.9250134159571, KL divergence: 0.39736391475710486\n",
      "Reconstruction loss: 176.0834118140021, KL divergence: 5.145749097026306\n",
      "Reconstruction loss: 277.83862870498496, KL divergence: 1.9045940919808013\n",
      "Reconstruction loss: 204.62745215247557, KL divergence: 0.7650011966667913\n",
      "Reconstruction loss: 233.96290707305124, KL divergence: 4.550308965500882\n",
      "Reconstruction loss: 267.5679208189154, KL divergence: 0.7239983058845645\n",
      "Reconstruction loss: 251.48713565594895, KL divergence: 0.495543014321804\n",
      "Reconstruction loss: 160.70549204943762, KL divergence: 0.22681786914263335\n",
      "Reconstruction loss: 183.54189146075953, KL divergence: 0.24131572668123025\n",
      "Reconstruction loss: 189.5930424654914, KL divergence: 0.2709645686542824\n",
      "Reconstruction loss: 259.44999136824094, KL divergence: 0.04292649670755311\n",
      "Reconstruction loss: 210.48036042316997, KL divergence: 0.409507085590091\n",
      "Reconstruction loss: 272.03822392354243, KL divergence: 0.8936857370480238\n",
      "Reconstruction loss: 199.54791967722534, KL divergence: 0.05338914452801863\n",
      "Reconstruction loss: 227.8319285023294, KL divergence: 0.03196173097566418\n",
      "Reconstruction loss: 228.3442117344049, KL divergence: 0.06622925027947385\n",
      "Reconstruction loss: 195.57015172528082, KL divergence: 1.8440342674080923\n",
      "Reconstruction loss: 156.5652373626428, KL divergence: 0.3574765435712993\n",
      "Reconstruction loss: 215.7082605299141, KL divergence: 0.5226966502789958\n",
      "Reconstruction loss: 182.74437240825358, KL divergence: 0.28824048669733104\n",
      "Reconstruction loss: 219.37793071940294, KL divergence: 0.2521727079118228\n",
      "Reconstruction loss: 231.69135678330184, KL divergence: 1.9973921498835454\n",
      "Reconstruction loss: 166.78649772278402, KL divergence: 0.0898667364933654\n",
      "Reconstruction loss: 220.46019123411185, KL divergence: 0.7383667575149548\n",
      "Reconstruction loss: 251.45160108379514, KL divergence: 0.7368206093111072\n",
      "Reconstruction loss: 202.742167533042, KL divergence: 0.9117039229711965\n",
      "Reconstruction loss: 258.1279853632698, KL divergence: 0.714898744294088\n",
      "Reconstruction loss: 229.58236105951022, KL divergence: 0.28469281301297295\n",
      "Reconstruction loss: 168.0309900021764, KL divergence: 0.17324141149605482\n",
      "Reconstruction loss: 183.45421656468653, KL divergence: 0.14506296933607254\n",
      "Reconstruction loss: 212.74532249413193, KL divergence: 0.038168829637713286\n",
      "Reconstruction loss: 157.21364379641278, KL divergence: 0.17324141149605482\n",
      "Reconstruction loss: 192.21878281229166, KL divergence: 0.030301148872598216\n",
      "Reconstruction loss: 246.73735083648077, KL divergence: 0.25264437951967833\n",
      "Reconstruction loss: 251.68172479426784, KL divergence: 0.11498612233337818\n",
      "Reconstruction loss: 257.03246947239865, KL divergence: 0.8125850990954033\n",
      "Reconstruction loss: 280.8370023674349, KL divergence: 1.6419905296208896\n",
      "Reconstruction loss: 257.14195344194854, KL divergence: 0.31173233955611174\n",
      "Reconstruction loss: 166.93225166559253, KL divergence: 0.29884526978032316\n",
      "Reconstruction loss: 156.02323750113914, KL divergence: 0.45276279194532865\n",
      "Reconstruction loss: 178.59728708640057, KL divergence: 0.17778370605211846\n",
      "Reconstruction loss: 202.40329801256473, KL divergence: 0.22900377346486217\n",
      "Reconstruction loss: 191.7016381176823, KL divergence: 0.2143220246604282\n",
      "Reconstruction loss: 172.09443265449335, KL divergence: 0.10145063959107953\n",
      "Reconstruction loss: 214.2749195869423, KL divergence: 2.9945242833267334\n",
      "Reconstruction loss: 253.7227526595707, KL divergence: 3.3243124350448743\n",
      "Reconstruction loss: 171.83612560964332, KL divergence: 0.1229396142001497\n",
      "Reconstruction loss: 245.59681552476823, KL divergence: 0.2506257426387389\n",
      "Reconstruction loss: 244.27456430193956, KL divergence: 1.8141917090864343\n",
      "Reconstruction loss: 237.11063996876675, KL divergence: 0.03661358107774043\n",
      "Reconstruction loss: 193.98040920582437, KL divergence: 0.036030153954001665\n",
      "Reconstruction loss: 230.7531600582485, KL divergence: 0.7979778084551397\n",
      "Reconstruction loss: 195.87915953750056, KL divergence: 0.12329893839658956\n",
      "Reconstruction loss: 185.39154073794907, KL divergence: 0.810987107651898\n",
      "Reconstruction loss: 176.08198306556423, KL divergence: 5.852238688978633\n",
      "Reconstruction loss: 237.87492863809823, KL divergence: 0.03321362529495531\n",
      "Reconstruction loss: 212.49030024704, KL divergence: 0.37020038461668187\n",
      "Reconstruction loss: 180.72405733050047, KL divergence: 0.04392732638365937\n",
      "Reconstruction loss: 238.88809838308367, KL divergence: 1.6127364051595734\n",
      "Reconstruction loss: 224.45815853355253, KL divergence: 1.0133612380577843\n",
      "Reconstruction loss: 206.60520193365923, KL divergence: 0.21555860889960432\n",
      "Reconstruction loss: 327.1007820073232, KL divergence: 0.3359814844551495\n",
      "Reconstruction loss: 154.4253651132428, KL divergence: 0.6459649103783394\n",
      "Reconstruction loss: 196.53148649451305, KL divergence: 0.09091987896696768\n",
      "Reconstruction loss: 241.63377073304724, KL divergence: 0.32838106905968784\n",
      "Reconstruction loss: 190.28809308735129, KL divergence: 0.14093182465619353\n",
      "Reconstruction loss: 242.77572239114323, KL divergence: 0.4045540973266713\n",
      "Reconstruction loss: 142.62092922398472, KL divergence: 0.6214220946401277\n",
      "Reconstruction loss: 161.93451242369144, KL divergence: 0.3641815854494893\n",
      "Reconstruction loss: 309.2293694546974, KL divergence: 0.043683742976202755\n",
      "Reconstruction loss: 283.52434382013246, KL divergence: 2.9863307649944826\n",
      "Reconstruction loss: 310.6418028043046, KL divergence: 0.5358873393763671\n",
      "Reconstruction loss: 196.81302377037213, KL divergence: 0.04716086901083294\n",
      "Reconstruction loss: 294.79534766374627, KL divergence: 2.9740647066760624\n",
      "Reconstruction loss: 177.771575768267, KL divergence: 0.05530399497617172\n",
      "Reconstruction loss: 133.79594693551832, KL divergence: 0.6343504087692677\n",
      "Reconstruction loss: 182.69642184283623, KL divergence: 0.05092296085192355\n",
      "Reconstruction loss: 204.71130084331878, KL divergence: 0.3927197817377797\n",
      "Reconstruction loss: 166.84128467167645, KL divergence: 8.693128792283412\n",
      "Reconstruction loss: 272.46230866756, KL divergence: 0.053138069677290845\n",
      "Reconstruction loss: 249.79664978310683, KL divergence: 3.0279342704112078\n",
      "Reconstruction loss: 248.17148649365106, KL divergence: 7.937025219178977\n",
      "Reconstruction loss: 155.50171474096814, KL divergence: 0.0677047863969471\n",
      "Reconstruction loss: 227.93117339881616, KL divergence: 0.09228593902242055\n",
      "Reconstruction loss: 148.99781526026732, KL divergence: 0.5789779824995848\n",
      "Reconstruction loss: 165.56160066534662, KL divergence: 4.781644522996996\n",
      "Reconstruction loss: 258.7401815774946, KL divergence: 0.21720102475359582\n",
      "Reconstruction loss: 247.31226529864793, KL divergence: 0.18691606175664116\n",
      "Reconstruction loss: 143.89494320402542, KL divergence: 0.7188637655533643\n",
      "Reconstruction loss: 296.2074212791276, KL divergence: 1.0005020295512996\n",
      "Reconstruction loss: 215.52265639341508, KL divergence: 0.08211422857907069\n",
      "Reconstruction loss: 163.56009510485, KL divergence: 8.142305301995911\n",
      "Reconstruction loss: 187.65372539664244, KL divergence: 5.592764005518685\n",
      "Reconstruction loss: 259.21032995591406, KL divergence: 0.5121868574455248\n",
      "Reconstruction loss: 242.77553341134688, KL divergence: 0.03056665358289251\n",
      "Reconstruction loss: 159.8513621975136, KL divergence: 0.5034899445393022\n",
      "Reconstruction loss: 171.48320567421365, KL divergence: 0.059748624010564466\n",
      "Reconstruction loss: 204.93580232244108, KL divergence: 0.17010373154119512\n",
      "Reconstruction loss: 183.76137481891664, KL divergence: 0.13731189816934747\n",
      "Reconstruction loss: 145.0337116931338, KL divergence: 0.6552211877056744\n",
      "Reconstruction loss: 140.53555186922284, KL divergence: 0.5972010866843782\n",
      "Reconstruction loss: 252.5030548506043, KL divergence: 1.0927152856009439\n",
      "Reconstruction loss: 132.48329272437743, KL divergence: 0.8113283432225653\n",
      "Reconstruction loss: 216.55138712299288, KL divergence: 1.5700107571343804\n",
      "Reconstruction loss: 246.30314434028026, KL divergence: 0.608982718875972\n",
      "Reconstruction loss: 155.29533440551057, KL divergence: 0.4947704770054969\n",
      "Reconstruction loss: 210.76597081314245, KL divergence: 3.57999017218145\n",
      "Reconstruction loss: 153.54583999274124, KL divergence: 0.17558772190054034\n",
      "Reconstruction loss: 251.94253861731025, KL divergence: 0.06233699506364043\n",
      "Reconstruction loss: 281.81498863771947, KL divergence: 1.1314174758757376\n",
      "Reconstruction loss: 208.33954667391052, KL divergence: 0.03209240217684234\n",
      "Reconstruction loss: 171.5691198962429, KL divergence: 0.5153427947107785\n",
      "Reconstruction loss: 178.95220970915165, KL divergence: 0.9044447762975958\n",
      "Reconstruction loss: 270.64399630439755, KL divergence: 1.4196719673178746\n",
      "Reconstruction loss: 212.63790788758843, KL divergence: 0.07027067229722644\n",
      "Reconstruction loss: 182.13355505851405, KL divergence: 0.3048121171923693\n",
      "Reconstruction loss: 153.65250621550248, KL divergence: 0.46212625088448617\n",
      "Reconstruction loss: 291.86848972783633, KL divergence: 0.7419042415918794\n",
      "Reconstruction loss: 268.4580610137802, KL divergence: 4.432307138711746\n",
      "Reconstruction loss: 306.48458974176265, KL divergence: 2.8067108533261718\n",
      "Reconstruction loss: 218.37717842241028, KL divergence: 0.8596624400356769\n",
      "Reconstruction loss: 242.13562760786192, KL divergence: 1.957536591992837\n",
      "Reconstruction loss: 281.7774241827273, KL divergence: 0.20977762049672838\n",
      "Reconstruction loss: 242.43193580476836, KL divergence: 0.08303589763981373\n",
      "Reconstruction loss: 176.31976049832343, KL divergence: 0.16900754634353815\n",
      "Reconstruction loss: 185.42807817404318, KL divergence: 0.06374186149302458\n",
      "Reconstruction loss: 193.16234958261697, KL divergence: 0.4305386007068159\n",
      "Reconstruction loss: 230.4029426948213, KL divergence: 0.21436903337477425\n",
      "Reconstruction loss: 156.01962002730622, KL divergence: 0.4626425989113175\n",
      "Reconstruction loss: 200.2216494497577, KL divergence: 0.1801008944687701\n",
      "Reconstruction loss: 155.48930497245186, KL divergence: 0.6476221629560843\n",
      "Reconstruction loss: 192.54349910354776, KL divergence: 0.26634693930599795\n",
      "Reconstruction loss: 273.68560045684904, KL divergence: 5.7309812615242866\n",
      "Reconstruction loss: 256.34261966571785, KL divergence: 1.016557362542105\n",
      "Reconstruction loss: 173.5097716638447, KL divergence: 0.6256134255067285\n",
      "Reconstruction loss: 153.58250737811485, KL divergence: 0.17915144950177264\n",
      "Reconstruction loss: 186.34508783398488, KL divergence: 3.2480398856017936\n",
      "Reconstruction loss: 201.94884510650365, KL divergence: 0.1926830537756266\n",
      "Reconstruction loss: 190.54139876822185, KL divergence: 0.031174073421823267\n",
      "Reconstruction loss: 168.46080564808062, KL divergence: 0.43959380647643337\n",
      "Reconstruction loss: 179.70004415911228, KL divergence: 0.18327521151476506\n",
      "Reconstruction loss: 190.44867928792888, KL divergence: 0.03172504930338277\n",
      "Reconstruction loss: 204.23450742576955, KL divergence: 0.11597029641992002\n",
      "Reconstruction loss: 230.5016555378215, KL divergence: 0.7518240608908292\n",
      "Reconstruction loss: 211.64166750841804, KL divergence: 0.26177988181037554\n",
      "Reconstruction loss: 195.57832458787487, KL divergence: 0.19155015771396855\n",
      "Reconstruction loss: 233.84713807721252, KL divergence: 4.405245968401157\n",
      "Reconstruction loss: 340.0645413703054, KL divergence: 0.606032290055015\n",
      "Reconstruction loss: 213.6399025292494, KL divergence: 0.034986537555497466\n",
      "Reconstruction loss: 191.4642575914351, KL divergence: 0.044717281933668696\n",
      "Reconstruction loss: 181.08287295717696, KL divergence: 0.19314870892011532\n",
      "Reconstruction loss: 154.54442986925017, KL divergence: 0.845097453904132\n",
      "Reconstruction loss: 179.91144207573865, KL divergence: 0.27911420483949084\n",
      "Reconstruction loss: 189.32213857323637, KL divergence: 0.19840079490017193\n",
      "Reconstruction loss: 245.08813770567838, KL divergence: 0.7736294126320111\n",
      "Reconstruction loss: 122.0490254252821, KL divergence: 0.8488329586759487\n",
      "Reconstruction loss: 149.08614990724467, KL divergence: 0.9999836308964194\n",
      "Reconstruction loss: 395.2893860891492, KL divergence: 0.4422770589501327\n",
      "Reconstruction loss: 156.917083554262, KL divergence: 1.054945263926016\n",
      "Reconstruction loss: 253.3613286563549, KL divergence: 0.43331301665313315\n",
      "Reconstruction loss: 191.21315377812374, KL divergence: 0.0744253513290466\n",
      "Reconstruction loss: 253.79070765063972, KL divergence: 0.7813039204914853\n",
      "Reconstruction loss: 143.07447375227315, KL divergence: 0.6093630927544036\n",
      "Reconstruction loss: 270.8164306408324, KL divergence: 1.3428928584348137\n",
      "Reconstruction loss: 145.15761485976765, KL divergence: 0.9245153640619006\n",
      "Reconstruction loss: 204.98704178881218, KL divergence: 0.05882981462421555\n",
      "Reconstruction loss: 223.21971456698088, KL divergence: 0.49472008973478804\n",
      "Reconstruction loss: 145.73397073510392, KL divergence: 0.6361725758850673\n",
      "Reconstruction loss: 182.89488599373175, KL divergence: 0.9875863341181317\n",
      "Reconstruction loss: 149.5362861358762, KL divergence: 0.44202600159848965\n",
      "Reconstruction loss: 269.26516398787135, KL divergence: 0.05262726538833551\n",
      "Reconstruction loss: 171.25116217431946, KL divergence: 0.3288018025289232\n",
      "Reconstruction loss: 161.5175635553487, KL divergence: 0.8144609741101732\n",
      "Reconstruction loss: 168.8686532290365, KL divergence: 0.061025136757841436\n",
      "Reconstruction loss: 209.00758381516616, KL divergence: 0.43562709681786066\n",
      "Reconstruction loss: 219.81458666754148, KL divergence: 0.045751539380496364\n",
      "Reconstruction loss: 160.0771983011124, KL divergence: 0.4213128585024684\n",
      "Reconstruction loss: 140.88608491086492, KL divergence: 0.4068825000776427\n",
      "Reconstruction loss: 159.10619307268453, KL divergence: 0.9582313684772099\n",
      "Reconstruction loss: 189.76035208194241, KL divergence: 0.05533949822745632\n",
      "Reconstruction loss: 247.36543722887413, KL divergence: 2.0195005840089504\n",
      "Reconstruction loss: 170.64334799995754, KL divergence: 0.4126603665951182\n",
      "Reconstruction loss: 222.4877310285982, KL divergence: 0.12183164112796085\n",
      "Reconstruction loss: 267.98521096560125, KL divergence: 0.5564989426295187\n",
      "Reconstruction loss: 264.9962983247808, KL divergence: 0.3395568663355008\n",
      "Reconstruction loss: 243.07776965189925, KL divergence: 0.05816841649458043\n",
      "Reconstruction loss: 285.8755310224233, KL divergence: 0.3056954109027597\n",
      "Reconstruction loss: 236.05178726180134, KL divergence: 0.2586467220946702\n",
      "Reconstruction loss: 324.5266567403217, KL divergence: 1.163840838132557\n",
      "Reconstruction loss: 166.87784829871907, KL divergence: 0.4185198620732969\n",
      "Reconstruction loss: 200.44895904221568, KL divergence: 0.11248125174866497\n",
      "Reconstruction loss: 162.47666718505235, KL divergence: 1.4068355508818224\n",
      "Reconstruction loss: 124.33016785707406, KL divergence: 0.7264880442043176\n",
      "Reconstruction loss: 187.50872675450879, KL divergence: 0.03700341911576066\n",
      "Reconstruction loss: 158.67930512923283, KL divergence: 0.7811705750548184\n",
      "Reconstruction loss: 252.83898756221225, KL divergence: 0.40720471054016355\n",
      "Reconstruction loss: 235.6690034964629, KL divergence: 0.07275195702723147\n",
      "Reconstruction loss: 177.87268239253774, KL divergence: 0.05089238819253705\n",
      "Reconstruction loss: 246.02100596131572, KL divergence: 1.4027649825011204\n",
      "Reconstruction loss: 211.92249270217292, KL divergence: 0.3505597559593477\n",
      "Reconstruction loss: 135.32315525203717, KL divergence: 0.8292098812789706\n",
      "Reconstruction loss: 198.6920104029679, KL divergence: 0.1865029850096761\n",
      "Reconstruction loss: 176.7808598596303, KL divergence: 0.1865029850096761\n",
      "Reconstruction loss: 200.6587169847423, KL divergence: 3.47284681599195\n",
      "Reconstruction loss: 281.39862822210534, KL divergence: 1.269210478104957\n",
      "Reconstruction loss: 248.43884608024769, KL divergence: 0.11217335643026993\n",
      "Reconstruction loss: 222.65046400852316, KL divergence: 0.04680763826970186\n",
      "Reconstruction loss: 209.64878396513546, KL divergence: 0.04028938954744837\n",
      "Reconstruction loss: 186.2503026451899, KL divergence: 0.48832753098367304\n",
      "Reconstruction loss: 125.4838582440477, KL divergence: 0.7896903360441083\n",
      "Reconstruction loss: 223.9514804885379, KL divergence: 1.1797153155872464\n",
      "Reconstruction loss: 127.54141988079748, KL divergence: 0.8865307142102714\n",
      "Reconstruction loss: 187.83227525004475, KL divergence: 0.05527145006478218\n",
      "Reconstruction loss: 229.43530407261733, KL divergence: 6.501732357865086\n",
      "Reconstruction loss: 352.35435585357527, KL divergence: 1.285256813562139\n",
      "Reconstruction loss: 177.78745570448038, KL divergence: 0.2143978074956454\n",
      "Reconstruction loss: 228.45142724360562, KL divergence: 0.03704896476236985\n",
      "Reconstruction loss: 256.0141140655352, KL divergence: 0.1310363830125743\n",
      "Reconstruction loss: 220.86730580059242, KL divergence: 0.1634362277490259\n",
      "Reconstruction loss: 259.3343402710461, KL divergence: 0.46082318343029777\n",
      "Reconstruction loss: 197.76408565177428, KL divergence: 0.18172313899967796\n",
      "Reconstruction loss: 220.08763113223893, KL divergence: 2.985589713332754\n",
      "Reconstruction loss: 186.50833532468312, KL divergence: 0.526820194330428\n",
      "Reconstruction loss: 124.47907751621065, KL divergence: 0.7620747150223213\n",
      "Reconstruction loss: 240.85993843138777, KL divergence: 0.32811084665156526\n",
      "Reconstruction loss: 219.77783278159313, KL divergence: 0.7447640916428919\n",
      "Reconstruction loss: 135.04150287721046, KL divergence: 0.7213856520617514\n",
      "Reconstruction loss: 258.14977549189484, KL divergence: 3.7784950521361123\n",
      "Reconstruction loss: 224.78317521241627, KL divergence: 0.2403778080298778\n",
      "Reconstruction loss: 178.5441894392369, KL divergence: 0.1652675484217117\n",
      "Reconstruction loss: 161.80693482837458, KL divergence: 0.32902176795529303\n",
      "Reconstruction loss: 187.99272057863715, KL divergence: 0.2908098472550758\n",
      "Reconstruction loss: 145.1434047337607, KL divergence: 0.3223863831957122\n",
      "Reconstruction loss: 202.27158260932111, KL divergence: 0.04413053584401394\n",
      "Reconstruction loss: 147.34554384153518, KL divergence: 0.48062475241467845\n",
      "Reconstruction loss: 165.47050310820674, KL divergence: 0.6969420295119595\n",
      "Reconstruction loss: 204.21563031362552, KL divergence: 0.19546342722249044\n",
      "Reconstruction loss: 227.26926586321764, KL divergence: 3.7355827504505665\n",
      "Reconstruction loss: 179.5081805233746, KL divergence: 0.35222287728312024\n",
      "Reconstruction loss: 224.71202249390495, KL divergence: 0.06608533876093009\n",
      "Reconstruction loss: 206.0209060899563, KL divergence: 0.08120774570583789\n",
      "Reconstruction loss: 213.4462076523357, KL divergence: 0.13079050361840228\n",
      "Reconstruction loss: 204.4272891819101, KL divergence: 0.03453970995579714\n",
      "Reconstruction loss: 254.09840156846042, KL divergence: 1.7449133802379717\n",
      "Reconstruction loss: 206.40119354542423, KL divergence: 0.9169979463332145\n",
      "Reconstruction loss: 165.3119581080621, KL divergence: 0.20703558709792058\n",
      "Reconstruction loss: 197.45571506978402, KL divergence: 0.9388310617341065\n",
      "Reconstruction loss: 163.42263439546053, KL divergence: 0.35412044154706535\n",
      "Reconstruction loss: 199.530828384203, KL divergence: 0.08354471783527823\n",
      "Reconstruction loss: 161.60767234569937, KL divergence: 0.5490506689441372\n",
      "Reconstruction loss: 172.41085571019937, KL divergence: 0.3496368463007202\n",
      "Reconstruction loss: 124.96994480136854, KL divergence: 0.5157190153214808\n",
      "Reconstruction loss: 248.56685938856725, KL divergence: 0.04501786776508043\n",
      "Reconstruction loss: 243.81897417702004, KL divergence: 0.5568690131951295\n",
      "Reconstruction loss: 201.16524013391492, KL divergence: 0.03323294454299258\n",
      "Reconstruction loss: 219.49655021922985, KL divergence: 0.03297289891844701\n",
      "Reconstruction loss: 183.8214398080188, KL divergence: 0.2559273832718147\n",
      "Reconstruction loss: 247.54665471192337, KL divergence: 0.301876595737823\n",
      "Reconstruction loss: 190.41513535376254, KL divergence: 0.04820217563459339\n",
      "Reconstruction loss: 194.54834652775145, KL divergence: 0.03753531183569464\n",
      "Reconstruction loss: 227.49806613465108, KL divergence: 0.033642804321150244\n",
      "Reconstruction loss: 154.75894996545475, KL divergence: 0.6344718422737199\n",
      "Reconstruction loss: 166.1511683033052, KL divergence: 0.6348291711144795\n",
      "Reconstruction loss: 236.34331906967884, KL divergence: 0.10081558388407946\n",
      "Reconstruction loss: 184.9393191186478, KL divergence: 0.8618107177759433\n",
      "Reconstruction loss: 263.24347561101536, KL divergence: 0.4684526841801793\n",
      "Reconstruction loss: 191.75864220519128, KL divergence: 0.06143493351505164\n",
      "Reconstruction loss: 153.01609534029842, KL divergence: 0.6006623480414237\n",
      "Reconstruction loss: 266.1983336266673, KL divergence: 4.667268203478678\n",
      "Reconstruction loss: 240.0722233623059, KL divergence: 0.17250052674572403\n",
      "Reconstruction loss: 230.6285951549002, KL divergence: 0.2905763909112975\n",
      "Reconstruction loss: 116.92216388553402, KL divergence: 0.535174501968021\n",
      "Reconstruction loss: 280.6672212862518, KL divergence: 0.16575579735975926\n",
      "Reconstruction loss: 176.99195596045774, KL divergence: 0.05403517918663564\n",
      "Reconstruction loss: 282.61565193895257, KL divergence: 0.05218609992136958\n",
      "Reconstruction loss: 169.37057809347928, KL divergence: 0.36781081995954723\n",
      "Reconstruction loss: 144.92077360867745, KL divergence: 0.3358165419747081\n",
      "Reconstruction loss: 172.5466383224497, KL divergence: 0.2249454975845852\n",
      "Reconstruction loss: 244.81515162574595, KL divergence: 0.706049588432008\n",
      "Reconstruction loss: 283.3604560083569, KL divergence: 0.03343194983058495\n",
      "Reconstruction loss: 223.87412811704291, KL divergence: 0.27117655782824374\n",
      "Reconstruction loss: 212.98952965180638, KL divergence: 0.2874791976039498\n",
      "Reconstruction loss: 223.308717210888, KL divergence: 0.7823271638612835\n",
      "Reconstruction loss: 212.80120913626058, KL divergence: 0.03991542403183079\n",
      "Reconstruction loss: 194.86418983561512, KL divergence: 0.9882027786497717\n",
      "Reconstruction loss: 287.8644850283829, KL divergence: 0.033976991458803674\n",
      "Reconstruction loss: 251.04836507990836, KL divergence: 2.5898946026430254\n",
      "Reconstruction loss: 207.56598946797362, KL divergence: 0.033268963919967365\n",
      "Reconstruction loss: 130.49629949216353, KL divergence: 0.47819541947988753\n",
      "Reconstruction loss: 136.81903983690927, KL divergence: 0.9491384339769849\n",
      "Reconstruction loss: 125.73933303371996, KL divergence: 0.6716830516766732\n",
      "Reconstruction loss: 133.48464421520453, KL divergence: 0.363784722858187\n",
      "Reconstruction loss: 109.7994651304491, KL divergence: 0.7400021717308982\n",
      "Reconstruction loss: 173.43679267922377, KL divergence: 0.686212993409524\n",
      "Reconstruction loss: 201.249645873722, KL divergence: 0.18954125927483395\n",
      "Reconstruction loss: 179.8711965487849, KL divergence: 0.207770394943825\n",
      "Reconstruction loss: 128.38725243799817, KL divergence: 0.9079967404415903\n",
      "Reconstruction loss: 127.96639250052064, KL divergence: 0.6719163521426423\n",
      "Reconstruction loss: 273.46030271553866, KL divergence: 0.03421997681432554\n",
      "Reconstruction loss: 136.12166969603584, KL divergence: 0.29902115666596524\n",
      "Reconstruction loss: 266.0665813964406, KL divergence: 0.17193602116273027\n",
      "Reconstruction loss: 164.98192046661697, KL divergence: 0.39185409567727153\n",
      "Reconstruction loss: 139.3450076654308, KL divergence: 0.4461218670122649\n",
      "Reconstruction loss: 225.52659443266268, KL divergence: 0.03377991605998726\n",
      "Reconstruction loss: 173.81754147773108, KL divergence: 0.05257049781771378\n",
      "Reconstruction loss: 168.38688936627204, KL divergence: 0.18852981046526573\n",
      "Reconstruction loss: 172.67561981761264, KL divergence: 0.18852981046526573\n",
      "Reconstruction loss: 212.6296569083335, KL divergence: 0.17745708289097273\n",
      "Reconstruction loss: 221.47016830689324, KL divergence: 0.22350965478911655\n",
      "Reconstruction loss: 279.8493743174391, KL divergence: 0.28538011307041206\n",
      "Reconstruction loss: 227.2227617579424, KL divergence: 0.15948051571446797\n",
      "Reconstruction loss: 183.085396809398, KL divergence: 0.18852981046526573\n",
      "Reconstruction loss: 197.37644063081757, KL divergence: 2.554134726480048\n",
      "Reconstruction loss: 243.04011396478109, KL divergence: 0.06510539822928163\n",
      "Reconstruction loss: 178.39663594604022, KL divergence: 0.3825825180987529\n",
      "Reconstruction loss: 126.43691078571752, KL divergence: 0.36244107609114706\n",
      "Reconstruction loss: 257.52284523124456, KL divergence: 0.2502397410893289\n",
      "Reconstruction loss: 187.4469770722832, KL divergence: 0.08466718993675282\n",
      "Reconstruction loss: 137.090500861327, KL divergence: 0.8542723592220722\n",
      "Reconstruction loss: 159.27368584466188, KL divergence: 0.18852981046526573\n",
      "Reconstruction loss: 126.69617147601407, KL divergence: 0.20636509505560902\n",
      "Reconstruction loss: 134.24533638644107, KL divergence: 0.6741909905940631\n",
      "Reconstruction loss: 196.65783820599745, KL divergence: 0.12866483020390973\n",
      "Reconstruction loss: 216.8303615471337, KL divergence: 0.05282070769234132\n",
      "Reconstruction loss: 188.56657340497276, KL divergence: 0.2401764189193717\n",
      "Reconstruction loss: 247.54821369610156, KL divergence: 0.04348148103554644\n",
      "Reconstruction loss: 160.99947520478122, KL divergence: 0.07697466135667513\n",
      "Reconstruction loss: 146.64397069612025, KL divergence: 0.1853658227483279\n",
      "Reconstruction loss: 222.7732528558442, KL divergence: 0.09897251466092133\n",
      "Reconstruction loss: 201.64337261203218, KL divergence: 0.07016746107017541\n",
      "Reconstruction loss: 248.9748494369757, KL divergence: 0.1359995700785504\n",
      "Reconstruction loss: 199.10337434913237, KL divergence: 0.14628750264747037\n",
      "Reconstruction loss: 118.8350469394137, KL divergence: 0.38243839679415376\n",
      "Reconstruction loss: 278.98979437273044, KL divergence: 0.09766645573431693\n",
      "Reconstruction loss: 173.3534884453374, KL divergence: 0.09614577085774567\n",
      "Reconstruction loss: 153.9351616919795, KL divergence: 0.7328018025481305\n",
      "Reconstruction loss: 172.76663742026088, KL divergence: 0.18783880117363133\n",
      "Reconstruction loss: 196.27362129431617, KL divergence: 6.3401980463674645\n",
      "Reconstruction loss: 263.5365833336514, KL divergence: 0.18609607178282817\n",
      "Reconstruction loss: 273.3579631076922, KL divergence: 0.18783880117363133\n",
      "Reconstruction loss: 177.46660614836128, KL divergence: 0.18783880117363133\n",
      "Reconstruction loss: 220.40981914782287, KL divergence: 1.81010520164203\n",
      "Reconstruction loss: 198.79539531325665, KL divergence: 2.286416309101436\n",
      "Reconstruction loss: 192.62099469314165, KL divergence: 0.09808647791472763\n",
      "Reconstruction loss: 236.82114334758037, KL divergence: 1.074487585780132\n",
      "Reconstruction loss: 266.28710467720487, KL divergence: 0.058405350595519656\n",
      "Reconstruction loss: 142.60966352949282, KL divergence: 0.18783880117363133\n",
      "Reconstruction loss: 135.2856836712544, KL divergence: 0.47428817944516505\n",
      "Reconstruction loss: 239.74486832068646, KL divergence: 1.094699244110292\n",
      "Reconstruction loss: 234.5480804677804, KL divergence: 0.03327499263647943\n",
      "Reconstruction loss: 238.80165778500384, KL divergence: 0.18783880117363133\n",
      "Reconstruction loss: 175.0255902518345, KL divergence: 0.1916301590639986\n",
      "Reconstruction loss: 230.18448727394775, KL divergence: 0.6626550897837351\n",
      "Reconstruction loss: 215.5743757382736, KL divergence: 0.18783880117363133\n",
      "Reconstruction loss: 168.30205592239156, KL divergence: 0.18783880117363133\n",
      "Reconstruction loss: 134.05992016487625, KL divergence: 0.4080511856326608\n",
      "Reconstruction loss: 130.23838539499053, KL divergence: 0.5187016940128546\n",
      "Reconstruction loss: 169.904487248061, KL divergence: 0.04776737717523233\n",
      "Reconstruction loss: 256.0407841893102, KL divergence: 0.13182556597804207\n",
      "Reconstruction loss: 176.94828145930188, KL divergence: 0.12914895759508394\n",
      "Reconstruction loss: 201.15880654462646, KL divergence: 0.03251055988220153\n",
      "Reconstruction loss: 235.13486122703364, KL divergence: 4.073010860991429\n",
      "Reconstruction loss: 225.2226293272053, KL divergence: 0.3446146378407717\n",
      "Reconstruction loss: 217.95414010746862, KL divergence: 0.03446916990459953\n",
      "Reconstruction loss: 176.66965215597014, KL divergence: 0.17746076750462741\n",
      "Reconstruction loss: 137.12821154618806, KL divergence: 0.24844103456296396\n",
      "Reconstruction loss: 212.12469815647654, KL divergence: 0.1872929796385252\n",
      "Reconstruction loss: 229.87867623979545, KL divergence: 2.502581132906004\n",
      "Reconstruction loss: 256.7705995773214, KL divergence: 1.385652485708244\n",
      "Reconstruction loss: 199.14218368057112, KL divergence: 0.09291299115131968\n",
      "Reconstruction loss: 184.2273582536343, KL divergence: 0.09775784347696898\n",
      "Reconstruction loss: 196.7527640345534, KL divergence: 0.1872929796385252\n",
      "Reconstruction loss: 161.4071229477303, KL divergence: 0.5474651695030484\n",
      "Reconstruction loss: 164.1958896749568, KL divergence: 0.11673284029803743\n",
      "Reconstruction loss: 241.85596943960581, KL divergence: 0.05302892400237291\n",
      "Reconstruction loss: 191.44428579976952, KL divergence: 0.1872929796385252\n",
      "Reconstruction loss: 209.28264510839125, KL divergence: 0.045612494657652214\n",
      "Reconstruction loss: 206.10629383887976, KL divergence: 0.07959325039077153\n",
      "Reconstruction loss: 143.57932340483353, KL divergence: 0.4469855349966823\n",
      "Reconstruction loss: 228.72692670767333, KL divergence: 0.05704933053138972\n",
      "Reconstruction loss: 216.61225075273097, KL divergence: 2.341749645897606\n",
      "Reconstruction loss: 228.52395690755708, KL divergence: 0.2173033126587402\n",
      "Reconstruction loss: 202.0644634384407, KL divergence: 0.0787892382665139\n",
      "Reconstruction loss: 265.08297237348285, KL divergence: 0.8200560237386858\n",
      "Reconstruction loss: 133.06374010840347, KL divergence: 0.41591773185843767\n",
      "Reconstruction loss: 148.6201378100787, KL divergence: 0.3191671951747021\n",
      "Reconstruction loss: 156.85251537838053, KL divergence: 0.1872929796385252\n",
      "Reconstruction loss: 127.9622910780986, KL divergence: 0.5428064203304133\n",
      "Reconstruction loss: 195.4045375536814, KL divergence: 0.1872929796385252\n",
      "Reconstruction loss: 180.71277950108615, KL divergence: 0.25743912389421225\n",
      "Reconstruction loss: 187.65995523335891, KL divergence: 0.04669818748421761\n",
      "Reconstruction loss: 182.16912690362363, KL divergence: 0.46131478722132285\n",
      "Reconstruction loss: 246.24635970505017, KL divergence: 5.129106384944646\n",
      "Reconstruction loss: 255.71876269191017, KL divergence: 0.9750221680694882\n",
      "Reconstruction loss: 119.45783739196696, KL divergence: 0.7380588222487017\n",
      "Reconstruction loss: 353.92912587719536, KL divergence: 1.1796543807959066\n",
      "Reconstruction loss: 172.32816937885744, KL divergence: 0.18780165056276216\n",
      "Reconstruction loss: 192.60318388964495, KL divergence: 0.10532572629512565\n",
      "Reconstruction loss: 260.47207576929685, KL divergence: 1.5811894254957655\n",
      "Reconstruction loss: 230.40085132320996, KL divergence: 0.4910931995506552\n",
      "Reconstruction loss: 226.29913863676072, KL divergence: 0.9769976969951728\n",
      "Reconstruction loss: 184.90214821101182, KL divergence: 7.456810012051807\n",
      "Reconstruction loss: 170.8952329414875, KL divergence: 0.09565052092923676\n",
      "Reconstruction loss: 190.53513485133956, KL divergence: 0.04793217721331694\n",
      "Reconstruction loss: 301.6240063142513, KL divergence: 0.9226479761895943\n",
      "Reconstruction loss: 223.39236322687464, KL divergence: 0.30978034891860795\n",
      "Reconstruction loss: 199.12774647300182, KL divergence: 0.11731190699102234\n",
      "Reconstruction loss: 312.47713393847965, KL divergence: 0.8005149529337794\n",
      "Reconstruction loss: 166.26951303234637, KL divergence: 0.3705213920919463\n",
      "Reconstruction loss: 215.90643878097407, KL divergence: 0.34051608394129174\n",
      "Reconstruction loss: 174.54833332089174, KL divergence: 0.046904788809298215\n",
      "Reconstruction loss: 300.4032196836477, KL divergence: 0.6606878712034303\n",
      "Reconstruction loss: 152.38360946187794, KL divergence: 0.19028591274411694\n",
      "Reconstruction loss: 185.78012568387328, KL divergence: 0.036534400847011095\n",
      "Reconstruction loss: 193.4342627396705, KL divergence: 0.5148897083221606\n",
      "Reconstruction loss: 150.63083652887633, KL divergence: 0.11982283472171201\n",
      "Reconstruction loss: 172.6461818810273, KL divergence: 0.08941981097898977\n",
      "Reconstruction loss: 144.14565655556606, KL divergence: 0.25296680230751123\n",
      "Reconstruction loss: 216.4106700088069, KL divergence: 0.15752060968285614\n",
      "Reconstruction loss: 279.9753058875324, KL divergence: 3.587943986892622\n",
      "Reconstruction loss: 176.6244972360613, KL divergence: 0.03748830657603858\n",
      "Reconstruction loss: 247.5213893819959, KL divergence: 2.443683175553533\n",
      "Reconstruction loss: 198.9579402553507, KL divergence: 0.02974848633242677\n",
      "Reconstruction loss: 203.5290748050959, KL divergence: 0.14403008454583766\n",
      "Reconstruction loss: 123.6553196957858, KL divergence: 0.5125452839892712\n",
      "Reconstruction loss: 234.024206816486, KL divergence: 0.18780165056276216\n",
      "Reconstruction loss: 207.76088971066056, KL divergence: 0.03215653662292012\n",
      "Reconstruction loss: 299.72995588348385, KL divergence: 0.7474315807314083\n",
      "Reconstruction loss: 247.37497138633563, KL divergence: 0.09122526754814109\n",
      "Reconstruction loss: 257.0834079746278, KL divergence: 0.8261566014278696\n",
      "Reconstruction loss: 211.81718643118748, KL divergence: 0.034178109828661984\n",
      "Reconstruction loss: 187.7016785524497, KL divergence: 0.2556575054081671\n",
      "Reconstruction loss: 256.79208947858814, KL divergence: 0.5861008478560149\n",
      "Reconstruction loss: 174.2122406077794, KL divergence: 0.1885877693137719\n",
      "Reconstruction loss: 308.27379626698416, KL divergence: 0.07834565420023282\n",
      "Reconstruction loss: 192.74240545592568, KL divergence: 0.11780639906537665\n",
      "Reconstruction loss: 264.8308626946639, KL divergence: 0.05658807818366868\n",
      "Reconstruction loss: 219.78304693155278, KL divergence: 1.0502078157365178\n",
      "Reconstruction loss: 224.86630279130983, KL divergence: 3.4810015597254806\n",
      "Reconstruction loss: 198.3897588253742, KL divergence: 0.03793856229104997\n",
      "Reconstruction loss: 182.22584673498136, KL divergence: 0.038125391654376395\n",
      "Reconstruction loss: 238.77409676803074, KL divergence: 0.13406933047552022\n",
      "Reconstruction loss: 302.52099489168586, KL divergence: 1.3077108566128575\n",
      "Reconstruction loss: 216.910409491622, KL divergence: 0.10707218976497612\n",
      "Reconstruction loss: 163.77671553209427, KL divergence: 0.3570398784481948\n",
      "Reconstruction loss: 228.70922752701966, KL divergence: 0.968957050490328\n",
      "Reconstruction loss: 182.4215814550804, KL divergence: 0.10440930025001482\n",
      "Reconstruction loss: 150.34367641380044, KL divergence: 0.1885877693137719\n",
      "Reconstruction loss: 207.12792752458392, KL divergence: 0.12852720643452636\n",
      "Reconstruction loss: 221.71892342341602, KL divergence: 0.24630459001370225\n",
      "Reconstruction loss: 213.76747296973556, KL divergence: 0.06011549859616533\n",
      "Reconstruction loss: 195.63067677906346, KL divergence: 0.03538494362993361\n",
      "Reconstruction loss: 209.6950963257716, KL divergence: 0.05485739153680658\n",
      "Reconstruction loss: 160.0306009116175, KL divergence: 0.1885877693137719\n",
      "Reconstruction loss: 149.44182624010398, KL divergence: 0.1885877693137719\n",
      "Reconstruction loss: 195.93531005522635, KL divergence: 0.03762686134323118\n",
      "Reconstruction loss: 148.2902111192359, KL divergence: 0.3497906493893085\n",
      "Reconstruction loss: 170.55153204582302, KL divergence: 0.34896711025451926\n",
      "Reconstruction loss: 299.35038322354467, KL divergence: 0.597422120743746\n",
      "Reconstruction loss: 142.11037557522394, KL divergence: 0.19541996080039692\n",
      "Reconstruction loss: 193.4061281130643, KL divergence: 0.19036108646051703\n",
      "Reconstruction loss: 150.9879822460005, KL divergence: 0.4310700322073\n",
      "Reconstruction loss: 204.97197689322982, KL divergence: 7.461923329505705\n",
      "Reconstruction loss: 235.57550551144638, KL divergence: 0.2735875823175959\n",
      "Reconstruction loss: 167.83906734039084, KL divergence: 0.4093317917024587\n",
      "Reconstruction loss: 244.147325168866, KL divergence: 0.18487491709869108\n",
      "Reconstruction loss: 129.89534663454464, KL divergence: 0.7427388957983885\n",
      "Reconstruction loss: 215.3198714492742, KL divergence: 0.07789425872705052\n",
      "Reconstruction loss: 120.24052843096418, KL divergence: 0.6468169148713515\n",
      "Reconstruction loss: 218.93053833123486, KL divergence: 0.07214118770449679\n",
      "Reconstruction loss: 271.70054042296965, KL divergence: 0.05284293714207772\n",
      "Reconstruction loss: 174.52414736315365, KL divergence: 0.19036108646051703\n",
      "Reconstruction loss: 164.58462606373865, KL divergence: 0.27244506360026516\n",
      "Reconstruction loss: 276.41522457634704, KL divergence: 1.2112994227101237\n",
      "Reconstruction loss: 125.22158039936346, KL divergence: 0.8218539909440548\n",
      "Reconstruction loss: 263.86624655727684, KL divergence: 0.6808137989158596\n",
      "Reconstruction loss: 226.53116199081666, KL divergence: 0.03450931064133711\n",
      "Reconstruction loss: 187.02659877052912, KL divergence: 0.06099655146975935\n",
      "Reconstruction loss: 167.9421830487338, KL divergence: 0.03024224232187367\n",
      "Reconstruction loss: 210.06683809453932, KL divergence: 0.7339498809340472\n",
      "Reconstruction loss: 215.7942345829777, KL divergence: 0.277337131069525\n",
      "Reconstruction loss: 133.6116615451195, KL divergence: 0.5722166392391321\n",
      "Reconstruction loss: 176.49405908277134, KL divergence: 0.19036108646051703\n",
      "Reconstruction loss: 220.1026809459779, KL divergence: 0.19036108646051703\n",
      "Reconstruction loss: 297.6672012328827, KL divergence: 3.132657403716368\n",
      "Reconstruction loss: 197.55155937667143, KL divergence: 0.18880923525983345\n",
      "Reconstruction loss: 200.37489005144073, KL divergence: 0.19036108646051703\n",
      "Reconstruction loss: 160.59365609849053, KL divergence: 0.5288357382274481\n",
      "Reconstruction loss: 135.8197851185393, KL divergence: 0.35865605812289436\n",
      "Reconstruction loss: 284.6601285915267, KL divergence: 0.07041827353898572\n",
      "Reconstruction loss: 250.08000934631303, KL divergence: 0.19036108646051703\n",
      "Reconstruction loss: 179.41734647749104, KL divergence: 0.19036108646051703\n",
      "Reconstruction loss: 204.5764971258808, KL divergence: 0.10879871253121648\n",
      "Reconstruction loss: 280.17361624597424, KL divergence: 1.1400398190090621\n",
      "Reconstruction loss: 278.46367348456255, KL divergence: 4.655939046833528\n",
      "Reconstruction loss: 183.412327856992, KL divergence: 0.11350085847947117\n",
      "Reconstruction loss: 183.76452223904755, KL divergence: 0.0432138763356929\n",
      "Reconstruction loss: 170.59787575466845, KL divergence: 0.19307704544491905\n",
      "Reconstruction loss: 200.27554581434742, KL divergence: 0.06631904809641864\n",
      "Reconstruction loss: 198.76132563802005, KL divergence: 0.19307704544491905\n",
      "Reconstruction loss: 219.40887364426874, KL divergence: 0.0487803550439293\n",
      "Reconstruction loss: 271.62768953193705, KL divergence: 0.8876292653521121\n",
      "Reconstruction loss: 171.66248048042667, KL divergence: 0.16976681020001466\n",
      "Reconstruction loss: 211.38964691973683, KL divergence: 5.098378654586984\n",
      "Reconstruction loss: 206.32537767774778, KL divergence: 2.681458782922412\n",
      "Reconstruction loss: 250.40699873016274, KL divergence: 0.10736982620361796\n",
      "Reconstruction loss: 216.17178133565798, KL divergence: 3.546902425165893\n",
      "Reconstruction loss: 202.82980521693574, KL divergence: 0.050046055462932004\n",
      "Reconstruction loss: 271.8914002948317, KL divergence: 0.06583183071772264\n",
      "Reconstruction loss: 165.91549548036784, KL divergence: 0.1527452994580787\n",
      "Reconstruction loss: 222.4255514131014, KL divergence: 5.321735823902513\n",
      "Reconstruction loss: 146.30920052919902, KL divergence: 0.25120987216506807\n",
      "Reconstruction loss: 197.37888704592774, KL divergence: 0.2209921871673184\n",
      "Reconstruction loss: 111.25466612253545, KL divergence: 0.701551966934391\n",
      "Reconstruction loss: 297.35748848339654, KL divergence: 0.5842978090601622\n",
      "Reconstruction loss: 186.30296598093156, KL divergence: 0.15722628390327498\n",
      "Reconstruction loss: 242.1483865632308, KL divergence: 0.3965520031579833\n",
      "Reconstruction loss: 215.81603795646367, KL divergence: 0.03444406523034732\n",
      "Reconstruction loss: 197.88487365225393, KL divergence: 0.19307704544491905\n",
      "Reconstruction loss: 199.10664289835924, KL divergence: 7.175231421851597\n",
      "Reconstruction loss: 169.7028228373973, KL divergence: 0.20401250293546896\n",
      "Reconstruction loss: 152.09950077052696, KL divergence: 0.09759871185637498\n",
      "Reconstruction loss: 136.54109076612062, KL divergence: 0.25763487605090973\n",
      "Reconstruction loss: 241.47721821294482, KL divergence: 0.03442553865262499\n",
      "Reconstruction loss: 137.77743055686213, KL divergence: 0.5434568759607784\n",
      "Reconstruction loss: 237.93009720890421, KL divergence: 1.8963814197190243\n",
      "Reconstruction loss: 157.85160004425234, KL divergence: 0.255307819646926\n",
      "Reconstruction loss: 155.23320897725569, KL divergence: 0.20186634142477583\n",
      "Reconstruction loss: 199.57205574899717, KL divergence: 0.19566770686637452\n",
      "Reconstruction loss: 212.77901711080727, KL divergence: 0.08734518153759074\n",
      "Reconstruction loss: 190.78135307801026, KL divergence: 0.20759574233206418\n",
      "Reconstruction loss: 191.30677739730692, KL divergence: 0.0943660514638135\n",
      "Reconstruction loss: 178.68819876333177, KL divergence: 0.23353206417221334\n",
      "Reconstruction loss: 171.60343760630184, KL divergence: 0.19566770686637452\n",
      "Reconstruction loss: 127.45140945620666, KL divergence: 0.5044479684557109\n",
      "Reconstruction loss: 148.4332821339483, KL divergence: 0.6408894577532305\n",
      "Reconstruction loss: 128.76133091604385, KL divergence: 0.6896856611889606\n",
      "Reconstruction loss: 176.49834550531662, KL divergence: 4.998328735096253\n",
      "Reconstruction loss: 188.5390633855483, KL divergence: 0.08357083423227935\n",
      "Reconstruction loss: 132.58608351333243, KL divergence: 0.5520130100965241\n",
      "Reconstruction loss: 204.7785616473173, KL divergence: 0.19566770686637452\n",
      "Reconstruction loss: 212.21530401657768, KL divergence: 0.0335201617577619\n",
      "Reconstruction loss: 191.53912109615112, KL divergence: 6.378078758707832\n",
      "Reconstruction loss: 147.2094349664496, KL divergence: 0.26548892912281574\n",
      "Reconstruction loss: 249.10032616680002, KL divergence: 0.08312934381427978\n",
      "Reconstruction loss: 221.92535419746605, KL divergence: 3.6393273567279687\n",
      "Reconstruction loss: 119.78488024798683, KL divergence: 0.22647339953534273\n",
      "Reconstruction loss: 209.64582654985827, KL divergence: 0.033794954065491545\n",
      "Reconstruction loss: 213.09863071717325, KL divergence: 0.130779392042573\n",
      "Reconstruction loss: 146.1585103231931, KL divergence: 0.39149562592414267\n",
      "Reconstruction loss: 194.29235499437362, KL divergence: 5.7315870963328415\n",
      "Reconstruction loss: 236.3218276427728, KL divergence: 0.19509207001189738\n",
      "Reconstruction loss: 147.16742947497193, KL divergence: 0.19566770686637452\n",
      "Reconstruction loss: 153.91831656604234, KL divergence: 0.24717159004242512\n",
      "Reconstruction loss: 212.02971379762317, KL divergence: 0.15132493067685165\n",
      "Reconstruction loss: 190.89686193818977, KL divergence: 0.19566770686637452\n",
      "Reconstruction loss: 206.09781233684475, KL divergence: 2.8351492475611106\n",
      "Reconstruction loss: 239.16000761443854, KL divergence: 0.03729853129435279\n",
      "Reconstruction loss: 180.7497033821495, KL divergence: 0.08858057810549186\n",
      "Reconstruction loss: 203.60921552119947, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 265.63325653171023, KL divergence: 0.03460826960047941\n",
      "Reconstruction loss: 191.29774036375352, KL divergence: 4.025731934918865\n",
      "Reconstruction loss: 270.50668849015506, KL divergence: 0.2552218157135279\n",
      "Reconstruction loss: 167.0869598880864, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 210.37775238471335, KL divergence: 0.046749337686087855\n",
      "Reconstruction loss: 116.3307547917857, KL divergence: 0.9901741849555991\n",
      "Reconstruction loss: 198.3189402391402, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 222.0000082211619, KL divergence: 0.06490876577549437\n",
      "Reconstruction loss: 240.93926387275195, KL divergence: 0.18247244272190816\n",
      "Reconstruction loss: 187.3951265956499, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 185.42303077312886, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 132.17520951128873, KL divergence: 0.3899065406686933\n",
      "Reconstruction loss: 261.52957132050267, KL divergence: 0.05483100774305183\n",
      "Reconstruction loss: 171.64963561526417, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 255.23732449525255, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 149.83378849681276, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 198.7538645459902, KL divergence: 4.306159360719281\n",
      "Reconstruction loss: 185.6000874021815, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 291.5758715710863, KL divergence: 1.0101208402984878\n",
      "Reconstruction loss: 185.34232592195843, KL divergence: 0.1397889863318318\n",
      "Reconstruction loss: 253.6635245812811, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 219.97429999704696, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 193.7095574879411, KL divergence: 4.982878580110125\n",
      "Reconstruction loss: 254.6739950404537, KL divergence: 0.11048963199875511\n",
      "Reconstruction loss: 163.85591850499532, KL divergence: 3.5865548053446297\n",
      "Reconstruction loss: 201.0346141287057, KL divergence: 0.1981106618909566\n",
      "Reconstruction loss: 126.8962918446147, KL divergence: 0.5450799765743264\n",
      "Reconstruction loss: 232.55186524185794, KL divergence: 0.19604558671891653\n",
      "Reconstruction loss: 131.19758207252337, KL divergence: 0.7620362094935277\n",
      "Reconstruction loss: 161.16256838923653, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 150.220566942399, KL divergence: 0.27118789905235013\n",
      "Reconstruction loss: 221.13357206897444, KL divergence: 0.04536070661269592\n",
      "Reconstruction loss: 193.884081783134, KL divergence: 0.48196499023256095\n",
      "Reconstruction loss: 155.57520075620846, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 202.90818310070057, KL divergence: 0.2699452313541139\n",
      "Reconstruction loss: 259.0334804167309, KL divergence: 0.7669274489618461\n",
      "Reconstruction loss: 289.03188235212156, KL divergence: 0.05358743407084321\n",
      "Reconstruction loss: 209.61110240394055, KL divergence: 0.23235304794235256\n",
      "Reconstruction loss: 158.52757292264846, KL divergence: 0.46775734384212425\n",
      "Reconstruction loss: 250.78761537739442, KL divergence: 0.04915204258781969\n",
      "Reconstruction loss: 192.9460917156025, KL divergence: 0.04346362252083413\n",
      "Reconstruction loss: 242.89485481519392, KL divergence: 0.034057574136178825\n",
      "Reconstruction loss: 183.6891061113049, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 260.4178518153384, KL divergence: 0.21946437411276365\n",
      "Reconstruction loss: 283.33553784156993, KL divergence: 1.0113356769047837\n",
      "Reconstruction loss: 207.5175139968021, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 327.96629689301585, KL divergence: 1.1748613355852375\n",
      "Reconstruction loss: 204.89629635656715, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 215.1721012725622, KL divergence: 0.40009931955945877\n",
      "Reconstruction loss: 198.56685869837582, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 360.8801051385474, KL divergence: 0.17450450628465286\n",
      "Reconstruction loss: 275.4953762742236, KL divergence: 0.0367359586668517\n",
      "Reconstruction loss: 164.45427408028343, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 90.37809026987152, KL divergence: 0.8987580583860673\n",
      "Reconstruction loss: 229.95642151418218, KL divergence: 2.263686359807383\n",
      "Reconstruction loss: 167.15963301586027, KL divergence: 0.3743688084958\n",
      "Reconstruction loss: 241.22545251979864, KL divergence: 0.30493165805628825\n",
      "Reconstruction loss: 196.17181391606758, KL divergence: 1.241078787808498\n",
      "Reconstruction loss: 209.29213331548078, KL divergence: 0.19953203324775176\n",
      "Reconstruction loss: 260.1233254544395, KL divergence: 0.8619440818906551\n",
      "Reconstruction loss: 125.77533048126551, KL divergence: 0.4917238531244728\n",
      "Reconstruction loss: 102.45585842457322, KL divergence: 0.6412374413977484\n",
      "Reconstruction loss: 169.68034131718437, KL divergence: 0.3317920663455843\n",
      "Reconstruction loss: 169.2956481216853, KL divergence: 0.2239065268249456\n",
      "Reconstruction loss: 151.2013372513306, KL divergence: 0.47128973688612774\n",
      "Reconstruction loss: 202.54240146905477, KL divergence: 0.19901246691312457\n",
      "Reconstruction loss: 202.46508368544954, KL divergence: 0.8527047380643733\n",
      "Reconstruction loss: 120.71756055575919, KL divergence: 1.1196746687040446\n",
      "Reconstruction loss: 107.23118388943112, KL divergence: 1.1858947397511703\n",
      "Reconstruction loss: 171.90533196501113, KL divergence: 0.19901246691312457\n",
      "Reconstruction loss: 221.1123634644284, KL divergence: 0.10826691512519776\n",
      "Reconstruction loss: 158.54503450727498, KL divergence: 0.5881217829113119\n",
      "Reconstruction loss: 249.58602968899197, KL divergence: 4.566706653651469\n",
      "Reconstruction loss: 189.5943852381429, KL divergence: 0.11172317569935863\n",
      "Reconstruction loss: 218.92995384747786, KL divergence: 0.09367780363403228\n",
      "Reconstruction loss: 131.8667256762441, KL divergence: 0.5029745361086477\n",
      "Reconstruction loss: 161.57919672227646, KL divergence: 0.9389222933466321\n",
      "Reconstruction loss: 277.1614002714223, KL divergence: 0.03641524915271738\n",
      "Reconstruction loss: 196.10940839817744, KL divergence: 0.4230667821180511\n",
      "Reconstruction loss: 153.80568295687098, KL divergence: 0.19901246691312457\n",
      "Reconstruction loss: 196.25723785847697, KL divergence: 0.18257638270095145\n",
      "Reconstruction loss: 284.17174350898847, KL divergence: 3.351548971326479\n",
      "Reconstruction loss: 198.69996040244803, KL divergence: 0.05044197140190698\n",
      "Reconstruction loss: 322.00628379450643, KL divergence: 0.24585500704024038\n",
      "Reconstruction loss: 222.11689143694, KL divergence: 0.03370158135783502\n",
      "Reconstruction loss: 332.37262417786087, KL divergence: 1.429705050724917\n",
      "Reconstruction loss: 176.05265684260638, KL divergence: 0.19901246691312457\n",
      "Reconstruction loss: 162.93108192034902, KL divergence: 0.33277557573839706\n",
      "Reconstruction loss: 154.63036807453847, KL divergence: 0.19901246691312457\n",
      "Reconstruction loss: 298.76210145684905, KL divergence: 0.03441317333392996\n",
      "Reconstruction loss: 155.14927294220468, KL divergence: 0.19901246691312457\n",
      "Reconstruction loss: 174.33446942127046, KL divergence: 0.19597502135310563\n",
      "Reconstruction loss: 148.040183308822, KL divergence: 0.21009384513475232\n",
      "Reconstruction loss: 215.79004858851795, KL divergence: 0.13834490073703198\n",
      "Reconstruction loss: 172.3526679199889, KL divergence: 0.19742488820217163\n",
      "Reconstruction loss: 169.0609029026898, KL divergence: 0.18928919694137963\n",
      "Reconstruction loss: 166.70103726024809, KL divergence: 0.11848729911484862\n",
      "Reconstruction loss: 150.04390639214105, KL divergence: 0.19742488820217163\n",
      "Reconstruction loss: 151.01884410576122, KL divergence: 0.07346939085612059\n",
      "Reconstruction loss: 219.75706133073436, KL divergence: 0.08507174136678791\n",
      "Reconstruction loss: 178.12155461926773, KL divergence: 0.04193081710390939\n",
      "Reconstruction loss: 208.17916347659042, KL divergence: 4.0350804831460945\n",
      "Reconstruction loss: 173.91274849344623, KL divergence: 0.19742488820217163\n",
      "Reconstruction loss: 145.61338049138215, KL divergence: 0.20353930580858443\n",
      "Reconstruction loss: 204.25087885263054, KL divergence: 0.12081406155596897\n",
      "Reconstruction loss: 215.52567562298168, KL divergence: 0.03602804269479526\n",
      "Reconstruction loss: 245.58526135977843, KL divergence: 1.9411487422354186\n",
      "Reconstruction loss: 214.26574413770504, KL divergence: 3.8135190715901457\n",
      "Reconstruction loss: 170.81849751788695, KL divergence: 0.6049833912807601\n",
      "Reconstruction loss: 128.430450418225, KL divergence: 1.1806694606541641\n",
      "Reconstruction loss: 369.70554864902005, KL divergence: 0.8894643473974613\n",
      "Reconstruction loss: 226.47763357030752, KL divergence: 0.18708326861166324\n",
      "Reconstruction loss: 153.09181127207273, KL divergence: 0.6326641959943575\n",
      "Reconstruction loss: 268.40534765767245, KL divergence: 0.19742488820217163\n",
      "Reconstruction loss: 152.82604334905517, KL divergence: 0.5505762813232296\n",
      "Reconstruction loss: 259.33870629327816, KL divergence: 1.2376305641456167\n",
      "Reconstruction loss: 279.96296052700177, KL divergence: 0.08799255736753692\n",
      "Reconstruction loss: 261.58134041238435, KL divergence: 5.107586435552895\n",
      "Reconstruction loss: 164.59102102078458, KL divergence: 0.19742488820217163\n",
      "Reconstruction loss: 170.92073736608563, KL divergence: 0.3914285800630076\n",
      "Reconstruction loss: 257.2953599767983, KL divergence: 0.8460523200710786\n",
      "Reconstruction loss: 196.77048055638352, KL divergence: 0.03336287741028904\n",
      "Reconstruction loss: 158.24276654457697, KL divergence: 0.4592437257210888\n",
      "Reconstruction loss: 174.19732627353198, KL divergence: 0.17624376597059066\n",
      "Reconstruction loss: 251.52148426791152, KL divergence: 0.2870641345362481\n",
      "Reconstruction loss: 180.39739990604448, KL divergence: 0.19499350726290188\n",
      "Reconstruction loss: 150.8519975360222, KL divergence: 0.31936213579571626\n",
      "Reconstruction loss: 203.50856089754774, KL divergence: 0.2835980347878022\n",
      "Reconstruction loss: 194.95938745071845, KL divergence: 5.555974051731281\n",
      "Reconstruction loss: 238.61532382902618, KL divergence: 4.577004675941997\n",
      "Reconstruction loss: 191.54603067184968, KL divergence: 0.03667454804799486\n",
      "Reconstruction loss: 118.24169240068056, KL divergence: 0.7786398898651252\n",
      "Reconstruction loss: 157.85969679330663, KL divergence: 0.40370723096333144\n",
      "Reconstruction loss: 209.00354352126814, KL divergence: 0.19282096017607808\n",
      "Reconstruction loss: 238.97988386802623, KL divergence: 0.06444056954181526\n",
      "Reconstruction loss: 181.94787594261757, KL divergence: 0.09652281872063001\n",
      "Reconstruction loss: 194.77296266377863, KL divergence: 0.08744119284490687\n",
      "Reconstruction loss: 169.89331849208622, KL divergence: 0.036673937749247876\n",
      "Reconstruction loss: 186.9049538450402, KL divergence: 0.19499350726290188\n",
      "Reconstruction loss: 174.03807373202488, KL divergence: 0.19499350726290188\n",
      "Reconstruction loss: 203.26884488940325, KL divergence: 0.19499350726290188\n",
      "Reconstruction loss: 274.13123909025796, KL divergence: 0.04663900621199385\n",
      "Reconstruction loss: 193.4881528751382, KL divergence: 4.186688686317941\n",
      "Reconstruction loss: 186.29558875801277, KL divergence: 0.032809103819357366\n",
      "Reconstruction loss: 118.98355334298768, KL divergence: 1.0221793159414698\n",
      "Reconstruction loss: 273.2103101636741, KL divergence: 1.6191337772984031\n",
      "Reconstruction loss: 274.3266153469527, KL divergence: 0.22725104369033367\n",
      "Reconstruction loss: 223.22715506196164, KL divergence: 0.2259506316814915\n",
      "Reconstruction loss: 220.08790784354414, KL divergence: 0.06244943049041235\n",
      "Reconstruction loss: 124.18169414493954, KL divergence: 0.4677681095310341\n",
      "Reconstruction loss: 151.72294341880428, KL divergence: 0.19499350726290188\n",
      "Reconstruction loss: 268.1891922840976, KL divergence: 0.15866699751786595\n",
      "Reconstruction loss: 163.7667118145737, KL divergence: 0.19499350726290188\n",
      "Reconstruction loss: 127.79349755139616, KL divergence: 1.2075815784789654\n",
      "Reconstruction loss: 189.27497117503086, KL divergence: 0.07785300789549304\n",
      "Reconstruction loss: 268.0688803053524, KL divergence: 0.22890499712827977\n",
      "Reconstruction loss: 200.58508797669032, KL divergence: 0.10844236530369156\n",
      "Reconstruction loss: 197.78849028698778, KL divergence: 7.613092612891532\n",
      "Reconstruction loss: 128.60834380978451, KL divergence: 0.19312649544709937\n",
      "Reconstruction loss: 238.9675694253496, KL divergence: 0.12830949137003284\n",
      "Reconstruction loss: 168.96274823629452, KL divergence: 0.4736833456363524\n",
      "Reconstruction loss: 176.63427758530253, KL divergence: 1.2493758725888853\n",
      "Reconstruction loss: 296.4196299431255, KL divergence: 0.24004422307422818\n",
      "Reconstruction loss: 251.47648102228598, KL divergence: 0.7944434113941431\n",
      "Reconstruction loss: 248.79236312130757, KL divergence: 0.2710977791581926\n",
      "Reconstruction loss: 239.76197566739503, KL divergence: 0.3018747010707151\n",
      "Reconstruction loss: 265.69483340851093, KL divergence: 0.42882732226888975\n",
      "Reconstruction loss: 184.67578780394743, KL divergence: 0.15900134677831979\n",
      "Reconstruction loss: 253.70007889367508, KL divergence: 2.6659562527241296\n",
      "Reconstruction loss: 238.25525337636714, KL divergence: 0.5708408661728004\n",
      "Reconstruction loss: 176.21204872824447, KL divergence: 0.12213779989543339\n",
      "Reconstruction loss: 126.67681285877431, KL divergence: 0.5826130467874877\n",
      "Reconstruction loss: 198.83123922477125, KL divergence: 0.22898090039428076\n",
      "Reconstruction loss: 264.2514836094989, KL divergence: 1.2036691771966566\n",
      "Reconstruction loss: 231.13669801075662, KL divergence: 4.749641491944243\n",
      "Reconstruction loss: 206.66718256880228, KL divergence: 1.0016801221936324\n",
      "Reconstruction loss: 129.19615860681301, KL divergence: 0.3754809089566209\n",
      "Reconstruction loss: 151.59189812153932, KL divergence: 0.2951012452695906\n",
      "Reconstruction loss: 198.72914314068612, KL divergence: 0.19312649544709937\n",
      "Reconstruction loss: 185.92059224047063, KL divergence: 0.08254128748792944\n",
      "Reconstruction loss: 204.89799118466107, KL divergence: 0.03814030977089894\n",
      "Reconstruction loss: 191.8483686167095, KL divergence: 0.21949574417657342\n",
      "Reconstruction loss: 112.84381176214296, KL divergence: 0.8306489832157706\n",
      "Reconstruction loss: 133.60777110047036, KL divergence: 0.19312649544709937\n",
      "Reconstruction loss: 161.98591330920638, KL divergence: 0.19312649544709937\n",
      "Reconstruction loss: 359.50767544807894, KL divergence: 0.15691882472480317\n",
      "Reconstruction loss: 187.37428249196927, KL divergence: 0.19312649544709937\n",
      "Reconstruction loss: 184.49773531761997, KL divergence: 0.16008375970032357\n",
      "Reconstruction loss: 141.9653875210264, KL divergence: 0.6239899544907768\n",
      "Reconstruction loss: 224.08559400316904, KL divergence: 2.64617103238951\n",
      "Reconstruction loss: 196.9420467810283, KL divergence: 0.08624298574970785\n",
      "Reconstruction loss: 147.0111379682748, KL divergence: 0.5671916607089807\n",
      "Reconstruction loss: 206.08073564938883, KL divergence: 0.06100328983076175\n",
      "Reconstruction loss: 192.98372866757052, KL divergence: 0.17669898210726886\n",
      "Reconstruction loss: 336.027844896717, KL divergence: 1.9623705524412558\n",
      "Reconstruction loss: 125.02257632659884, KL divergence: 0.42766744095298137\n",
      "Reconstruction loss: 192.43001107096848, KL divergence: 0.03298644014266128\n",
      "Reconstruction loss: 179.5429343301618, KL divergence: 0.19288272118872807\n",
      "Reconstruction loss: 147.1033637919273, KL divergence: 0.05377517687617994\n",
      "Reconstruction loss: 277.2558810056996, KL divergence: 1.2443830193452157\n",
      "Reconstruction loss: 170.31138929211826, KL divergence: 0.19288272118872807\n",
      "Reconstruction loss: 214.3284637832482, KL divergence: 0.08609376019330578\n",
      "Reconstruction loss: 159.298721667723, KL divergence: 8.315877172215533\n",
      "Reconstruction loss: 278.81387015879943, KL divergence: 4.12254720202962\n",
      "Reconstruction loss: 231.29943672878676, KL divergence: 0.16239768617794764\n",
      "Reconstruction loss: 250.57361058192816, KL divergence: 1.6770394857267963\n",
      "Reconstruction loss: 234.28454965639742, KL divergence: 0.43038374750226155\n",
      "Reconstruction loss: 130.86114996758923, KL divergence: 0.7906002677130282\n",
      "Reconstruction loss: 244.46960171594208, KL divergence: 0.8480216341078052\n",
      "Reconstruction loss: 184.32971350663007, KL divergence: 0.16248996769524726\n",
      "Reconstruction loss: 314.3786276843979, KL divergence: 0.6860401662130251\n",
      "Reconstruction loss: 131.59687528550725, KL divergence: 0.6977770193333224\n",
      "Reconstruction loss: 266.95047650895367, KL divergence: 0.5604267560811373\n",
      "Reconstruction loss: 104.76121600323009, KL divergence: 1.3550284929631804\n",
      "Reconstruction loss: 307.09484748860626, KL divergence: 0.9770689811654738\n",
      "Reconstruction loss: 176.8518459394657, KL divergence: 0.09454979039759148\n",
      "Reconstruction loss: 228.99195809409326, KL divergence: 0.2531779744222604\n",
      "Reconstruction loss: 162.5985173847868, KL divergence: 0.19077687229009188\n",
      "Reconstruction loss: 187.76729650547745, KL divergence: 0.19288272118872807\n",
      "Reconstruction loss: 167.35824051966227, KL divergence: 0.4779250577840754\n",
      "Reconstruction loss: 174.5252399258929, KL divergence: 0.19398864389124876\n",
      "Reconstruction loss: 236.42429041828996, KL divergence: 0.23417702471812096\n",
      "Reconstruction loss: 199.61812269618127, KL divergence: 0.0948751039879448\n",
      "Reconstruction loss: 271.5898850364425, KL divergence: 0.8399486657270128\n",
      "Reconstruction loss: 260.2022327059337, KL divergence: 0.036878094004290596\n",
      "Reconstruction loss: 176.98233708837571, KL divergence: 0.1940485061358253\n",
      "Reconstruction loss: 246.97276103309795, KL divergence: 2.2769964389868695\n",
      "Reconstruction loss: 131.37348830705503, KL divergence: 0.22822678396687823\n",
      "Reconstruction loss: 212.88677629742733, KL divergence: 0.1940485061358253\n",
      "Reconstruction loss: 280.57969684623845, KL divergence: 1.2358141753236664\n",
      "Reconstruction loss: 301.926478749236, KL divergence: 1.1161255736709332\n",
      "Reconstruction loss: 220.02556374324524, KL divergence: 0.5458028422146706\n",
      "Reconstruction loss: 211.61339724130482, KL divergence: 0.17087015402517197\n",
      "Reconstruction loss: 261.6858687848345, KL divergence: 0.21902560373005558\n",
      "Reconstruction loss: 197.78281522257677, KL divergence: 0.059248453847389804\n",
      "Reconstruction loss: 163.46104792731978, KL divergence: 0.16733297312367734\n",
      "Reconstruction loss: 287.78257259091527, KL divergence: 0.45810713127449243\n",
      "Reconstruction loss: 161.23983826264762, KL divergence: 1.1202048954699815\n",
      "Reconstruction loss: 234.5553906575778, KL divergence: 5.111908787225551\n",
      "Reconstruction loss: 255.95677343722718, KL divergence: 0.1002519153291292\n",
      "Reconstruction loss: 163.57864932391954, KL divergence: 0.22356842921810194\n",
      "Reconstruction loss: 272.67741802146946, KL divergence: 0.1338493555907801\n",
      "Reconstruction loss: 200.55142597369496, KL divergence: 3.7474892539274185\n",
      "Reconstruction loss: 185.5798798629821, KL divergence: 0.14116277604605731\n",
      "Reconstruction loss: 227.0156341475573, KL divergence: 0.03390715457572813\n",
      "Reconstruction loss: 130.20913932870064, KL divergence: 1.0934963989063002\n",
      "Reconstruction loss: 208.9091698539185, KL divergence: 0.055445846967333734\n",
      "Reconstruction loss: 231.40782407613148, KL divergence: 1.2295201413053296\n",
      "Reconstruction loss: 186.01395102561668, KL divergence: 5.790242793654221\n",
      "Reconstruction loss: 144.3739096578487, KL divergence: 0.46905464587438966\n",
      "Reconstruction loss: 211.0464976201526, KL divergence: 0.20598762844398122\n",
      "Reconstruction loss: 247.84231517010332, KL divergence: 0.5354453464875673\n",
      "Reconstruction loss: 205.37033332464256, KL divergence: 2.250552581112779\n",
      "Reconstruction loss: 212.13347205344593, KL divergence: 0.15873892601137946\n",
      "Reconstruction loss: 147.49060985181313, KL divergence: 0.1969885923758498\n",
      "Reconstruction loss: 191.2070149967637, KL divergence: 0.1969307914005084\n",
      "Reconstruction loss: 227.4205183825939, KL divergence: 0.7440829065706532\n",
      "Reconstruction loss: 178.57679201305223, KL divergence: 0.09859814817805823\n",
      "Reconstruction loss: 263.9890997634941, KL divergence: 0.12003111491024415\n",
      "Reconstruction loss: 139.8169610087104, KL divergence: 1.0481410347328457\n",
      "Reconstruction loss: 190.17924963194582, KL divergence: 0.03382768711624495\n",
      "Reconstruction loss: 231.80117672751766, KL divergence: 0.036187886759884325\n",
      "Reconstruction loss: 220.08294495845433, KL divergence: 0.03403313172540284\n",
      "Reconstruction loss: 120.49855938702987, KL divergence: 0.5778188776368677\n",
      "Reconstruction loss: 214.7907818947785, KL divergence: 0.28418729761045614\n",
      "Reconstruction loss: 196.68954826077487, KL divergence: 4.194096272800161\n",
      "Reconstruction loss: 199.0796106899046, KL divergence: 0.03862014104568906\n",
      "Reconstruction loss: 191.59224273631267, KL divergence: 0.15534093637459312\n",
      "Reconstruction loss: 201.32526911362737, KL divergence: 0.32722558628783244\n",
      "Reconstruction loss: 242.72892799202376, KL divergence: 0.3108464808790098\n",
      "Reconstruction loss: 225.9692605954359, KL divergence: 0.03404261767884076\n",
      "Reconstruction loss: 168.06398395068024, KL divergence: 0.21943401702668253\n",
      "Reconstruction loss: 163.54090810555027, KL divergence: 0.1969885923758498\n",
      "Reconstruction loss: 162.8137667848958, KL divergence: 0.24338272529266297\n",
      "Reconstruction loss: 172.0172747410926, KL divergence: 0.5539273135580076\n",
      "Reconstruction loss: 228.70068823391318, KL divergence: 0.2716266991386429\n",
      "Reconstruction loss: 264.6991973347889, KL divergence: 4.157846048221838\n",
      "Reconstruction loss: 158.70959566729027, KL divergence: 0.1969885923758498\n",
      "Reconstruction loss: 234.76538339752824, KL divergence: 0.7968343845287451\n",
      "Reconstruction loss: 236.58745766450687, KL divergence: 0.048177679921700745\n",
      "Reconstruction loss: 171.77235159013173, KL divergence: 0.04529712470368619\n",
      "Reconstruction loss: 145.6960347006363, KL divergence: 0.26785128902663535\n",
      "Reconstruction loss: 137.14427661638803, KL divergence: 0.31772955654255247\n",
      "Reconstruction loss: 200.88740703144197, KL divergence: 0.041805220878265104\n",
      "Reconstruction loss: 183.45466918988723, KL divergence: 0.05630569126553697\n",
      "Reconstruction loss: 181.79241083595758, KL divergence: 0.15347182480625715\n",
      "Reconstruction loss: 180.67499988906053, KL divergence: 0.03979025844747591\n",
      "Reconstruction loss: 161.7001767453561, KL divergence: 0.21541347745211492\n",
      "Reconstruction loss: 143.6428560573035, KL divergence: 0.3144511675504649\n",
      "Reconstruction loss: 232.94343385667844, KL divergence: 0.045315064039295305\n",
      "Reconstruction loss: 120.63430458786304, KL divergence: 0.6091899412175537\n",
      "Reconstruction loss: 259.7303692776022, KL divergence: 0.03514479900822087\n",
      "Reconstruction loss: 129.05217384196766, KL divergence: 0.9941655798470038\n",
      "Reconstruction loss: 151.27967830308006, KL divergence: 1.0067093003855048\n",
      "Reconstruction loss: 195.92916155579925, KL divergence: 4.957563819017474\n",
      "Reconstruction loss: 183.97916900563638, KL divergence: 5.821301004122213\n",
      "Reconstruction loss: 229.3068429288583, KL divergence: 0.12573257934737647\n",
      "Reconstruction loss: 127.94708287523163, KL divergence: 0.27365285051299293\n",
      "Reconstruction loss: 224.32916445232456, KL divergence: 0.20232112705159938\n",
      "Reconstruction loss: 195.15902797910712, KL divergence: 0.14990321554580477\n",
      "Reconstruction loss: 212.69068063004127, KL divergence: 0.1653289849065695\n",
      "Reconstruction loss: 280.5310430548207, KL divergence: 0.8586909338757919\n",
      "Reconstruction loss: 221.58011572555318, KL divergence: 1.0513147451925913\n",
      "Reconstruction loss: 175.26245018272215, KL divergence: 0.20047911795705925\n",
      "Reconstruction loss: 137.84904671360968, KL divergence: 0.6089990656154107\n",
      "Reconstruction loss: 117.07342997974217, KL divergence: 1.1296301559612574\n",
      "Reconstruction loss: 273.1684380075435, KL divergence: 0.1637207360664047\n",
      "Reconstruction loss: 199.23932002895913, KL divergence: 0.2078330648271377\n",
      "Reconstruction loss: 202.9521765959036, KL divergence: 0.22620412074710744\n",
      "Reconstruction loss: 331.73102960954407, KL divergence: 1.4888538247324759\n",
      "Reconstruction loss: 250.17295054328827, KL divergence: 1.101925069612948\n",
      "Reconstruction loss: 146.04148616315524, KL divergence: 0.36935003143899636\n",
      "Reconstruction loss: 196.50312419289975, KL divergence: 0.05728065344280481\n",
      "Reconstruction loss: 144.6914383744643, KL divergence: 1.1730916823427087\n",
      "Reconstruction loss: 233.3525820072569, KL divergence: 0.1853085524007707\n",
      "Reconstruction loss: 231.50379918875592, KL divergence: 0.8487543751885636\n",
      "Reconstruction loss: 104.08074770389766, KL divergence: 0.7411851452253428\n",
      "Reconstruction loss: 232.70524758311473, KL divergence: 0.2037354418220244\n",
      "Reconstruction loss: 320.7971889670657, KL divergence: 0.08416541937047661\n",
      "Reconstruction loss: 251.3957850907904, KL divergence: 0.3427478112004779\n",
      "Reconstruction loss: 325.92317531235346, KL divergence: 0.43547999432460316\n",
      "Reconstruction loss: 296.1397686485467, KL divergence: 0.27280441068316436\n",
      "Reconstruction loss: 202.89614715493644, KL divergence: 0.22113926589920763\n",
      "Reconstruction loss: 149.6344346010073, KL divergence: 0.2037354418220244\n",
      "Reconstruction loss: 224.45076727933335, KL divergence: 0.2037354418220244\n",
      "Reconstruction loss: 392.29708989880265, KL divergence: 1.4873350713871052\n",
      "Reconstruction loss: 218.8515784147521, KL divergence: 0.06269194084370716\n",
      "Reconstruction loss: 228.26455175050728, KL divergence: 0.0639128507898124\n",
      "Reconstruction loss: 122.64758342127315, KL divergence: 1.0290128372209963\n",
      "Reconstruction loss: 221.49207451528883, KL divergence: 0.10031260053839669\n",
      "Reconstruction loss: 254.53682147160396, KL divergence: 0.8454661963150676\n",
      "Reconstruction loss: 212.40555218847874, KL divergence: 0.3459988908666508\n",
      "Reconstruction loss: 217.38765038358258, KL divergence: 0.14215908035339897\n",
      "Reconstruction loss: 284.5694460675895, KL divergence: 1.3731380595156737\n",
      "Reconstruction loss: 293.21224569845, KL divergence: 0.28267071204224603\n",
      "Reconstruction loss: 169.9969030026453, KL divergence: 0.20716302699254596\n",
      "Reconstruction loss: 283.24835881830074, KL divergence: 0.04034420856730392\n",
      "Reconstruction loss: 249.73225198231518, KL divergence: 0.893143166217687\n",
      "Reconstruction loss: 170.62787458171647, KL divergence: 0.2037354418220244\n",
      "Reconstruction loss: 110.10504542632101, KL divergence: 1.0615843630068889\n",
      "Reconstruction loss: 234.21448373222594, KL divergence: 3.158450050650803\n",
      "Reconstruction loss: 238.58011780983873, KL divergence: 1.5255843348065126\n",
      "Reconstruction loss: 198.24479950294688, KL divergence: 0.08749180480451335\n",
      "Reconstruction loss: 116.3880917754351, KL divergence: 1.3834154289306664\n",
      "Reconstruction loss: 198.79995838701973, KL divergence: 3.8043292300840497\n",
      "Reconstruction loss: 204.98773804076922, KL divergence: 0.2140760510599647\n",
      "Reconstruction loss: 206.40150623446135, KL divergence: 0.14665154431236094\n",
      "Reconstruction loss: 194.17039967597313, KL divergence: 0.23987472757300776\n",
      "Reconstruction loss: 261.551963137572, KL divergence: 0.03638345085743344\n",
      "Reconstruction loss: 303.20434445187755, KL divergence: 1.1186342712021222\n",
      "Reconstruction loss: 152.1819738870358, KL divergence: 0.24109238225774937\n",
      "Reconstruction loss: 112.90342655664611, KL divergence: 0.8827906541129292\n",
      "Reconstruction loss: 206.15858991742047, KL divergence: 0.07264533477626356\n",
      "Reconstruction loss: 179.16949785714456, KL divergence: 0.20589791729614637\n",
      "Reconstruction loss: 222.78420418629338, KL divergence: 0.10161791531120845\n",
      "Reconstruction loss: 320.21285622580535, KL divergence: 0.3323177795719347\n",
      "Reconstruction loss: 219.00862802861417, KL divergence: 0.03798675280918479\n",
      "Reconstruction loss: 225.4882542618974, KL divergence: 0.06231314954168288\n",
      "Reconstruction loss: 272.82103155024913, KL divergence: 0.6146035796047816\n",
      "Reconstruction loss: 208.8200643587228, KL divergence: 0.7628777568614543\n",
      "Reconstruction loss: 349.5952950530759, KL divergence: 2.3351082274330355\n",
      "Reconstruction loss: 152.6221570838159, KL divergence: 0.32455802496884356\n",
      "Reconstruction loss: 299.5149661503408, KL divergence: 1.3175712104079336\n",
      "Reconstruction loss: 253.1686806292413, KL divergence: 0.7459789704196058\n",
      "Reconstruction loss: 157.74586698721703, KL divergence: 0.20598482996035633\n",
      "Reconstruction loss: 157.87253041433019, KL divergence: 0.5804585595788718\n",
      "Reconstruction loss: 181.47915954305915, KL divergence: 0.26871700116895214\n",
      "Reconstruction loss: 323.6765143218885, KL divergence: 0.4925650577701677\n",
      "Reconstruction loss: 238.09673025046874, KL divergence: 0.07330911017556141\n",
      "Reconstruction loss: 212.77424771825406, KL divergence: 1.605035318668663\n",
      "Reconstruction loss: 228.68941845387562, KL divergence: 3.590361480082165\n",
      "Reconstruction loss: 216.5870276926765, KL divergence: 3.2631263087205458\n",
      "Reconstruction loss: 147.4855848075338, KL divergence: 0.5428226961041384\n",
      "Reconstruction loss: 261.33559545814495, KL divergence: 1.8952807637903275\n",
      "Reconstruction loss: 144.7440106286077, KL divergence: 0.6370762943646449\n",
      "Reconstruction loss: 238.15778978982414, KL divergence: 0.03726124907972056\n",
      "Reconstruction loss: 193.2142632189184, KL divergence: 0.6244023018994851\n",
      "Reconstruction loss: 244.6567840124897, KL divergence: 0.05326735270965943\n",
      "Reconstruction loss: 246.21180787587605, KL divergence: 0.3632519528256346\n",
      "Reconstruction loss: 249.72925887903767, KL divergence: 0.04960040708875013\n",
      "Reconstruction loss: 189.2940564738612, KL divergence: 0.10711255985147683\n",
      "Reconstruction loss: 158.50187102317778, KL divergence: 0.2051610384740425\n",
      "Reconstruction loss: 177.1895137173229, KL divergence: 0.2065282662112849\n",
      "Reconstruction loss: 193.55668172724216, KL divergence: 0.2065282662112849\n",
      "Reconstruction loss: 258.4827099760552, KL divergence: 0.2190650755006774\n",
      "Reconstruction loss: 184.2922965512828, KL divergence: 0.025756059113149432\n",
      "Reconstruction loss: 286.0976495519804, KL divergence: 0.10379313707273125\n",
      "Reconstruction loss: 188.89430621113831, KL divergence: 0.09749021529851482\n",
      "Reconstruction loss: 148.843034600188, KL divergence: 0.2065282662112849\n",
      "Reconstruction loss: 218.8040976537558, KL divergence: 0.1202533540647795\n",
      "Reconstruction loss: 215.26678845946688, KL divergence: 0.8219581595467285\n",
      "Reconstruction loss: 229.2791873491696, KL divergence: 5.806675552179298\n",
      "Reconstruction loss: 123.41440724674517, KL divergence: 0.8372974760637228\n",
      "Reconstruction loss: 147.05715961639288, KL divergence: 0.39917722064967553\n",
      "Reconstruction loss: 164.00655189848752, KL divergence: 0.358634749645533\n",
      "Reconstruction loss: 181.68824158578497, KL divergence: 0.2065282662112849\n",
      "Reconstruction loss: 163.96620035025143, KL divergence: 0.2065282662112849\n",
      "Reconstruction loss: 216.43629131917618, KL divergence: 0.5189593277498749\n",
      "Reconstruction loss: 204.83516731568932, KL divergence: 0.38809590919875864\n",
      "Reconstruction loss: 283.16658794282955, KL divergence: 0.05760104800483856\n",
      "Reconstruction loss: 276.83690055274644, KL divergence: 0.20702631369592994\n",
      "Reconstruction loss: 278.9837391988027, KL divergence: 0.04059159264464329\n",
      "Reconstruction loss: 229.10826813719837, KL divergence: 0.1591081314352208\n",
      "Reconstruction loss: 175.80960947661075, KL divergence: 0.2064346609113945\n",
      "Reconstruction loss: 152.16558000673277, KL divergence: 0.2718752665955483\n",
      "Reconstruction loss: 146.19827246768415, KL divergence: 1.241413056628867\n",
      "Reconstruction loss: 167.10107256939222, KL divergence: 0.2065282662112849\n",
      "Reconstruction loss: 145.56574553539156, KL divergence: 0.26875906524973664\n",
      "Reconstruction loss: 234.1542238352237, KL divergence: 2.879763496446847\n",
      "Reconstruction loss: 231.04950123596697, KL divergence: 0.39032271673642\n",
      "Reconstruction loss: 217.93741595461785, KL divergence: 0.034733184382733484\n",
      "Reconstruction loss: 140.55806512345927, KL divergence: 0.337329950969938\n",
      "Reconstruction loss: 203.1730045160054, KL divergence: 0.37981893605153466\n",
      "Reconstruction loss: 188.89843733619455, KL divergence: 0.20773352599183958\n",
      "Reconstruction loss: 133.30374000553002, KL divergence: 0.6032538859913775\n",
      "Reconstruction loss: 189.3309925748086, KL divergence: 0.20773352599183958\n",
      "Reconstruction loss: 216.89467194563784, KL divergence: 0.18162687056736543\n",
      "Reconstruction loss: 142.88262562067345, KL divergence: 0.9311656642980843\n",
      "Reconstruction loss: 211.98744983949533, KL divergence: 0.21022382938741385\n",
      "Reconstruction loss: 188.5160925652136, KL divergence: 0.20773352599183958\n",
      "Reconstruction loss: 300.2071728547195, KL divergence: 0.058460803894883395\n",
      "Reconstruction loss: 208.02598918861315, KL divergence: 4.459081069924534\n",
      "Reconstruction loss: 212.30873055661732, KL divergence: 0.19822527379108446\n",
      "Reconstruction loss: 133.60623898814285, KL divergence: 1.2183413084067705\n",
      "Reconstruction loss: 141.53255363185298, KL divergence: 0.9503543374219433\n",
      "Reconstruction loss: 140.7806026393618, KL divergence: 0.5608539436247302\n",
      "Reconstruction loss: 153.19334282065984, KL divergence: 0.2131189086355692\n",
      "Reconstruction loss: 145.9280045358438, KL divergence: 0.21412536202854549\n",
      "Reconstruction loss: 204.81188701364923, KL divergence: 0.24064075763764853\n",
      "Reconstruction loss: 253.60635176186443, KL divergence: 0.05182022394647934\n",
      "Reconstruction loss: 217.39880497296087, KL divergence: 0.04339664038828417\n",
      "Reconstruction loss: 229.81889848181123, KL divergence: 4.783671302162189\n",
      "Reconstruction loss: 290.8410754311327, KL divergence: 0.6158053728662598\n",
      "Reconstruction loss: 282.6037038879205, KL divergence: 1.6798589053937225\n",
      "Reconstruction loss: 154.64971514672214, KL divergence: 0.4224426241796438\n",
      "Reconstruction loss: 141.09533315030734, KL divergence: 0.7672450141038929\n",
      "Reconstruction loss: 261.5585006064933, KL divergence: 0.03868225565883793\n",
      "Reconstruction loss: 235.6199850211395, KL divergence: 1.8718266662976424\n",
      "Reconstruction loss: 327.35044977373286, KL divergence: 0.911408564661456\n",
      "Reconstruction loss: 229.13556569661853, KL divergence: 0.050902065905933325\n",
      "Reconstruction loss: 109.05212153524879, KL divergence: 1.431733106321238\n",
      "Reconstruction loss: 249.09031957868257, KL divergence: 0.3220051594898656\n",
      "Reconstruction loss: 278.0220325866899, KL divergence: 1.7702659971569994\n",
      "Reconstruction loss: 130.23387571733593, KL divergence: 0.8335529364219199\n",
      "Reconstruction loss: 225.1167701432724, KL divergence: 0.03991450227627463\n",
      "Reconstruction loss: 276.9729216588553, KL divergence: 0.08346741228098958\n",
      "Reconstruction loss: 194.12915064718968, KL divergence: 0.04017590809449634\n",
      "Reconstruction loss: 132.3113978586061, KL divergence: 1.578796798714255\n",
      "Reconstruction loss: 272.36611270125655, KL divergence: 1.8112187595011453\n",
      "Reconstruction loss: 234.59887906219663, KL divergence: 0.41796529918197284\n",
      "Reconstruction loss: 286.75454740611286, KL divergence: 0.2379512260138673\n",
      "Reconstruction loss: 241.4502913149194, KL divergence: 0.17163435460417503\n",
      "Reconstruction loss: 199.8657070837317, KL divergence: 0.09583425320705785\n",
      "Reconstruction loss: 170.7215163957349, KL divergence: 0.20894141781777725\n",
      "Reconstruction loss: 217.45528437305828, KL divergence: 0.09358296417454154\n",
      "Reconstruction loss: 265.1604575964169, KL divergence: 0.07463943665148137\n",
      "Reconstruction loss: 219.57564406334384, KL divergence: 0.04672022304705281\n",
      "Reconstruction loss: 163.4845845909095, KL divergence: 0.2821507612901152\n",
      "Reconstruction loss: 133.79974054436724, KL divergence: 0.4509918956062121\n",
      "Reconstruction loss: 269.37780314156083, KL divergence: 0.2931464303632658\n",
      "Reconstruction loss: 159.87397910967985, KL divergence: 0.5498434196894477\n",
      "Reconstruction loss: 314.4499008302647, KL divergence: 0.20029930322876066\n",
      "Reconstruction loss: 192.1874728919704, KL divergence: 0.1325553726432842\n",
      "Reconstruction loss: 188.42732905768585, KL divergence: 0.5099231771598219\n",
      "Reconstruction loss: 208.28115642646293, KL divergence: 0.033845477559786286\n",
      "Reconstruction loss: 200.91991296504165, KL divergence: 0.031574984908341375\n",
      "Reconstruction loss: 261.53714151639116, KL divergence: 6.187738480851728\n",
      "Reconstruction loss: 199.4576219777328, KL divergence: 0.08669816327080315\n",
      "Reconstruction loss: 109.0543614965498, KL divergence: 1.0983166428892432\n",
      "Reconstruction loss: 229.8649600524975, KL divergence: 0.41897154650392365\n",
      "Reconstruction loss: 204.60549363237772, KL divergence: 0.12469485988807427\n",
      "Reconstruction loss: 267.76316555763924, KL divergence: 0.04187721358145696\n",
      "Reconstruction loss: 293.92139019886537, KL divergence: 1.9445450376191995\n",
      "Reconstruction loss: 149.22701858110116, KL divergence: 13.008850985906573\n",
      "Reconstruction loss: 216.4616880823121, KL divergence: 0.15439860722792276\n",
      "Reconstruction loss: 146.76289985768204, KL divergence: 0.23421860825381052\n",
      "Reconstruction loss: 145.0704733784617, KL divergence: 0.32811814580893084\n",
      "Reconstruction loss: 241.0431434500225, KL divergence: 0.8136419047358822\n",
      "Reconstruction loss: 137.82963498527016, KL divergence: 0.228563329639977\n",
      "Reconstruction loss: 111.199211421727, KL divergence: 1.0444087451528241\n",
      "Reconstruction loss: 306.31142691081516, KL divergence: 0.18175829045312109\n",
      "Reconstruction loss: 179.0799182432913, KL divergence: 0.20743220383538036\n",
      "Reconstruction loss: 236.18741863798016, KL divergence: 0.9924767799593266\n",
      "Reconstruction loss: 113.29217695168043, KL divergence: 0.857449338401301\n",
      "Reconstruction loss: 227.89904435595355, KL divergence: 0.07662061141382048\n",
      "Reconstruction loss: 220.17201121732504, KL divergence: 0.2099179647375637\n",
      "Reconstruction loss: 267.71842406006783, KL divergence: 1.3555155974371829\n",
      "Reconstruction loss: 283.92299743991157, KL divergence: 0.04828842604769362\n",
      "Reconstruction loss: 176.24820481452406, KL divergence: 0.06357455207816715\n",
      "Reconstruction loss: 297.0063155414483, KL divergence: 2.431124071618915\n",
      "Reconstruction loss: 326.24264133761994, KL divergence: 1.9898960683009688\n",
      "Reconstruction loss: 278.59262518529584, KL divergence: 0.5886678281150772\n",
      "Reconstruction loss: 162.11583001213205, KL divergence: 0.8269861138643668\n",
      "Reconstruction loss: 215.88165573346686, KL divergence: 0.2539855259053513\n",
      "Reconstruction loss: 246.26460887959152, KL divergence: 2.659949147090348\n",
      "Reconstruction loss: 213.63878605435912, KL divergence: 0.042658572181391186\n",
      "Reconstruction loss: 268.21470615139566, KL divergence: 0.1325409927679247\n",
      "Reconstruction loss: 149.84185850407994, KL divergence: 0.6675561990426386\n",
      "Reconstruction loss: 218.00927516754336, KL divergence: 0.5473604257748057\n",
      "Reconstruction loss: 153.61222628802443, KL divergence: 0.74809030100078\n",
      "Reconstruction loss: 228.99587971571145, KL divergence: 0.8476450759888501\n",
      "Reconstruction loss: 157.63495398616257, KL divergence: 0.2099179647375637\n",
      "Reconstruction loss: 192.34255412530987, KL divergence: 0.2099179647375637\n",
      "Reconstruction loss: 224.6820730023444, KL divergence: 0.20898569243137238\n",
      "Reconstruction loss: 223.39608630584146, KL divergence: 0.8905551389552976\n",
      "Reconstruction loss: 256.32248776769717, KL divergence: 1.082932124080052\n",
      "Reconstruction loss: 117.80180825731676, KL divergence: 1.3979704887356506\n",
      "Reconstruction loss: 140.99691310620597, KL divergence: 0.2891131494198263\n",
      "Reconstruction loss: 185.27556457240172, KL divergence: 0.2104528430175172\n",
      "Reconstruction loss: 156.3643715118881, KL divergence: 0.13755513467977837\n",
      "Reconstruction loss: 162.7665548202488, KL divergence: 0.2104528430175172\n",
      "Reconstruction loss: 129.2307175916524, KL divergence: 1.4677709016595173\n",
      "Reconstruction loss: 120.76263545439333, KL divergence: 0.9689889053313536\n",
      "Reconstruction loss: 206.31914135979804, KL divergence: 6.347438410926704\n",
      "Reconstruction loss: 231.2437421129154, KL divergence: 0.2323785066419874\n",
      "Reconstruction loss: 164.31339228903246, KL divergence: 0.3021241052102151\n",
      "Reconstruction loss: 174.09819721315102, KL divergence: 0.2104528430175172\n",
      "Reconstruction loss: 174.83252962956635, KL divergence: 5.0705501389956105\n",
      "Reconstruction loss: 218.604488489933, KL divergence: 0.6196892313052453\n",
      "Reconstruction loss: 170.7365183425057, KL divergence: 0.2104528430175172\n",
      "Reconstruction loss: 206.56919776809627, KL divergence: 0.2259426110683041\n",
      "Reconstruction loss: 256.1665908982777, KL divergence: 1.3230405184186795\n",
      "Reconstruction loss: 135.1078405428196, KL divergence: 0.8385115897048606\n",
      "Reconstruction loss: 182.54112175624633, KL divergence: 0.34338127611032593\n",
      "Reconstruction loss: 109.68031192102501, KL divergence: 0.9405916368555617\n",
      "Reconstruction loss: 240.10317624706755, KL divergence: 0.04308170087218466\n",
      "Reconstruction loss: 177.6363864828056, KL divergence: 0.03273889851790934\n",
      "Reconstruction loss: 225.87777870796208, KL divergence: 0.614112342451172\n",
      "Reconstruction loss: 156.2448479966062, KL divergence: 0.3401815668680457\n",
      "Reconstruction loss: 150.12741882446505, KL divergence: 0.2757280028336583\n",
      "Reconstruction loss: 223.06715531455632, KL divergence: 0.09238091442614921\n",
      "Reconstruction loss: 184.3460494313594, KL divergence: 0.5367528799872971\n",
      "Reconstruction loss: 276.2050572468563, KL divergence: 1.15391818458451\n",
      "Reconstruction loss: 146.22950267843834, KL divergence: 0.2104528430175172\n",
      "Reconstruction loss: 244.07275615916635, KL divergence: 0.059105149368736065\n",
      "Reconstruction loss: 132.79603599479248, KL divergence: 0.3223299602662384\n",
      "Reconstruction loss: 275.3747483272276, KL divergence: 0.1731402902123635\n",
      "Reconstruction loss: 130.2063698983764, KL divergence: 0.2891861902631218\n",
      "Reconstruction loss: 237.32073014860046, KL divergence: 2.8174336240061733\n",
      "Reconstruction loss: 231.86625915018146, KL divergence: 0.26907702604746747\n",
      "Reconstruction loss: 362.02382190228707, KL divergence: 0.38151406242260055\n",
      "Reconstruction loss: 167.29077003551822, KL divergence: 0.21124010170304625\n",
      "Reconstruction loss: 250.78073665508285, KL divergence: 0.8637781319277168\n",
      "Reconstruction loss: 270.9161027663972, KL divergence: 0.5081914828669165\n",
      "Reconstruction loss: 265.0756502942681, KL divergence: 0.47995956187724803\n",
      "Reconstruction loss: 228.24776818314388, KL divergence: 0.051108742548836295\n",
      "Reconstruction loss: 162.2526879337223, KL divergence: 0.080921905582232\n",
      "Reconstruction loss: 195.5360085880262, KL divergence: 0.43409770916695267\n",
      "Reconstruction loss: 306.756768195583, KL divergence: 3.173115853257165\n",
      "Reconstruction loss: 199.50693847322722, KL divergence: 0.05239192857251235\n",
      "Reconstruction loss: 118.84794665758929, KL divergence: 0.6774458924995119\n",
      "Reconstruction loss: 174.36930987888047, KL divergence: 0.21124010170304625\n",
      "Reconstruction loss: 118.41743061015389, KL divergence: 1.4706817966663204\n",
      "Reconstruction loss: 185.12625858971475, KL divergence: 0.24868559310920868\n",
      "Reconstruction loss: 213.3434616302252, KL divergence: 0.2721662250815083\n",
      "Reconstruction loss: 150.8252333543915, KL divergence: 0.31909024278073805\n",
      "Reconstruction loss: 203.20263325040798, KL divergence: 0.1997961176035304\n",
      "Reconstruction loss: 214.16936190422774, KL divergence: 0.2631829565030445\n",
      "Reconstruction loss: 223.25829867695927, KL divergence: 0.19421347045994503\n",
      "Reconstruction loss: 175.98307850973245, KL divergence: 0.21159078360201222\n",
      "Reconstruction loss: 235.6399495722396, KL divergence: 0.14051107719652234\n",
      "Reconstruction loss: 326.20530079572023, KL divergence: 0.2092910079158879\n",
      "Reconstruction loss: 159.25636436630663, KL divergence: 0.27696433770533846\n",
      "Reconstruction loss: 306.09141708168, KL divergence: 1.010291518345161\n",
      "Reconstruction loss: 192.28872828550706, KL divergence: 0.21124010170304625\n",
      "Reconstruction loss: 199.18790366409675, KL divergence: 0.053905659120954796\n",
      "Reconstruction loss: 356.50636696110644, KL divergence: 0.11626944798200506\n",
      "Reconstruction loss: 271.1898008905475, KL divergence: 2.3093282296620963\n",
      "Reconstruction loss: 251.12382296943747, KL divergence: 0.2309525426932666\n",
      "Reconstruction loss: 204.26440654411164, KL divergence: 0.21124010170304625\n",
      "Reconstruction loss: 208.48730561408422, KL divergence: 0.21124010170304625\n",
      "Reconstruction loss: 198.6146745310403, KL divergence: 0.19087871014542934\n",
      "Reconstruction loss: 180.49569422234464, KL divergence: 0.2130287490161299\n",
      "Reconstruction loss: 224.80604289528233, KL divergence: 0.05191606633424528\n",
      "Reconstruction loss: 245.22168477168395, KL divergence: 0.8770216492272969\n",
      "Reconstruction loss: 218.70709488508868, KL divergence: 0.06720913892093994\n",
      "Reconstruction loss: 165.8311495357595, KL divergence: 0.4903418523311039\n",
      "Reconstruction loss: 183.39258289379615, KL divergence: 0.5481817458191118\n",
      "Reconstruction loss: 235.6730878297086, KL divergence: 0.05242132953850759\n",
      "Reconstruction loss: 192.62508912178967, KL divergence: 0.21119867888195998\n",
      "Reconstruction loss: 264.4818822773278, KL divergence: 0.34072814022289566\n",
      "Reconstruction loss: 232.1249592369069, KL divergence: 0.1561318170885192\n",
      "Reconstruction loss: 171.70029302435134, KL divergence: 6.45696107868719\n",
      "Reconstruction loss: 239.49336280874098, KL divergence: 0.09035313355185531\n",
      "Reconstruction loss: 199.88095712751166, KL divergence: 0.14488568088234838\n",
      "Reconstruction loss: 105.08510855653982, KL divergence: 0.9758093572318058\n",
      "Reconstruction loss: 151.49184548780607, KL divergence: 0.3366345723172545\n",
      "Reconstruction loss: 265.4683456449267, KL divergence: 0.0574257750878831\n",
      "Reconstruction loss: 198.0251361288129, KL divergence: 2.5380748516963028\n",
      "Reconstruction loss: 133.3463516766745, KL divergence: 0.9021884512751088\n",
      "Reconstruction loss: 242.6758482378158, KL divergence: 0.6069653172246441\n",
      "Reconstruction loss: 220.608724919196, KL divergence: 2.71983194285732\n",
      "Reconstruction loss: 196.66408829727624, KL divergence: 0.04461972197627467\n",
      "Reconstruction loss: 190.89759590838975, KL divergence: 0.06235921554279805\n",
      "Reconstruction loss: 162.20947688958998, KL divergence: 0.21119867888195998\n",
      "Reconstruction loss: 147.59372807649373, KL divergence: 0.33984465211577664\n",
      "Reconstruction loss: 202.7681789943884, KL divergence: 0.0978408897805692\n",
      "Reconstruction loss: 271.2786666041793, KL divergence: 0.05015001394541113\n",
      "Reconstruction loss: 231.46181352991738, KL divergence: 0.24221136829635603\n",
      "Reconstruction loss: 268.4990819910922, KL divergence: 1.8307910149704083\n",
      "Reconstruction loss: 158.72098388535068, KL divergence: 0.8262957966209827\n",
      "Reconstruction loss: 318.9275998306029, KL divergence: 0.053584710588764184\n",
      "Reconstruction loss: 149.714745198071, KL divergence: 0.3663015040494048\n",
      "Reconstruction loss: 195.5093871322436, KL divergence: 0.10932849121067617\n",
      "Reconstruction loss: 219.1590934408935, KL divergence: 0.10167257198535179\n",
      "Reconstruction loss: 211.28482004742204, KL divergence: 0.5334738539492723\n",
      "Reconstruction loss: 140.82025871227827, KL divergence: 1.0136329489694702\n",
      "Reconstruction loss: 140.89190914393416, KL divergence: 0.760671949685473\n",
      "Reconstruction loss: 169.01476932780844, KL divergence: 4.1979508453499985\n",
      "Reconstruction loss: 152.3284281054428, KL divergence: 1.3326679912329702\n",
      "Reconstruction loss: 232.12121186357976, KL divergence: 0.08795225812399193\n",
      "Reconstruction loss: 206.44924883487724, KL divergence: 0.038079016161522705\n",
      "Reconstruction loss: 207.65155799770884, KL divergence: 0.6557078539102255\n",
      "Reconstruction loss: 258.68253808377915, KL divergence: 0.13182911583805995\n",
      "Reconstruction loss: 188.10140341565238, KL divergence: 0.4665136010918275\n",
      "Reconstruction loss: 267.22568793384363, KL divergence: 0.34149086640858295\n",
      "Reconstruction loss: 137.8555694850906, KL divergence: 1.026125642085582\n",
      "Reconstruction loss: 260.3673656117189, KL divergence: 0.1650662783788011\n",
      "Reconstruction loss: 193.82251647732497, KL divergence: 0.044027868442939155\n",
      "Reconstruction loss: 232.40256551882516, KL divergence: 0.2120024417466841\n",
      "Reconstruction loss: 160.86804079558766, KL divergence: 0.7303849298606465\n",
      "Reconstruction loss: 179.50159213390242, KL divergence: 0.0429929412043375\n",
      "Reconstruction loss: 111.74668138503753, KL divergence: 1.67490666933036\n",
      "Reconstruction loss: 252.66357799485212, KL divergence: 6.432758909424699\n",
      "Reconstruction loss: 201.80833495838226, KL divergence: 0.16091976055591817\n",
      "Reconstruction loss: 255.0473630548008, KL divergence: 0.0685343193143576\n",
      "Reconstruction loss: 229.27065618290118, KL divergence: 0.6166118459697965\n",
      "Reconstruction loss: 263.61794626426695, KL divergence: 1.4695265166970448\n",
      "Reconstruction loss: 225.68665198918544, KL divergence: 0.20212087248495797\n",
      "Reconstruction loss: 126.92954098002953, KL divergence: 1.499120455961576\n",
      "Reconstruction loss: 232.37697028523502, KL divergence: 0.1360894984617223\n",
      "Reconstruction loss: 125.67575393075526, KL divergence: 0.7650312305000886\n",
      "Reconstruction loss: 265.6538502728223, KL divergence: 1.1414846601819282\n",
      "Reconstruction loss: 184.3234761245768, KL divergence: 0.20729431644996954\n",
      "Reconstruction loss: 192.41426193003076, KL divergence: 0.041584171822041105\n",
      "Reconstruction loss: 164.42119376069624, KL divergence: 0.1981416174934193\n",
      "Reconstruction loss: 280.53107144051853, KL divergence: 0.06756898387490334\n",
      "Reconstruction loss: 291.26103931507726, KL divergence: 1.2504256763016968\n",
      "Reconstruction loss: 256.5217722167644, KL divergence: 0.13564348792040642\n",
      "Reconstruction loss: 169.13350742826856, KL divergence: 0.2129860786984632\n",
      "Reconstruction loss: 193.52231176964338, KL divergence: 0.7491065187310731\n",
      "Reconstruction loss: 170.63131391006536, KL divergence: 0.4118732747233233\n",
      "Reconstruction loss: 221.15608079408392, KL divergence: 0.10961298789489854\n",
      "Reconstruction loss: 222.4232879712127, KL divergence: 0.054239964167970556\n",
      "Reconstruction loss: 244.98048625716265, KL divergence: 0.5374505137241186\n",
      "Reconstruction loss: 206.11395901787702, KL divergence: 0.4816474547543645\n",
      "Reconstruction loss: 186.11620001143314, KL divergence: 0.2129860786984632\n",
      "Reconstruction loss: 202.0103721102917, KL divergence: 0.2866783404338547\n",
      "Reconstruction loss: 238.5694893321965, KL divergence: 0.11971809097312913\n",
      "Reconstruction loss: 217.66598373062533, KL divergence: 0.045669633353014494\n",
      "Reconstruction loss: 189.999940514682, KL divergence: 0.1781749975572713\n",
      "Reconstruction loss: 218.6485685639031, KL divergence: 0.5263578647572822\n",
      "Reconstruction loss: 186.50820306166386, KL divergence: 0.15229077119622508\n",
      "Reconstruction loss: 251.53115038226588, KL divergence: 0.06389946985122308\n",
      "Reconstruction loss: 209.2369539814846, KL divergence: 0.20509382707855234\n",
      "Reconstruction loss: 237.549306862731, KL divergence: 0.11209584342087037\n",
      "Reconstruction loss: 199.72203879701212, KL divergence: 0.0490424344189736\n",
      "Reconstruction loss: 199.42742300178796, KL divergence: 0.05498020821038241\n",
      "Reconstruction loss: 255.89478253830154, KL divergence: 1.101143353984963\n",
      "Reconstruction loss: 141.75645768657967, KL divergence: 1.36933617999362\n",
      "Reconstruction loss: 118.41295977516882, KL divergence: 1.8064104327386508\n",
      "Reconstruction loss: 239.2606104125433, KL divergence: 2.1245959718964884\n",
      "Reconstruction loss: 216.76556870678306, KL divergence: 0.4654804471935502\n",
      "Reconstruction loss: 197.39355306376171, KL divergence: 2.679373200026133\n",
      "Reconstruction loss: 232.17799711492282, KL divergence: 0.5874815915908078\n",
      "Reconstruction loss: 204.35061681936935, KL divergence: 2.0911767445585605\n",
      "Reconstruction loss: 131.98271648589127, KL divergence: 1.1006000999646475\n",
      "Reconstruction loss: 212.25302324853806, KL divergence: 0.19026282494782681\n",
      "Reconstruction loss: 187.30463323768356, KL divergence: 4.169258557540907\n",
      "Reconstruction loss: 229.1579646037456, KL divergence: 0.04912353931359231\n",
      "Reconstruction loss: 175.02589242270238, KL divergence: 0.09604417772532686\n",
      "Reconstruction loss: 246.83820259407605, KL divergence: 0.8442534049701733\n",
      "Reconstruction loss: 173.89529913019882, KL divergence: 0.2126381467174574\n",
      "Reconstruction loss: 192.1735933723249, KL divergence: 0.21254590970353066\n",
      "Reconstruction loss: 240.05249601189013, KL divergence: 0.05219066493338481\n",
      "Reconstruction loss: 151.51428296840166, KL divergence: 0.7513887177763571\n",
      "Reconstruction loss: 237.7716315835964, KL divergence: 0.051123804675928586\n",
      "Reconstruction loss: 107.83546318960241, KL divergence: 1.45430094303086\n",
      "Reconstruction loss: 181.33868101849546, KL divergence: 0.3202018499235176\n",
      "Reconstruction loss: 175.02115973636657, KL divergence: 0.1799872418477068\n",
      "Reconstruction loss: 192.23743952265386, KL divergence: 0.09263613145500005\n",
      "Reconstruction loss: 178.48482469804503, KL divergence: 0.2406863370958467\n",
      "Reconstruction loss: 165.74691753031695, KL divergence: 0.21051538393243818\n",
      "Reconstruction loss: 118.6975266060521, KL divergence: 1.3703934133227738\n",
      "Reconstruction loss: 214.28997464260283, KL divergence: 0.09881710081364764\n",
      "Reconstruction loss: 168.55793884570446, KL divergence: 0.1038797303576941\n",
      "Reconstruction loss: 187.32942782729742, KL divergence: 0.08114873345624074\n",
      "Reconstruction loss: 317.9961937667412, KL divergence: 0.04827862244875908\n",
      "Reconstruction loss: 222.32632825329034, KL divergence: 0.1828158556900895\n",
      "Reconstruction loss: 195.19487272645472, KL divergence: 0.16849897198766406\n",
      "Reconstruction loss: 239.06391352732476, KL divergence: 2.9808679977349497\n",
      "Reconstruction loss: 166.0678637708907, KL divergence: 0.7904781990075243\n",
      "Reconstruction loss: 265.92125017914606, KL divergence: 0.2051525673293011\n",
      "Reconstruction loss: 234.40213340914545, KL divergence: 0.17482749848057816\n",
      "Reconstruction loss: 290.58644135275665, KL divergence: 0.2936187065247785\n",
      "Reconstruction loss: 203.38122780867891, KL divergence: 0.19123939734209555\n",
      "Reconstruction loss: 217.24729519483805, KL divergence: 3.5277523758223492\n",
      "Reconstruction loss: 220.56274877818333, KL divergence: 0.09345736968227181\n",
      "Reconstruction loss: 188.71790710409917, KL divergence: 0.1299098489754465\n",
      "Reconstruction loss: 116.78934700393172, KL divergence: 1.4847499701825564\n",
      "Reconstruction loss: 211.84425745553943, KL divergence: 0.05794519666846648\n",
      "Reconstruction loss: 174.31985934119763, KL divergence: 0.6939953168141045\n",
      "Reconstruction loss: 108.99265559623066, KL divergence: 1.5543194917678442\n",
      "Reconstruction loss: 160.31996920167376, KL divergence: 0.3227338217713053\n",
      "Reconstruction loss: 175.75684416842358, KL divergence: 0.48341120129057663\n",
      "Reconstruction loss: 177.2915782427072, KL divergence: 0.17370686604864072\n",
      "Reconstruction loss: 201.29679701165833, KL divergence: 3.677187541600447\n",
      "Reconstruction loss: 152.86680132154885, KL divergence: 0.21257201052127495\n",
      "Reconstruction loss: 269.4850616705791, KL divergence: 0.1010653931566059\n",
      "Reconstruction loss: 229.61922481915735, KL divergence: 0.043481579026905026\n",
      "Reconstruction loss: 162.80586281349792, KL divergence: 0.46151251925977643\n",
      "Reconstruction loss: 166.61091482152855, KL divergence: 0.25422966014317794\n",
      "Reconstruction loss: 140.1295877111348, KL divergence: 0.6981098384012261\n",
      "Reconstruction loss: 108.39890903728106, KL divergence: 1.053302595920564\n",
      "Reconstruction loss: 140.3673417929765, KL divergence: 1.4888398618046677\n",
      "Reconstruction loss: 321.5041277289814, KL divergence: 0.29746317281012136\n",
      "Reconstruction loss: 228.96127876961958, KL divergence: 0.6302214123730252\n",
      "Reconstruction loss: 221.2762847117763, KL divergence: 0.6618519621264276\n",
      "Reconstruction loss: 253.89407021042587, KL divergence: 1.2274414769306041\n",
      "Reconstruction loss: 157.74232340140077, KL divergence: 0.4980746595731529\n",
      "Reconstruction loss: 274.22453811198335, KL divergence: 0.05525791353798637\n",
      "Reconstruction loss: 178.97009543420796, KL divergence: 0.4097266904745094\n",
      "Reconstruction loss: 163.57129480246363, KL divergence: 0.3296411447491782\n",
      "Reconstruction loss: 122.67936678539553, KL divergence: 0.633353966419209\n",
      "Reconstruction loss: 286.6127386169459, KL divergence: 0.509481771682637\n",
      "Reconstruction loss: 131.37423521121863, KL divergence: 0.7928788010929895\n",
      "Reconstruction loss: 210.25640598553062, KL divergence: 0.05894757863756339\n",
      "Reconstruction loss: 140.2458113857818, KL divergence: 0.6329785467617397\n",
      "Reconstruction loss: 136.06974889385663, KL divergence: 0.7458865938813908\n",
      "Reconstruction loss: 172.56921988820648, KL divergence: 0.5607174979836462\n",
      "Reconstruction loss: 141.17186836322503, KL divergence: 1.3455258739757046\n",
      "Reconstruction loss: 176.76649085818624, KL divergence: 0.24696269323522663\n",
      "Reconstruction loss: 185.49219342744203, KL divergence: 0.0939823052434916\n",
      "Reconstruction loss: 202.12826722299206, KL divergence: 0.06774269477698097\n",
      "Reconstruction loss: 302.7977105657478, KL divergence: 0.5719455240970375\n",
      "Reconstruction loss: 219.02733367500576, KL divergence: 0.07292697445479673\n",
      "Reconstruction loss: 200.73132592155278, KL divergence: 0.3115180605564576\n",
      "Reconstruction loss: 266.05902008406383, KL divergence: 0.11538559538202797\n",
      "Reconstruction loss: 187.5839866734693, KL divergence: 0.27488301918349956\n",
      "Reconstruction loss: 173.09899822763825, KL divergence: 0.2667117092154842\n",
      "Reconstruction loss: 305.5815273622147, KL divergence: 0.21917872841610014\n",
      "Reconstruction loss: 225.5127930607332, KL divergence: 0.8309234219962518\n",
      "Reconstruction loss: 238.72722471213464, KL divergence: 0.11019957635177718\n",
      "Reconstruction loss: 142.33798844401036, KL divergence: 1.1365144234839069\n",
      "Reconstruction loss: 206.22587407901378, KL divergence: 0.12674597501012708\n",
      "Reconstruction loss: 243.24826557450623, KL divergence: 0.15445975559802066\n",
      "Reconstruction loss: 165.5099815762201, KL divergence: 0.6310559716750954\n",
      "Reconstruction loss: 191.02622421137858, KL divergence: 0.21122722351439543\n",
      "Reconstruction loss: 92.42858567269096, KL divergence: 0.8637482643929141\n",
      "Reconstruction loss: 195.66121731074088, KL divergence: 0.6013312187484074\n",
      "Reconstruction loss: 171.05690575104586, KL divergence: 2.390554307092404\n",
      "Reconstruction loss: 144.41419259746633, KL divergence: 0.7649769405076772\n",
      "Reconstruction loss: 166.6178598136184, KL divergence: 0.21122722351439543\n",
      "Reconstruction loss: 197.30106039472165, KL divergence: 0.21122722351439543\n",
      "Reconstruction loss: 240.30461009034838, KL divergence: 0.22603712732500247\n",
      "Reconstruction loss: 98.69545600081837, KL divergence: 0.9731713586336377\n",
      "Reconstruction loss: 232.2555591580428, KL divergence: 0.08623367265109466\n",
      "Reconstruction loss: 198.24534826434683, KL divergence: 0.11613937351206327\n",
      "Reconstruction loss: 288.2223045927508, KL divergence: 1.0444421409724918\n",
      "Reconstruction loss: 200.4699964859698, KL divergence: 0.10400573942291408\n",
      "Reconstruction loss: 243.15704976998626, KL divergence: 0.12733461838430743\n",
      "Reconstruction loss: 184.00892340090337, KL divergence: 0.26765273486791563\n",
      "Reconstruction loss: 177.41632104012405, KL divergence: 6.767660949137186\n",
      "Reconstruction loss: 112.30486834339516, KL divergence: 1.674615864948709\n",
      "Reconstruction loss: 209.68021661022703, KL divergence: 5.302259381528582\n",
      "Reconstruction loss: 234.24799647811022, KL divergence: 0.8720885625113168\n",
      "Reconstruction loss: 182.8841490610218, KL divergence: 0.28921332725690263\n",
      "Reconstruction loss: 192.32331818438877, KL divergence: 0.15131744817917936\n",
      "Reconstruction loss: 215.51642599334025, KL divergence: 0.09310664652530859\n",
      "Reconstruction loss: 225.13137277304065, KL divergence: 0.054374222279370554\n",
      "Reconstruction loss: 257.9043576261464, KL divergence: 0.3772426453792008\n",
      "Reconstruction loss: 204.3335544293858, KL divergence: 5.481652969683518\n",
      "Reconstruction loss: 171.96845695989677, KL divergence: 0.16516316667653047\n",
      "Reconstruction loss: 269.8695317246557, KL divergence: 1.096405709921289\n",
      "Reconstruction loss: 169.44390580691993, KL divergence: 0.33916134370474454\n",
      "Reconstruction loss: 233.50483630493787, KL divergence: 3.310398158136798\n",
      "Reconstruction loss: 162.4136013554519, KL divergence: 0.21122490352586426\n",
      "Reconstruction loss: 191.5993963022733, KL divergence: 0.06369504057476899\n",
      "Reconstruction loss: 184.71251550553242, KL divergence: 0.11470068048754473\n",
      "Reconstruction loss: 261.4944168206583, KL divergence: 0.2758249807806139\n",
      "Reconstruction loss: 175.70481684009417, KL divergence: 0.18780783902104525\n",
      "Reconstruction loss: 206.84421908589462, KL divergence: 0.46961399129305376\n",
      "Reconstruction loss: 174.79907611455707, KL divergence: 0.32750141654245596\n",
      "Reconstruction loss: 186.89516264002535, KL divergence: 0.2108189288218998\n",
      "Reconstruction loss: 244.9310512826632, KL divergence: 0.08925347140155421\n",
      "Reconstruction loss: 216.1477416192071, KL divergence: 0.19954833284041085\n",
      "Reconstruction loss: 185.49514552187924, KL divergence: 0.351955544037673\n",
      "Reconstruction loss: 121.13945428330777, KL divergence: 0.9814174772183856\n",
      "Reconstruction loss: 285.77253165537246, KL divergence: 0.15503701884225568\n",
      "Reconstruction loss: 227.27953775875716, KL divergence: 0.5476496418985413\n",
      "Reconstruction loss: 151.81616713515422, KL divergence: 0.20503970739643518\n",
      "Reconstruction loss: 143.03311433002588, KL divergence: 1.0553447523877924\n",
      "Reconstruction loss: 209.0471541675937, KL divergence: 0.5504154982465689\n",
      "Reconstruction loss: 217.9510072170109, KL divergence: 0.630778896794173\n",
      "Reconstruction loss: 154.18120179661642, KL divergence: 0.44844016972249895\n",
      "Reconstruction loss: 235.4976340598548, KL divergence: 0.15102266818682103\n",
      "Reconstruction loss: 171.28223611506849, KL divergence: 0.6531627157499285\n",
      "Reconstruction loss: 254.48316786417004, KL divergence: 0.10252539284777273\n",
      "Reconstruction loss: 202.67218981676055, KL divergence: 0.065994799053262\n",
      "Reconstruction loss: 213.93565687411515, KL divergence: 0.695927198285241\n",
      "Reconstruction loss: 241.97231086989697, KL divergence: 1.136711898948155\n",
      "Reconstruction loss: 159.6618408138301, KL divergence: 0.3796000304804234\n",
      "Reconstruction loss: 206.03989259992443, KL divergence: 0.2466087092592965\n",
      "Reconstruction loss: 178.93531588925003, KL divergence: 0.0661135505122582\n",
      "Reconstruction loss: 199.71005094149058, KL divergence: 0.06022003273552917\n",
      "Reconstruction loss: 229.93917705069902, KL divergence: 0.04222571732941571\n",
      "Reconstruction loss: 172.62578950104745, KL divergence: 0.045205769687335806\n",
      "Reconstruction loss: 173.0203176247112, KL divergence: 0.45927511217187184\n",
      "Reconstruction loss: 147.65510818646902, KL divergence: 0.2775394466059213\n",
      "Reconstruction loss: 220.67527993592594, KL divergence: 0.3738490124412037\n",
      "Reconstruction loss: 202.8450549020608, KL divergence: 0.3079030993860842\n",
      "Reconstruction loss: 190.44398989406392, KL divergence: 0.06146859890315831\n",
      "Reconstruction loss: 210.28312597697897, KL divergence: 0.3542591786607213\n",
      "Reconstruction loss: 251.80791478062537, KL divergence: 0.06802305728243552\n",
      "Reconstruction loss: 133.4846778753804, KL divergence: 0.8612486253522775\n",
      "Reconstruction loss: 183.85825352850298, KL divergence: 0.12839935459402624\n",
      "Reconstruction loss: 188.85428951226942, KL divergence: 0.3692567344955732\n",
      "Reconstruction loss: 267.31162229570856, KL divergence: 0.050717552855656156\n",
      "Reconstruction loss: 267.51425496546534, KL divergence: 2.6658180682885977\n",
      "Reconstruction loss: 250.3289856015454, KL divergence: 0.05243014558399717\n",
      "Reconstruction loss: 187.86745668980325, KL divergence: 0.052451188598355014\n",
      "Reconstruction loss: 326.04981676355, KL divergence: 0.2946251742417959\n",
      "Reconstruction loss: 230.11980166462615, KL divergence: 0.40286635029609436\n",
      "Reconstruction loss: 173.89234554858024, KL divergence: 0.21720137567902253\n",
      "Reconstruction loss: 158.12985651619437, KL divergence: 0.26452342506670856\n",
      "Reconstruction loss: 167.1186290185067, KL divergence: 0.1635512284560951\n",
      "Reconstruction loss: 195.09522116839094, KL divergence: 0.2162433800956826\n",
      "Reconstruction loss: 193.40773878219684, KL divergence: 0.06511475036372572\n",
      "Reconstruction loss: 116.635609791431, KL divergence: 1.4328426527500953\n",
      "Reconstruction loss: 249.0007079775461, KL divergence: 0.4884613846232725\n",
      "Reconstruction loss: 184.1097768590587, KL divergence: 0.6255580055394514\n",
      "Reconstruction loss: 168.0501211289747, KL divergence: 0.06033392664483972\n",
      "Reconstruction loss: 232.89442717591305, KL divergence: 0.5878631751604411\n",
      "Reconstruction loss: 268.54621416041914, KL divergence: 0.22679136288508556\n",
      "Reconstruction loss: 220.9036731563845, KL divergence: 0.1873514417725492\n",
      "Reconstruction loss: 114.03240434491468, KL divergence: 0.7327608628637245\n",
      "Reconstruction loss: 166.69303703227422, KL divergence: 0.21022545005380971\n",
      "Reconstruction loss: 162.30215984643752, KL divergence: 0.19274504018665495\n",
      "Reconstruction loss: 195.96864563495603, KL divergence: 2.0712590457267894\n",
      "Reconstruction loss: 175.89442983405945, KL divergence: 0.07801980251834784\n",
      "Reconstruction loss: 234.09102961506946, KL divergence: 0.1236870221959282\n",
      "Reconstruction loss: 180.72798338828898, KL divergence: 0.08189277673363743\n",
      "Reconstruction loss: 172.86329596419097, KL divergence: 0.09664336957870912\n",
      "Reconstruction loss: 201.43583912791755, KL divergence: 0.5713213094159506\n",
      "Reconstruction loss: 200.67985586583944, KL divergence: 3.6524718141527783\n",
      "Reconstruction loss: 187.46574875878747, KL divergence: 0.625073457061894\n",
      "Reconstruction loss: 334.50044070846593, KL divergence: 0.3374354703413201\n",
      "Reconstruction loss: 241.09090960021177, KL divergence: 0.10273395548271286\n",
      "Reconstruction loss: 164.53837607228243, KL divergence: 0.07901723716049802\n",
      "Reconstruction loss: 203.9343945589717, KL divergence: 0.054454566908286706\n",
      "Reconstruction loss: 190.33633967637513, KL divergence: 0.04734359884610112\n",
      "Reconstruction loss: 239.9656488311548, KL divergence: 1.4184486354944172\n",
      "Reconstruction loss: 228.81996941461597, KL divergence: 0.04729004850434737\n",
      "Reconstruction loss: 145.4912065170991, KL divergence: 0.08702962449059398\n",
      "Reconstruction loss: 234.15241440159335, KL divergence: 0.14980037890578207\n",
      "Reconstruction loss: 186.60874629602944, KL divergence: 7.270240806796888\n",
      "Reconstruction loss: 194.10867695440504, KL divergence: 0.07989672474014203\n",
      "Reconstruction loss: 222.85182161217045, KL divergence: 0.18192323704947594\n",
      "Reconstruction loss: 235.94701879410036, KL divergence: 2.515358772648659\n",
      "Reconstruction loss: 233.8370119102098, KL divergence: 0.09874873230742398\n",
      "Reconstruction loss: 212.96601428254252, KL divergence: 0.06058777851310582\n",
      "Reconstruction loss: 239.3571291461154, KL divergence: 0.04651757361678821\n",
      "Reconstruction loss: 134.6969064777108, KL divergence: 0.9434966936612488\n",
      "Reconstruction loss: 218.36882415282508, KL divergence: 0.9866327345088022\n",
      "Reconstruction loss: 222.07030841805897, KL divergence: 0.08144788607602432\n",
      "Reconstruction loss: 194.17936182636274, KL divergence: 0.40197314975771176\n",
      "Reconstruction loss: 290.2619435695966, KL divergence: 0.9026537914830725\n",
      "Reconstruction loss: 199.49586298335953, KL divergence: 0.07329447764649516\n",
      "Reconstruction loss: 207.49670991487756, KL divergence: 0.06929509832769692\n",
      "Reconstruction loss: 244.49178324633272, KL divergence: 0.04082504814028609\n",
      "Reconstruction loss: 261.92684303131955, KL divergence: 2.738142330544865\n",
      "Reconstruction loss: 257.25317966114073, KL divergence: 0.05844769804876032\n",
      "Reconstruction loss: 198.73420765404947, KL divergence: 0.050634686301608445\n",
      "Reconstruction loss: 251.66515435167418, KL divergence: 0.23191408516242162\n",
      "Reconstruction loss: 186.33533098366473, KL divergence: 0.9465100911973978\n",
      "Reconstruction loss: 207.20016626456794, KL divergence: 0.07999828985498736\n",
      "Reconstruction loss: 243.2318463039187, KL divergence: 0.3381479294846032\n",
      "Reconstruction loss: 251.04662838706577, KL divergence: 0.12199165736110412\n",
      "Reconstruction loss: 234.23023405218117, KL divergence: 0.05454098557961212\n",
      "Reconstruction loss: 277.69982878154497, KL divergence: 1.138984575699069\n",
      "Reconstruction loss: 162.09373768037963, KL divergence: 0.19300093760802556\n",
      "Reconstruction loss: 183.61761585159164, KL divergence: 0.10744409829301338\n",
      "Reconstruction loss: 164.4841427984583, KL divergence: 0.095880227906194\n",
      "Reconstruction loss: 233.18867667215073, KL divergence: 0.11827968190669952\n",
      "Reconstruction loss: 204.90021145493373, KL divergence: 0.08902733217678738\n",
      "Reconstruction loss: 189.96769206543735, KL divergence: 0.2012237744574467\n",
      "Reconstruction loss: 208.82607234520358, KL divergence: 0.17194101483322233\n",
      "Reconstruction loss: 262.0600862970826, KL divergence: 0.0569100068470173\n",
      "Reconstruction loss: 137.83805228236935, KL divergence: 0.20899587523191526\n",
      "Reconstruction loss: 222.46096623847671, KL divergence: 0.4093950982402656\n",
      "Reconstruction loss: 179.00043584865162, KL divergence: 0.06393296618955507\n",
      "Reconstruction loss: 224.89142680989454, KL divergence: 0.15208179430217744\n",
      "Reconstruction loss: 200.20664330132547, KL divergence: 0.13086617218777075\n",
      "Reconstruction loss: 268.6048824278797, KL divergence: 0.5932134784985373\n",
      "Reconstruction loss: 147.4207288480374, KL divergence: 0.26905130365348107\n",
      "Reconstruction loss: 336.3057640390425, KL divergence: 1.2879992042166928\n",
      "Reconstruction loss: 171.21375467556862, KL divergence: 0.07473159735567764\n",
      "Reconstruction loss: 139.3802379202831, KL divergence: 1.0554211221794523\n",
      "Reconstruction loss: 175.03804488225757, KL divergence: 0.058739606683195\n",
      "Reconstruction loss: 184.62650274988175, KL divergence: 0.04861135134424854\n",
      "Reconstruction loss: 190.41169783549208, KL divergence: 0.04136444079289969\n",
      "Reconstruction loss: 197.5891281987639, KL divergence: 0.04087305117171802\n",
      "Reconstruction loss: 277.1006438016965, KL divergence: 0.873430574338534\n",
      "Reconstruction loss: 197.57796458693076, KL divergence: 0.49783872197909634\n",
      "Reconstruction loss: 201.39905050230013, KL divergence: 0.05721946674454298\n",
      "Reconstruction loss: 167.6779833196817, KL divergence: 0.08149751811255918\n",
      "Reconstruction loss: 131.4101718933142, KL divergence: 1.1107139719735848\n",
      "Reconstruction loss: 184.32736238413042, KL divergence: 0.041634692547588725\n",
      "Reconstruction loss: 202.8384981745662, KL divergence: 0.04407016016816395\n",
      "Reconstruction loss: 228.31337767303734, KL divergence: 0.04901914940287039\n",
      "Reconstruction loss: 111.8962740470746, KL divergence: 0.5697733553684101\n",
      "Reconstruction loss: 213.10214354041238, KL divergence: 6.154437594410297\n",
      "Reconstruction loss: 256.1907899786977, KL divergence: 0.8676465526839736\n",
      "Reconstruction loss: 166.082381517492, KL divergence: 0.17232513753809203\n",
      "Reconstruction loss: 237.13720746108083, KL divergence: 0.45578042305572886\n",
      "Reconstruction loss: 220.0025897145706, KL divergence: 0.06254636012809461\n",
      "Reconstruction loss: 187.15345851092928, KL divergence: 0.20977299095798924\n",
      "Reconstruction loss: 220.9186467177833, KL divergence: 0.05210287440918265\n",
      "Reconstruction loss: 308.8436437108247, KL divergence: 0.29271281157786444\n",
      "Reconstruction loss: 178.04060040509484, KL divergence: 0.04544885614965449\n",
      "Reconstruction loss: 229.2817210093384, KL divergence: 0.1585740762570369\n",
      "Reconstruction loss: 151.90718537989738, KL divergence: 0.4236635803278446\n",
      "Reconstruction loss: 211.26710711896317, KL divergence: 4.946089109179773\n",
      "Reconstruction loss: 240.2258048457192, KL divergence: 2.246045172042378\n",
      "Reconstruction loss: 127.85602586294195, KL divergence: 0.4052519640794554\n",
      "Reconstruction loss: 188.82863740802208, KL divergence: 0.23307298841536894\n",
      "Reconstruction loss: 231.68381362349703, KL divergence: 0.055029124591849776\n",
      "Reconstruction loss: 252.00889510618148, KL divergence: 0.048491938505492604\n",
      "Reconstruction loss: 171.6449517708217, KL divergence: 0.12684680597296977\n",
      "Reconstruction loss: 212.3069727596494, KL divergence: 0.1164221176590593\n",
      "Reconstruction loss: 214.80931564688495, KL divergence: 0.045830661112004845\n",
      "Reconstruction loss: 181.9490280431689, KL divergence: 0.053562538571331586\n",
      "Reconstruction loss: 210.92181724000108, KL divergence: 0.032521324541123275\n",
      "Reconstruction loss: 246.00067896269184, KL divergence: 0.0404068469998507\n",
      "Reconstruction loss: 170.9330347037139, KL divergence: 0.3969849288841926\n",
      "Reconstruction loss: 177.16658899883834, KL divergence: 0.051568739106507944\n",
      "Reconstruction loss: 155.33630565734796, KL divergence: 0.12543134798711691\n",
      "Reconstruction loss: 158.14499134592864, KL divergence: 0.7133287193559146\n",
      "Reconstruction loss: 132.49529424912257, KL divergence: 0.3743563735474821\n",
      "Reconstruction loss: 158.02072501238888, KL divergence: 0.1725412020019229\n",
      "Reconstruction loss: 176.23290780389, KL divergence: 0.04171211381820694\n",
      "Reconstruction loss: 160.7891408660916, KL divergence: 0.6504206655336418\n",
      "Reconstruction loss: 156.5320072335112, KL divergence: 0.21481355679878655\n",
      "Reconstruction loss: 194.77017023879097, KL divergence: 0.26018469053435217\n",
      "Reconstruction loss: 200.58050173393593, KL divergence: 0.043625200104977724\n",
      "Reconstruction loss: 175.00759654263828, KL divergence: 0.2661125039900016\n",
      "Reconstruction loss: 251.6121617071512, KL divergence: 5.477884022436676\n",
      "Reconstruction loss: 218.10775606650495, KL divergence: 0.06183063491948354\n",
      "Reconstruction loss: 160.04059512668698, KL divergence: 0.19106349558032104\n",
      "Reconstruction loss: 142.22473026295137, KL divergence: 0.3212730368128533\n",
      "Reconstruction loss: 186.52612978753376, KL divergence: 0.08105611343255575\n",
      "Reconstruction loss: 291.48884864093316, KL divergence: 3.3257374342190653\n",
      "Reconstruction loss: 118.173921403261, KL divergence: 0.8289129187279676\n",
      "Reconstruction loss: 142.06583871304917, KL divergence: 6.919652075396476\n",
      "Reconstruction loss: 203.95620471469138, KL divergence: 0.05163571664707778\n",
      "Reconstruction loss: 159.17222921981485, KL divergence: 0.44422183267254506\n",
      "Reconstruction loss: 230.5020789320888, KL divergence: 0.048030483190624806\n",
      "Reconstruction loss: 199.70607727795021, KL divergence: 0.06542458558822212\n",
      "Reconstruction loss: 129.3642074731551, KL divergence: 0.21139190985529288\n",
      "Reconstruction loss: 226.56455061543483, KL divergence: 0.11309775130888039\n",
      "Reconstruction loss: 179.72665732640615, KL divergence: 0.08960165945288939\n",
      "Reconstruction loss: 214.28511419254224, KL divergence: 0.05052400009221153\n",
      "Reconstruction loss: 306.5093204685609, KL divergence: 0.2809965120425135\n",
      "Reconstruction loss: 216.88955101955884, KL divergence: 0.043052753358931384\n",
      "Reconstruction loss: 254.88251479395038, KL divergence: 0.2878428797356728\n",
      "Reconstruction loss: 144.5773085447261, KL divergence: 0.45194111291724176\n",
      "Reconstruction loss: 137.4129631104691, KL divergence: 0.8103140515487858\n",
      "Reconstruction loss: 128.49122810512563, KL divergence: 0.7783351149817501\n",
      "Reconstruction loss: 211.8912748381759, KL divergence: 0.11133857564749278\n",
      "Reconstruction loss: 161.69161333760573, KL divergence: 0.8519794553104377\n",
      "Reconstruction loss: 188.53946638610591, KL divergence: 0.1200482147364818\n",
      "Reconstruction loss: 224.1003717423132, KL divergence: 0.3985462835348967\n",
      "Reconstruction loss: 169.27502923522133, KL divergence: 0.10461582834260597\n",
      "Reconstruction loss: 114.22734200449403, KL divergence: 1.1017430714653524\n",
      "Reconstruction loss: 158.19871326754756, KL divergence: 0.4261677843049883\n",
      "Reconstruction loss: 112.8651490953828, KL divergence: 0.6582803229271614\n",
      "Reconstruction loss: 169.04589908231975, KL divergence: 0.13658069563797587\n",
      "Reconstruction loss: 195.85886303047397, KL divergence: 0.04576761724149597\n",
      "Reconstruction loss: 182.96061412362616, KL divergence: 0.21193427718276125\n",
      "Reconstruction loss: 195.19834088897247, KL divergence: 0.0525587224070766\n",
      "Reconstruction loss: 130.46874255955447, KL divergence: 0.3029281416572773\n",
      "Reconstruction loss: 197.9945967637587, KL divergence: 0.21134615295650766\n",
      "Reconstruction loss: 91.2689612051417, KL divergence: 1.4868966143218507\n",
      "Reconstruction loss: 272.5890591007316, KL divergence: 4.614238022760798\n",
      "Reconstruction loss: 267.8571681699141, KL divergence: 0.09995498446473433\n",
      "Reconstruction loss: 185.5286991996919, KL divergence: 6.761918614625229\n",
      "Reconstruction loss: 192.5711763406166, KL divergence: 0.053024546248566895\n",
      "Reconstruction loss: 227.7312492991831, KL divergence: 0.2657033070640971\n",
      "Reconstruction loss: 281.89031744496594, KL divergence: 0.14765042270055334\n",
      "Reconstruction loss: 202.63460635795965, KL divergence: 0.0949883257916257\n",
      "Reconstruction loss: 148.39754278362523, KL divergence: 0.21067512207206496\n",
      "Reconstruction loss: 151.83128875957595, KL divergence: 0.31410117132449195\n",
      "Reconstruction loss: 148.14343359663104, KL divergence: 1.2579111649460861\n",
      "Reconstruction loss: 255.71095694657035, KL divergence: 0.2121972839532788\n",
      "Reconstruction loss: 132.3774003806835, KL divergence: 0.5490387955914782\n",
      "Reconstruction loss: 140.4059548042231, KL divergence: 0.881149207343032\n",
      "Reconstruction loss: 274.93323685766995, KL divergence: 0.2279870233612707\n",
      "Reconstruction loss: 236.16007430651996, KL divergence: 0.21130333495767623\n",
      "Reconstruction loss: 195.90794388887713, KL divergence: 0.9808109380112793\n",
      "Reconstruction loss: 178.28810562670725, KL divergence: 0.2121972839532788\n",
      "Reconstruction loss: 121.81021034987506, KL divergence: 0.8088242223333466\n",
      "Reconstruction loss: 288.4244702416156, KL divergence: 0.18338574694597826\n",
      "Reconstruction loss: 123.74161038492385, KL divergence: 0.6212920071068624\n",
      "Reconstruction loss: 212.46591770419562, KL divergence: 0.08392313629229603\n",
      "Reconstruction loss: 162.11385320188208, KL divergence: 0.12829872551539911\n",
      "Reconstruction loss: 231.1652710899961, KL divergence: 0.2760458163203257\n",
      "Reconstruction loss: 178.9060911091829, KL divergence: 0.24462748795246808\n",
      "Reconstruction loss: 180.84417048851589, KL divergence: 0.2072772011125642\n",
      "Reconstruction loss: 255.95855995613516, KL divergence: 0.6691280216383597\n",
      "Reconstruction loss: 191.90294318898415, KL divergence: 0.0493389511880013\n",
      "Reconstruction loss: 224.30483957449138, KL divergence: 0.17741893319513263\n",
      "Reconstruction loss: 202.44244424865195, KL divergence: 0.49740288274696004\n",
      "Reconstruction loss: 210.69935598671685, KL divergence: 0.11836831646685225\n",
      "Reconstruction loss: 254.79089632886283, KL divergence: 0.23504715671027665\n",
      "Reconstruction loss: 212.34309840263285, KL divergence: 0.05899788145869045\n",
      "Reconstruction loss: 260.4314108864801, KL divergence: 1.057133864868694\n",
      "Reconstruction loss: 236.54340804696756, KL divergence: 0.13032551589044533\n",
      "Reconstruction loss: 209.62414618178087, KL divergence: 0.12013979280202502\n",
      "Reconstruction loss: 210.49255844723916, KL divergence: 0.06619028854012804\n",
      "Reconstruction loss: 157.71373488071538, KL divergence: 0.3005593676990005\n",
      "Reconstruction loss: 177.004489746725, KL divergence: 0.29058804038963304\n",
      "Reconstruction loss: 186.40433896843336, KL divergence: 4.430980753452852\n",
      "Reconstruction loss: 207.99462503896916, KL divergence: 0.2118616258189424\n",
      "Reconstruction loss: 258.1557423271661, KL divergence: 0.34898651482802084\n",
      "Reconstruction loss: 225.73145804735412, KL divergence: 0.06492263860029229\n",
      "Reconstruction loss: 130.06956205552748, KL divergence: 0.883663372422062\n",
      "Reconstruction loss: 191.88084295060875, KL divergence: 0.08698411738682965\n",
      "Reconstruction loss: 203.96086358833415, KL divergence: 0.053361531414257535\n",
      "Reconstruction loss: 227.50188681917675, KL divergence: 0.1092470363257354\n",
      "Reconstruction loss: 197.09571463261085, KL divergence: 0.06398689766664828\n",
      "Reconstruction loss: 153.39955758275707, KL divergence: 0.23153150455073057\n",
      "Reconstruction loss: 230.19299749756397, KL divergence: 0.060762100785105855\n",
      "Reconstruction loss: 195.27288773038578, KL divergence: 0.2195863040920194\n",
      "Reconstruction loss: 214.30372493129516, KL divergence: 9.44114188911501\n",
      "Reconstruction loss: 233.49864356783746, KL divergence: 5.8794722266517185\n",
      "Reconstruction loss: 217.59526579309534, KL divergence: 11.27344531408175\n",
      "Reconstruction loss: 155.5117764135553, KL divergence: 1.1611395726505636\n",
      "Reconstruction loss: 214.71061512529258, KL divergence: 0.04684253895123519\n",
      "Reconstruction loss: 304.7867649916607, KL divergence: 2.852889507502895\n",
      "Reconstruction loss: 172.68159553073417, KL divergence: 0.4750133116848674\n",
      "Reconstruction loss: 146.75656843363183, KL divergence: 0.1429512312951841\n",
      "Reconstruction loss: 181.17626205330401, KL divergence: 0.04624369603249673\n",
      "Reconstruction loss: 283.30962076801893, KL divergence: 0.0427628922750064\n",
      "Reconstruction loss: 216.63501496770556, KL divergence: 0.13013953881342244\n",
      "Reconstruction loss: 199.77458376038942, KL divergence: 0.06788269241023664\n",
      "Reconstruction loss: 250.14356066297063, KL divergence: 1.1756046340100952\n",
      "Reconstruction loss: 284.2160998080534, KL divergence: 0.6312509350964959\n",
      "Reconstruction loss: 212.84938077039877, KL divergence: 1.7782774787210562\n",
      "Reconstruction loss: 164.19423818455687, KL divergence: 0.560072812747217\n",
      "Reconstruction loss: 214.66348455946246, KL divergence: 0.13478573220660067\n",
      "Reconstruction loss: 257.4843630033854, KL divergence: 4.48109597821931\n",
      "Reconstruction loss: 170.20998270385996, KL divergence: 0.2461576424985974\n",
      "Reconstruction loss: 160.7742330724688, KL divergence: 0.2339075404652166\n",
      "Reconstruction loss: 248.19348837891738, KL divergence: 0.05030058692117911\n",
      "Reconstruction loss: 226.8789279900954, KL divergence: 0.04886989611317011\n",
      "Reconstruction loss: 257.12572665681694, KL divergence: 0.9133766197151778\n",
      "Reconstruction loss: 122.50450968102182, KL divergence: 1.223586623682594\n",
      "Reconstruction loss: 295.31200330264664, KL divergence: 0.5540427005804878\n",
      "Reconstruction loss: 249.08779330966672, KL divergence: 0.6234828750049959\n",
      "Reconstruction loss: 182.84151473615867, KL divergence: 0.14423058268905026\n",
      "Reconstruction loss: 190.49540202639292, KL divergence: 0.40132598762519817\n",
      "Reconstruction loss: 159.36180370736597, KL divergence: 4.046082452214151\n",
      "Reconstruction loss: 145.27588374076825, KL divergence: 0.4273149984145669\n",
      "Reconstruction loss: 262.8584044719355, KL divergence: 0.5768924920876145\n",
      "Reconstruction loss: 154.84326929757214, KL divergence: 0.17973006673574088\n",
      "Reconstruction loss: 194.29443051774575, KL divergence: 0.04552817844763146\n",
      "Reconstruction loss: 196.25163990824063, KL divergence: 0.21227802179984162\n",
      "Reconstruction loss: 148.7214624022447, KL divergence: 0.20892202513851688\n",
      "Reconstruction loss: 309.24190448680605, KL divergence: 0.6723664350885932\n",
      "Reconstruction loss: 209.04582762138182, KL divergence: 0.04920366791707792\n",
      "Reconstruction loss: 140.3060553275085, KL divergence: 0.7555384306029302\n",
      "Reconstruction loss: 129.88854461524846, KL divergence: 0.6950000568498358\n",
      "Reconstruction loss: 217.06328409165886, KL divergence: 0.14919362524449037\n",
      "Reconstruction loss: 233.51589472931386, KL divergence: 5.345244369213663\n",
      "Reconstruction loss: 147.6948352122464, KL divergence: 0.4991398941394291\n",
      "Reconstruction loss: 88.73606481060915, KL divergence: 1.210111287127233\n",
      "Reconstruction loss: 315.95401342167673, KL divergence: 2.922749475427679\n",
      "Reconstruction loss: 284.3703448926109, KL divergence: 1.048056845667529\n",
      "Reconstruction loss: 240.80427384216028, KL divergence: 0.07554987737360314\n",
      "Reconstruction loss: 183.19006200433384, KL divergence: 0.10886219358322025\n",
      "Reconstruction loss: 226.82907662220265, KL divergence: 0.2316757668492419\n",
      "Reconstruction loss: 209.67728449344406, KL divergence: 0.242745422920166\n",
      "Reconstruction loss: 221.86472692247878, KL divergence: 3.419094872617935\n",
      "Reconstruction loss: 146.2241052784599, KL divergence: 0.45361613822600394\n",
      "Reconstruction loss: 178.26085880471157, KL divergence: 5.056201689481994\n",
      "Reconstruction loss: 174.87173863256191, KL divergence: 0.05695295022888808\n",
      "Reconstruction loss: 206.2578570933575, KL divergence: 0.04799331495163489\n",
      "Reconstruction loss: 216.74447461993122, KL divergence: 0.05872617488520959\n",
      "Reconstruction loss: 147.9885507006002, KL divergence: 0.27509981167055864\n",
      "Reconstruction loss: 246.92253827358292, KL divergence: 0.10937264276894382\n",
      "Reconstruction loss: 186.2064877138475, KL divergence: 0.18023968800882512\n",
      "Reconstruction loss: 146.50222028918267, KL divergence: 0.43699813007855476\n",
      "Reconstruction loss: 175.9618703841902, KL divergence: 0.6755647611158062\n",
      "Reconstruction loss: 185.5699618565144, KL divergence: 0.07389557108026046\n",
      "Reconstruction loss: 190.14991073639277, KL divergence: 0.06855033732792792\n",
      "Reconstruction loss: 293.51660282665966, KL divergence: 0.7009946559515169\n",
      "Reconstruction loss: 198.46139254070192, KL divergence: 0.04656087136236198\n",
      "Reconstruction loss: 194.00363182715364, KL divergence: 0.1115896659785734\n",
      "Reconstruction loss: 220.32623674855208, KL divergence: 0.1593616971046679\n",
      "Reconstruction loss: 231.01918986178617, KL divergence: 0.7362709326229735\n",
      "Reconstruction loss: 232.2649083518932, KL divergence: 4.294883766733901\n",
      "Reconstruction loss: 221.57211985992063, KL divergence: 0.21808009634471215\n",
      "Reconstruction loss: 123.0948115923554, KL divergence: 1.3770978965983414\n",
      "Reconstruction loss: 172.9219433093194, KL divergence: 0.49376023173937994\n",
      "Reconstruction loss: 163.3127203985077, KL divergence: 0.08378436305509823\n",
      "Reconstruction loss: 284.66349378447285, KL divergence: 0.04065424649443006\n",
      "Reconstruction loss: 175.4318214562251, KL divergence: 0.04358071067200059\n",
      "Reconstruction loss: 218.7113879406374, KL divergence: 0.040026745644792305\n",
      "Reconstruction loss: 200.24772261380497, KL divergence: 0.0720075544548004\n",
      "Reconstruction loss: 263.11290381572775, KL divergence: 5.694915794512189\n",
      "Reconstruction loss: 211.76999748331943, KL divergence: 6.176021793428556\n",
      "Reconstruction loss: 173.1310892099977, KL divergence: 0.04015334962549205\n",
      "Reconstruction loss: 299.6865288677458, KL divergence: 0.18785321962659612\n",
      "Reconstruction loss: 248.61236767012366, KL divergence: 0.09460475741906427\n",
      "Reconstruction loss: 299.1277138949469, KL divergence: 0.20460686244256732\n",
      "Reconstruction loss: 145.76208710535047, KL divergence: 0.2197069340515596\n",
      "Reconstruction loss: 262.8291803401961, KL divergence: 3.158095523883115\n",
      "Reconstruction loss: 167.4490718576238, KL divergence: 0.3206771182635391\n",
      "Reconstruction loss: 199.96823713132815, KL divergence: 0.11157311179849916\n",
      "Reconstruction loss: 178.3454416851048, KL divergence: 0.039890349131633196\n",
      "Reconstruction loss: 170.08586632805498, KL divergence: 0.09850649954559715\n",
      "Reconstruction loss: 182.43128330649762, KL divergence: 0.35781219474263176\n",
      "Reconstruction loss: 248.80644854072534, KL divergence: 0.06548911805763286\n",
      "Reconstruction loss: 293.69879097996613, KL divergence: 0.908074889083831\n",
      "Reconstruction loss: 236.30934265360634, KL divergence: 0.12861593435913543\n",
      "Reconstruction loss: 122.00115409233007, KL divergence: 1.113198131125725\n",
      "Reconstruction loss: 138.93568617851417, KL divergence: 0.48582829984137077\n",
      "Reconstruction loss: 257.7622895513495, KL divergence: 1.9461774194733557\n",
      "Reconstruction loss: 200.93481793988832, KL divergence: 0.45856771336232643\n",
      "Reconstruction loss: 243.14014764588953, KL divergence: 1.1373747195276558\n",
      "Reconstruction loss: 163.11872065439962, KL divergence: 0.10110318344451535\n",
      "Reconstruction loss: 289.4929543317375, KL divergence: 0.041045415462785395\n",
      "Reconstruction loss: 229.33908223789845, KL divergence: 0.24622767772844179\n",
      "Reconstruction loss: 235.22693320935838, KL divergence: 0.4329431731282773\n",
      "Reconstruction loss: 198.9127469703543, KL divergence: 5.124558685652538\n",
      "Reconstruction loss: 274.77292470187933, KL divergence: 0.8656014221375422\n",
      "Reconstruction loss: 226.62653354412527, KL divergence: 1.7573985532339589\n",
      "Reconstruction loss: 193.95853073636488, KL divergence: 0.04052704877933455\n",
      "Reconstruction loss: 195.07396138248876, KL divergence: 0.6332519139011183\n",
      "Reconstruction loss: 189.20804824741487, KL divergence: 0.07953503740676604\n",
      "Reconstruction loss: 171.5695136597106, KL divergence: 4.816067702325334\n",
      "Reconstruction loss: 218.78640501297497, KL divergence: 0.3268495311826984\n",
      "Reconstruction loss: 229.93072505646433, KL divergence: 0.7731691664215924\n",
      "Reconstruction loss: 176.54080378038668, KL divergence: 1.413230890316501\n",
      "Reconstruction loss: 196.58116943124082, KL divergence: 0.14822092128648223\n",
      "Reconstruction loss: 231.24825973004698, KL divergence: 0.5930071042731193\n",
      "Reconstruction loss: 209.1948443815008, KL divergence: 0.588557107801536\n",
      "Reconstruction loss: 280.6250671877603, KL divergence: 0.9874545665746861\n",
      "Reconstruction loss: 255.6302808657578, KL divergence: 0.0958494198454864\n",
      "Reconstruction loss: 208.9984359898648, KL divergence: 1.4285931734075308\n",
      "Reconstruction loss: 200.71307721353625, KL divergence: 0.3458632508437239\n",
      "Reconstruction loss: 191.21992027142994, KL divergence: 0.08612845175722733\n",
      "Reconstruction loss: 135.48533689008622, KL divergence: 0.44225128100279176\n",
      "Reconstruction loss: 244.79805169798007, KL divergence: 0.22703649344975424\n",
      "Reconstruction loss: 164.95711480884967, KL divergence: 0.09427251660197827\n",
      "Reconstruction loss: 194.84621367100988, KL divergence: 0.056795870914437\n",
      "Reconstruction loss: 232.00267303206203, KL divergence: 0.5387345484787103\n",
      "Reconstruction loss: 183.58154784074335, KL divergence: 0.05544631882216644\n",
      "Reconstruction loss: 225.97456566380566, KL divergence: 0.14916953263212662\n",
      "Reconstruction loss: 349.21035655964806, KL divergence: 0.16902020492720826\n",
      "Reconstruction loss: 349.71572650799567, KL divergence: 0.6958614260172554\n",
      "Reconstruction loss: 200.20209972760307, KL divergence: 0.04404490315677234\n",
      "Reconstruction loss: 167.9806404865572, KL divergence: 0.04986029021559496\n",
      "Reconstruction loss: 191.09988495805862, KL divergence: 0.0933282302899135\n",
      "Reconstruction loss: 208.54092536062564, KL divergence: 0.08025321393166357\n",
      "Reconstruction loss: 236.11017146492946, KL divergence: 0.2051830985798671\n",
      "Reconstruction loss: 195.42737630129824, KL divergence: 0.04360645565603011\n",
      "Reconstruction loss: 267.54479848431527, KL divergence: 3.3721852326918995\n",
      "Reconstruction loss: 290.0329965797479, KL divergence: 0.3296255413651589\n",
      "Reconstruction loss: 149.5370266428747, KL divergence: 0.4374487559372374\n",
      "Reconstruction loss: 267.8178709010065, KL divergence: 3.0144505685045693\n",
      "Reconstruction loss: 265.665269286732, KL divergence: 1.8407760487176499\n",
      "Reconstruction loss: 204.8360710398916, KL divergence: 0.04626654682542963\n",
      "Reconstruction loss: 167.3461515485128, KL divergence: 0.042293889948953556\n",
      "Reconstruction loss: 177.22314283032648, KL divergence: 0.05964426899708325\n",
      "Reconstruction loss: 175.56358242445114, KL divergence: 0.04452907962759178\n",
      "Reconstruction loss: 174.25455109240033, KL divergence: 0.7772007712855366\n",
      "Reconstruction loss: 140.53077706580814, KL divergence: 0.20713037693294323\n",
      "Reconstruction loss: 169.41279839396336, KL divergence: 0.052320503418266995\n",
      "Reconstruction loss: 217.02032717783695, KL divergence: 5.281799002780826\n",
      "Reconstruction loss: 148.8442524741585, KL divergence: 0.8146894012307833\n",
      "Reconstruction loss: 186.73866445652192, KL divergence: 0.05520192978964966\n",
      "Reconstruction loss: 237.67047834943284, KL divergence: 0.1517878712902252\n",
      "Reconstruction loss: 187.73596633544332, KL divergence: 0.11711146240496001\n",
      "Reconstruction loss: 205.51483204711732, KL divergence: 0.7048300759934799\n",
      "Reconstruction loss: 281.80838793287467, KL divergence: 3.2038865860792236\n",
      "Reconstruction loss: 229.90294682706167, KL divergence: 0.5221627686799896\n",
      "Reconstruction loss: 271.5637760894926, KL divergence: 0.09869526908965293\n",
      "Reconstruction loss: 295.4631510722749, KL divergence: 0.06861601961576697\n",
      "Reconstruction loss: 151.97145104973663, KL divergence: 0.0696814247227529\n",
      "Reconstruction loss: 211.10361125851375, KL divergence: 0.3851127726794024\n",
      "Reconstruction loss: 190.36507459862753, KL divergence: 0.39310859620076866\n",
      "Reconstruction loss: 260.4618081619807, KL divergence: 0.36768075729297833\n",
      "Reconstruction loss: 214.28212161145962, KL divergence: 0.23956449285982662\n",
      "Reconstruction loss: 106.91949569617488, KL divergence: 0.6911497946453244\n",
      "Reconstruction loss: 153.85172920436906, KL divergence: 0.3879203570238661\n",
      "Reconstruction loss: 242.6501171337383, KL divergence: 2.125924276220746\n",
      "Reconstruction loss: 167.57964942315203, KL divergence: 0.04176985886441248\n",
      "Reconstruction loss: 141.63213750810576, KL divergence: 0.5373071208734896\n",
      "Reconstruction loss: 130.61345237415222, KL divergence: 1.3488572119907642\n",
      "Reconstruction loss: 245.50486476585448, KL divergence: 5.609905164868845\n",
      "Reconstruction loss: 292.7233862402011, KL divergence: 0.2673743142529426\n",
      "Reconstruction loss: 204.7878939502213, KL divergence: 0.37073156645434835\n",
      "Reconstruction loss: 134.5994319632471, KL divergence: 0.8186568956207185\n",
      "Reconstruction loss: 220.65091856642363, KL divergence: 0.05336380302616078\n",
      "Reconstruction loss: 217.42553831227778, KL divergence: 0.3183767340253152\n",
      "Reconstruction loss: 223.72604657410506, KL divergence: 0.05509205248046928\n",
      "Reconstruction loss: 214.5542021817708, KL divergence: 0.12287796106189802\n",
      "Reconstruction loss: 220.20372236426198, KL divergence: 0.09358702882456976\n",
      "Reconstruction loss: 155.46374241651688, KL divergence: 0.9795676971024949\n",
      "Reconstruction loss: 199.93587576165416, KL divergence: 0.041119885354535035\n",
      "Reconstruction loss: 208.42676097107056, KL divergence: 0.07158619488911028\n",
      "Reconstruction loss: 217.73345656835053, KL divergence: 0.048905608210967955\n",
      "Reconstruction loss: 181.27289155178937, KL divergence: 0.05668197003091974\n",
      "Reconstruction loss: 228.3954597341592, KL divergence: 0.04636320557919199\n",
      "Reconstruction loss: 160.15211977217155, KL divergence: 0.36348262986851876\n",
      "Reconstruction loss: 128.9383474261603, KL divergence: 0.49876570318713837\n",
      "Reconstruction loss: 153.52320126969846, KL divergence: 0.12430345224021833\n",
      "Reconstruction loss: 177.27875716316217, KL divergence: 0.24011902097480053\n",
      "Reconstruction loss: 119.01385549447977, KL divergence: 0.8429334679526709\n",
      "Reconstruction loss: 125.63320710888482, KL divergence: 0.8456818482781787\n",
      "Reconstruction loss: 227.46636930777817, KL divergence: 0.22826801459743207\n",
      "Reconstruction loss: 273.26595232659747, KL divergence: 0.9021293878519259\n",
      "Reconstruction loss: 198.51634463397693, KL divergence: 0.04688698452529655\n",
      "Reconstruction loss: 340.0095637066011, KL divergence: 0.9979901333984793\n",
      "Reconstruction loss: 193.23429397747037, KL divergence: 0.04202278010498123\n",
      "Reconstruction loss: 295.71819521473265, KL divergence: 0.5475923437467118\n",
      "Reconstruction loss: 222.09158289613848, KL divergence: 0.13613124974303886\n",
      "Reconstruction loss: 137.35232265766697, KL divergence: 0.5612366266846249\n",
      "Reconstruction loss: 138.90063638493552, KL divergence: 0.2785298718721471\n",
      "Reconstruction loss: 184.31362472908103, KL divergence: 0.08355309099899377\n",
      "Reconstruction loss: 258.0408760197724, KL divergence: 0.09433167317749613\n",
      "Reconstruction loss: 239.02839003879856, KL divergence: 0.25232882594972283\n",
      "Reconstruction loss: 170.51617827370268, KL divergence: 0.05125143561411111\n",
      "Reconstruction loss: 190.52103246398858, KL divergence: 0.04305324664090987\n",
      "Reconstruction loss: 130.38982123082297, KL divergence: 1.2248768338808997\n",
      "Reconstruction loss: 248.92666350571457, KL divergence: 5.113846511599547\n",
      "Reconstruction loss: 198.46400738072956, KL divergence: 0.045313696239396295\n",
      "Reconstruction loss: 210.6319967464955, KL divergence: 0.10826940399368179\n",
      "Reconstruction loss: 144.32159141224852, KL divergence: 0.36897137469528685\n",
      "Reconstruction loss: 189.34924495865368, KL divergence: 0.09425487704607632\n",
      "Reconstruction loss: 267.8955288009688, KL divergence: 2.5043582899674783\n",
      "Reconstruction loss: 156.57271158731476, KL divergence: 0.11510823490001798\n",
      "Reconstruction loss: 226.9122741877656, KL divergence: 0.06333231583873805\n",
      "Reconstruction loss: 175.07922641085574, KL divergence: 0.07432960011357609\n",
      "Reconstruction loss: 164.197891830348, KL divergence: 0.06078189769902559\n",
      "Reconstruction loss: 222.2139559543171, KL divergence: 0.06308814874344204\n",
      "Reconstruction loss: 330.5584245601607, KL divergence: 0.11879976158360572\n",
      "Reconstruction loss: 130.24727651978364, KL divergence: 0.42728517566056773\n",
      "Reconstruction loss: 205.0944612865943, KL divergence: 0.06185416231010049\n",
      "Reconstruction loss: 188.6219813423965, KL divergence: 0.16315744237108953\n",
      "Reconstruction loss: 152.6803464514746, KL divergence: 0.2692782991784945\n",
      "Reconstruction loss: 224.5958014088426, KL divergence: 0.11600142672540026\n",
      "Reconstruction loss: 205.9104110004613, KL divergence: 0.05008454537679702\n",
      "Reconstruction loss: 191.97255612076196, KL divergence: 4.86063588220912\n",
      "Reconstruction loss: 323.9518654289959, KL divergence: 0.23366827170246796\n",
      "Reconstruction loss: 139.94432100835488, KL divergence: 0.8562249612060343\n",
      "Reconstruction loss: 298.4635528940322, KL divergence: 0.31150945149335835\n",
      "Reconstruction loss: 172.18368763519294, KL divergence: 5.343302592291138\n",
      "Reconstruction loss: 284.72705130366637, KL divergence: 0.10461410523578579\n",
      "Reconstruction loss: 182.228213086928, KL divergence: 0.13406165429827366\n",
      "Reconstruction loss: 185.73926539462775, KL divergence: 0.04451702910903521\n",
      "Reconstruction loss: 133.07734817147906, KL divergence: 0.26142031784666425\n",
      "Reconstruction loss: 193.04930935290497, KL divergence: 0.24887286638093675\n",
      "Reconstruction loss: 218.34356770903275, KL divergence: 0.04453529275825835\n",
      "Reconstruction loss: 242.58860198157114, KL divergence: 0.5448872468469788\n",
      "Reconstruction loss: 141.53587293435237, KL divergence: 0.7887763210483509\n",
      "Reconstruction loss: 259.02853781742505, KL divergence: 0.3366652457044902\n",
      "Reconstruction loss: 152.99543658809705, KL divergence: 0.2107357372574703\n",
      "Reconstruction loss: 148.4147480929842, KL divergence: 0.38889361849317283\n",
      "Reconstruction loss: 265.7198524995241, KL divergence: 2.0615360718493783\n",
      "Reconstruction loss: 198.26523424218055, KL divergence: 0.07162053806565133\n",
      "Reconstruction loss: 185.77397295488626, KL divergence: 0.4595622908435859\n",
      "Reconstruction loss: 188.57502517328697, KL divergence: 0.21289966152845863\n",
      "Reconstruction loss: 216.165802451174, KL divergence: 0.20999673284854703\n",
      "Reconstruction loss: 208.75052514151733, KL divergence: 0.09261781331170316\n",
      "Reconstruction loss: 206.97060466674665, KL divergence: 0.11051532070834652\n",
      "Reconstruction loss: 232.54305628840515, KL divergence: 0.1238204795436893\n",
      "Reconstruction loss: 205.65047079890596, KL divergence: 0.11661328971071316\n",
      "Reconstruction loss: 177.66931354689183, KL divergence: 0.21321804951909828\n",
      "Reconstruction loss: 228.81123685631331, KL divergence: 0.07918957628046158\n",
      "Reconstruction loss: 175.4552590121404, KL divergence: 0.4579710624158655\n",
      "Reconstruction loss: 215.93850586188648, KL divergence: 0.23112952343072346\n",
      "Reconstruction loss: 213.01188568539555, KL divergence: 0.5557236155222978\n",
      "Reconstruction loss: 347.3385584160598, KL divergence: 0.07527900126594983\n",
      "Reconstruction loss: 200.77357475056817, KL divergence: 0.0655210050698734\n",
      "Reconstruction loss: 240.58578447888567, KL divergence: 0.05798126515427532\n",
      "Reconstruction loss: 234.5727781920623, KL divergence: 0.1503249921983001\n",
      "Reconstruction loss: 184.68233340912764, KL divergence: 0.06876095428150347\n",
      "Reconstruction loss: 282.32121461866654, KL divergence: 0.28548341611679257\n",
      "Reconstruction loss: 175.1422678701129, KL divergence: 0.04326494808857512\n",
      "Reconstruction loss: 140.15736177720478, KL divergence: 0.24401684246533878\n",
      "Reconstruction loss: 201.14756460732158, KL divergence: 0.04264425029037944\n",
      "Reconstruction loss: 130.29895620673645, KL divergence: 0.8915050746336731\n",
      "Reconstruction loss: 206.12712492327432, KL divergence: 0.05072366488081287\n",
      "Reconstruction loss: 241.02798337269243, KL divergence: 0.37358130289661995\n",
      "Reconstruction loss: 120.2893004702226, KL divergence: 1.0147278432210998\n",
      "Reconstruction loss: 290.17154507355974, KL divergence: 1.9120027978487788\n",
      "Reconstruction loss: 243.37335146844052, KL divergence: 0.3074127361868625\n",
      "Reconstruction loss: 216.59689157727834, KL divergence: 0.16431770894075715\n",
      "Reconstruction loss: 133.35563120362147, KL divergence: 0.2612168194514707\n",
      "Reconstruction loss: 212.80431858373447, KL divergence: 0.04760755410942863\n",
      "Reconstruction loss: 311.8736974271799, KL divergence: 0.29457752493219586\n",
      "Reconstruction loss: 276.09548575481983, KL divergence: 0.06928615312878439\n",
      "Reconstruction loss: 239.93260297012208, KL divergence: 0.6578647716625796\n",
      "Reconstruction loss: 217.67277984837398, KL divergence: 2.915839005093258\n",
      "Reconstruction loss: 199.8401118423349, KL divergence: 0.6874644285397139\n",
      "Reconstruction loss: 272.5897640463456, KL divergence: 2.7360916831465394\n",
      "Reconstruction loss: 146.10375662804137, KL divergence: 0.4719817722406242\n",
      "Reconstruction loss: 116.79981865782969, KL divergence: 0.986978602090867\n",
      "Reconstruction loss: 171.78739297429536, KL divergence: 0.05793462933857413\n",
      "Reconstruction loss: 222.49902837106194, KL divergence: 0.0486657236149427\n",
      "Reconstruction loss: 151.05609243906014, KL divergence: 0.21487861852714563\n",
      "Reconstruction loss: 129.1576432240933, KL divergence: 0.7788624765560362\n",
      "Reconstruction loss: 257.5231361295645, KL divergence: 2.767862282335538\n",
      "Reconstruction loss: 145.00424600288088, KL divergence: 0.9956845267515513\n",
      "Reconstruction loss: 172.91593794721368, KL divergence: 0.15087132263594005\n",
      "Reconstruction loss: 113.8110030025041, KL divergence: 0.7542726887205226\n",
      "Reconstruction loss: 172.76619107302102, KL divergence: 0.04332739375147415\n",
      "Reconstruction loss: 126.10771509003523, KL divergence: 0.5807890938489941\n",
      "Reconstruction loss: 123.36043659479908, KL divergence: 0.35811894882196793\n",
      "Reconstruction loss: 247.85692841566686, KL divergence: 0.0586197916265247\n",
      "Reconstruction loss: 213.57484693871754, KL divergence: 0.06151472910615735\n",
      "Reconstruction loss: 203.9053550168404, KL divergence: 0.06990820704151729\n",
      "Reconstruction loss: 174.26728322690556, KL divergence: 0.20932581070032324\n",
      "Reconstruction loss: 196.19282007006206, KL divergence: 0.20650589407383335\n",
      "Reconstruction loss: 132.0132377881108, KL divergence: 0.3507265989157872\n",
      "Reconstruction loss: 142.46936644215464, KL divergence: 0.21487861852714563\n",
      "Reconstruction loss: 200.25529593015995, KL divergence: 0.051305894106056515\n",
      "Reconstruction loss: 199.63109992580536, KL divergence: 0.29364154751227284\n",
      "Reconstruction loss: 205.4471785987199, KL divergence: 0.04349225315961258\n",
      "Reconstruction loss: 255.28183738353903, KL divergence: 1.484343338542343\n",
      "Reconstruction loss: 136.70448167517293, KL divergence: 0.9895436407789144\n",
      "Reconstruction loss: 256.54628883963045, KL divergence: 1.244969029092032\n",
      "Reconstruction loss: 271.30674278251, KL divergence: 0.13704498186007202\n",
      "Reconstruction loss: 347.7322415312562, KL divergence: 0.0685827971082264\n",
      "Reconstruction loss: 123.90158778045584, KL divergence: 1.1527782684342494\n",
      "Reconstruction loss: 220.5058064310207, KL divergence: 0.055851966898936345\n",
      "Reconstruction loss: 146.41327225427204, KL divergence: 0.3907512649149565\n",
      "Reconstruction loss: 195.5948035688122, KL divergence: 0.09047977611311764\n",
      "Reconstruction loss: 216.23147935887147, KL divergence: 0.10510269012557605\n",
      "Reconstruction loss: 279.29219081515856, KL divergence: 0.36391098412853934\n",
      "Reconstruction loss: 229.03312887653954, KL divergence: 4.81275364311498\n",
      "Reconstruction loss: 241.48616435491874, KL divergence: 0.23249838411397716\n",
      "Reconstruction loss: 154.9189843820328, KL divergence: 0.13884663575826894\n",
      "Reconstruction loss: 241.4320699088047, KL divergence: 0.04396826343970278\n",
      "Reconstruction loss: 228.8931116613045, KL divergence: 0.7918333690925073\n",
      "Reconstruction loss: 256.8394649658642, KL divergence: 0.13571041093292496\n",
      "Reconstruction loss: 179.54286003800303, KL divergence: 0.04754172947200502\n",
      "Reconstruction loss: 272.0940971249257, KL divergence: 0.10472348215601263\n",
      "Reconstruction loss: 217.99713669560475, KL divergence: 0.05177184688850217\n",
      "Reconstruction loss: 279.52456373614245, KL divergence: 3.0734999096590316\n",
      "Reconstruction loss: 224.35327966538122, KL divergence: 0.08941383872268316\n",
      "Reconstruction loss: 161.64830649875097, KL divergence: 0.2168338154800718\n",
      "Reconstruction loss: 185.0222068306764, KL divergence: 0.1446769984062109\n",
      "Reconstruction loss: 206.81798503342765, KL divergence: 1.0051422527216074\n",
      "Reconstruction loss: 155.4985438617822, KL divergence: 0.13290987597928294\n",
      "Reconstruction loss: 311.062630936911, KL divergence: 0.5997779281077513\n",
      "Reconstruction loss: 158.8013466737882, KL divergence: 0.2168338154800718\n",
      "Reconstruction loss: 226.66770758100887, KL divergence: 0.34631466235789643\n",
      "Reconstruction loss: 212.5490519996202, KL divergence: 0.05895323263177682\n",
      "Reconstruction loss: 185.22477751446036, KL divergence: 0.2097234031734438\n",
      "Reconstruction loss: 135.8987669734143, KL divergence: 0.2733180917106571\n",
      "Reconstruction loss: 204.39378072222394, KL divergence: 0.05954626715146433\n",
      "Reconstruction loss: 179.18401620409747, KL divergence: 0.2167023241300538\n",
      "Reconstruction loss: 134.32060579596157, KL divergence: 0.5553825045237619\n",
      "Reconstruction loss: 181.94845087071246, KL divergence: 0.09709532669727888\n",
      "Reconstruction loss: 240.02784499601387, KL divergence: 0.22137337000727153\n",
      "Reconstruction loss: 245.34722391990812, KL divergence: 0.07061751354024676\n",
      "Reconstruction loss: 217.5990290626793, KL divergence: 0.19209383819831405\n",
      "Reconstruction loss: 285.1011071116819, KL divergence: 0.9133969349817166\n",
      "Reconstruction loss: 151.4637703876062, KL divergence: 0.1425845176257836\n",
      "Reconstruction loss: 195.40184436319544, KL divergence: 1.8575524539123296\n",
      "Reconstruction loss: 147.10853095484745, KL divergence: 0.35855328048391394\n",
      "Reconstruction loss: 259.0327227460684, KL divergence: 3.097499208825811\n",
      "Reconstruction loss: 157.28422423015996, KL divergence: 0.21799986972236501\n",
      "Reconstruction loss: 196.30793253191752, KL divergence: 0.21799986972236501\n",
      "Reconstruction loss: 213.01434445338083, KL divergence: 0.08367424280324681\n",
      "Reconstruction loss: 237.61267255844052, KL divergence: 0.5575213009015914\n",
      "Reconstruction loss: 243.27202573133957, KL divergence: 0.9486537084245094\n",
      "Reconstruction loss: 217.87826917943283, KL divergence: 1.1950280718990423\n",
      "Reconstruction loss: 113.71292661197643, KL divergence: 0.8587102928393372\n",
      "Reconstruction loss: 282.0274914456649, KL divergence: 0.0542100034935763\n",
      "Reconstruction loss: 237.36592912215633, KL divergence: 0.044153156847767605\n",
      "Reconstruction loss: 296.27768398552587, KL divergence: 0.6034378670997607\n",
      "Reconstruction loss: 150.77862470787704, KL divergence: 0.21799986972236501\n",
      "Reconstruction loss: 269.41364878739876, KL divergence: 0.18753451466654103\n",
      "Reconstruction loss: 213.33158479993722, KL divergence: 0.048802408562636346\n",
      "Reconstruction loss: 162.3954352711731, KL divergence: 0.3002701999096452\n",
      "Reconstruction loss: 227.5512189675071, KL divergence: 0.1073614623375948\n",
      "Reconstruction loss: 228.69001517761194, KL divergence: 0.3365435452090427\n",
      "Reconstruction loss: 192.04758119002773, KL divergence: 0.046667736197409826\n",
      "Reconstruction loss: 217.084247151286, KL divergence: 0.044837084516737624\n",
      "Reconstruction loss: 152.03734125237108, KL divergence: 0.1966962987179494\n",
      "Reconstruction loss: 250.8495885903698, KL divergence: 0.41234861223977953\n",
      "Reconstruction loss: 177.74491018558345, KL divergence: 0.09777259586994963\n",
      "Reconstruction loss: 219.80484474166295, KL divergence: 0.04418762837921003\n",
      "Reconstruction loss: 205.86764012343076, KL divergence: 0.045095038394414866\n",
      "Reconstruction loss: 209.09373732177784, KL divergence: 0.05058474742771307\n",
      "Reconstruction loss: 248.3191226354593, KL divergence: 0.1254401506540791\n",
      "Reconstruction loss: 182.99320593154653, KL divergence: 0.2119949161444743\n",
      "Reconstruction loss: 193.70555165436153, KL divergence: 0.10109649076756383\n",
      "Reconstruction loss: 154.65676281795072, KL divergence: 0.25735780350781334\n",
      "Reconstruction loss: 206.9306687289258, KL divergence: 1.987906230478726\n",
      "Reconstruction loss: 168.12743264185036, KL divergence: 0.13776504717520838\n",
      "Reconstruction loss: 161.29013058961277, KL divergence: 0.21884847320557094\n",
      "Reconstruction loss: 265.87831614861307, KL divergence: 0.6897194006115439\n",
      "Reconstruction loss: 207.19242555096065, KL divergence: 0.08567614795343664\n",
      "Reconstruction loss: 129.83475918059068, KL divergence: 0.2776146895581112\n",
      "Reconstruction loss: 196.70201175107331, KL divergence: 0.054463686771343334\n",
      "Reconstruction loss: 237.83191473904344, KL divergence: 0.04646411561676128\n",
      "Reconstruction loss: 192.64775527188363, KL divergence: 0.08659016257680119\n",
      "Reconstruction loss: 255.53108247865018, KL divergence: 0.11587177235709151\n",
      "Reconstruction loss: 142.56772476750558, KL divergence: 0.21884847320557094\n",
      "Reconstruction loss: 249.7452888267045, KL divergence: 0.10206039376878057\n",
      "Reconstruction loss: 268.2818913441197, KL divergence: 2.6082491784804915\n",
      "Reconstruction loss: 175.39903758286658, KL divergence: 0.23466150965675392\n",
      "Reconstruction loss: 215.18761039264734, KL divergence: 0.04500081458378119\n",
      "Reconstruction loss: 161.21503439421588, KL divergence: 0.21544713851551145\n",
      "Reconstruction loss: 201.76550806981328, KL divergence: 0.23593642811364096\n",
      "Reconstruction loss: 198.182867477375, KL divergence: 0.15546435843591333\n",
      "Reconstruction loss: 270.60984699570554, KL divergence: 3.804229294660399\n",
      "Reconstruction loss: 212.20514452614222, KL divergence: 0.35076255168635473\n",
      "Reconstruction loss: 204.2687108205145, KL divergence: 0.09209708238106201\n",
      "Reconstruction loss: 215.71755396461313, KL divergence: 0.20338403473174294\n",
      "Reconstruction loss: 240.941421635594, KL divergence: 0.15474125370854191\n",
      "Reconstruction loss: 199.64366951938717, KL divergence: 0.07794877801992911\n",
      "Reconstruction loss: 321.562003273786, KL divergence: 0.35256693667732214\n",
      "Reconstruction loss: 142.6387788131992, KL divergence: 0.22257844906355745\n",
      "Reconstruction loss: 194.23764278493442, KL divergence: 0.06827377539704527\n",
      "Reconstruction loss: 118.48668168430184, KL divergence: 0.8271984625737461\n",
      "Reconstruction loss: 141.78413684233135, KL divergence: 0.3118740214025606\n",
      "Reconstruction loss: 200.7978438178047, KL divergence: 0.15139268316546078\n",
      "Reconstruction loss: 110.95019397118789, KL divergence: 1.248241365877498\n",
      "Reconstruction loss: 198.92878276589852, KL divergence: 0.04667222560483908\n",
      "Reconstruction loss: 165.23153433125964, KL divergence: 0.47874446685306526\n",
      "Reconstruction loss: 188.49775941268234, KL divergence: 0.21843907199260448\n",
      "Reconstruction loss: 251.03216615996178, KL divergence: 0.08978887672673275\n",
      "Reconstruction loss: 255.853624194617, KL divergence: 1.835931309616666\n",
      "Reconstruction loss: 139.39674137991656, KL divergence: 0.39943843498387943\n",
      "Reconstruction loss: 94.19666640344761, KL divergence: 0.8666653635841797\n",
      "Reconstruction loss: 260.34601835402464, KL divergence: 1.2788148359955538\n",
      "Reconstruction loss: 299.50632662789815, KL divergence: 0.0665221178457645\n",
      "Reconstruction loss: 222.66849010682392, KL divergence: 5.638089247190097\n",
      "Reconstruction loss: 114.6740488242933, KL divergence: 1.448328556096333\n",
      "Reconstruction loss: 157.73018234799568, KL divergence: 0.3816358290891952\n",
      "Reconstruction loss: 170.04669837414212, KL divergence: 0.11391574159863976\n",
      "Reconstruction loss: 160.91195635781588, KL divergence: 6.128285972379592\n",
      "Reconstruction loss: 213.5182349382223, KL divergence: 1.4441664421843923\n",
      "Reconstruction loss: 141.25038556247517, KL divergence: 0.21550352371089254\n",
      "Reconstruction loss: 240.96673422064282, KL divergence: 0.12238033317561403\n",
      "Reconstruction loss: 155.37758088069262, KL divergence: 0.18874442362736393\n",
      "Reconstruction loss: 193.45833619032447, KL divergence: 7.076219934011082\n",
      "Reconstruction loss: 160.07201503748757, KL divergence: 0.21843907199260448\n",
      "Reconstruction loss: 304.018149472314, KL divergence: 0.8039254793969829\n",
      "Reconstruction loss: 257.4376145177529, KL divergence: 0.10927286359413962\n",
      "Reconstruction loss: 247.96669918981766, KL divergence: 2.7941617197318136\n",
      "Reconstruction loss: 128.12358236814964, KL divergence: 1.2695206087311606\n",
      "Reconstruction loss: 256.63266080983556, KL divergence: 0.08114764587589113\n",
      "Reconstruction loss: 204.44858190464728, KL divergence: 0.07355011741947615\n",
      "Reconstruction loss: 107.65089537310237, KL divergence: 0.7329331419455427\n",
      "Reconstruction loss: 210.93022062297342, KL divergence: 0.07552906621752004\n",
      "Reconstruction loss: 179.62261032528698, KL divergence: 0.3160060916380872\n",
      "Reconstruction loss: 197.79452090455376, KL divergence: 0.2128371429524697\n",
      "Reconstruction loss: 151.53776122534444, KL divergence: 0.5547900459152552\n",
      "Reconstruction loss: 177.86607734571146, KL divergence: 0.14158493087014756\n",
      "Reconstruction loss: 208.4070998528781, KL divergence: 5.126477052825094\n",
      "Reconstruction loss: 210.01963285073404, KL divergence: 0.06173249191224345\n",
      "Reconstruction loss: 216.74196038593396, KL divergence: 0.42254584332687295\n",
      "Reconstruction loss: 227.6624030055927, KL divergence: 1.250333147003504\n",
      "Reconstruction loss: 225.3274428332311, KL divergence: 0.42533326237339647\n",
      "Reconstruction loss: 202.566606779568, KL divergence: 0.2562017639137483\n",
      "Reconstruction loss: 271.8714236050515, KL divergence: 1.485766447368432\n",
      "Reconstruction loss: 248.78903037164918, KL divergence: 0.6870943620677941\n",
      "Reconstruction loss: 159.22918850097813, KL divergence: 0.21783187459808873\n",
      "Reconstruction loss: 275.97762781388053, KL divergence: 0.49613497532220574\n",
      "Reconstruction loss: 190.33291893397706, KL divergence: 0.06830605015919683\n",
      "Reconstruction loss: 193.91037970831246, KL divergence: 0.11689519004141868\n",
      "Reconstruction loss: 265.04451350593285, KL divergence: 0.4680938508784036\n",
      "Reconstruction loss: 182.61322392331977, KL divergence: 0.19635114817450883\n",
      "Reconstruction loss: 178.28238729433144, KL divergence: 5.826623386073795\n",
      "Reconstruction loss: 156.70907651819135, KL divergence: 0.4154472161357159\n",
      "Reconstruction loss: 235.929548658344, KL divergence: 0.21420685041540988\n",
      "Reconstruction loss: 265.41232462442326, KL divergence: 1.684161438986242\n",
      "Reconstruction loss: 106.8775566130684, KL divergence: 1.4192499387574897\n",
      "Reconstruction loss: 267.1891752712884, KL divergence: 0.11293354750530976\n",
      "Reconstruction loss: 205.76856355328897, KL divergence: 0.04557737737602868\n",
      "Reconstruction loss: 266.4235287438562, KL divergence: 0.9751704666083253\n",
      "Reconstruction loss: 229.56672634280315, KL divergence: 0.32756147985634604\n",
      "Reconstruction loss: 332.365671386792, KL divergence: 0.9018264034476546\n",
      "Reconstruction loss: 220.01330630639052, KL divergence: 0.06072654340652178\n",
      "Reconstruction loss: 111.86313259058107, KL divergence: 1.2440277226324976\n",
      "Reconstruction loss: 203.21796689091934, KL divergence: 0.06548832861669024\n",
      "Reconstruction loss: 195.44095631100316, KL divergence: 0.046875064981003245\n",
      "Reconstruction loss: 174.34883149084743, KL divergence: 0.19246447188921595\n",
      "Reconstruction loss: 273.95601664146415, KL divergence: 3.4659597897531897\n",
      "Reconstruction loss: 277.1942542364135, KL divergence: 0.23671861174391817\n",
      "Reconstruction loss: 103.84608500797432, KL divergence: 1.0223873924536562\n",
      "Reconstruction loss: 164.7371238386285, KL divergence: 0.2351078338561854\n",
      "Reconstruction loss: 166.8978084694939, KL divergence: 0.21039557015637056\n",
      "Reconstruction loss: 186.25760143272993, KL divergence: 0.05367929341301769\n",
      "Reconstruction loss: 204.96243463307277, KL divergence: 0.13714617526704276\n",
      "Reconstruction loss: 149.20290668879795, KL divergence: 0.5312219617748036\n",
      "Reconstruction loss: 143.37280187234953, KL divergence: 0.21581564141317083\n",
      "Reconstruction loss: 148.5557328073729, KL divergence: 0.29156381089384203\n",
      "Reconstruction loss: 227.42052818676015, KL divergence: 0.06175659475843348\n",
      "Reconstruction loss: 198.08417086587315, KL divergence: 0.1119086105682981\n",
      "Reconstruction loss: 140.0318562095866, KL divergence: 0.8925903206069944\n",
      "Reconstruction loss: 153.9819393663422, KL divergence: 0.2208595766326577\n",
      "Reconstruction loss: 156.50170133352466, KL divergence: 0.265757998086191\n",
      "Reconstruction loss: 130.00019871968829, KL divergence: 0.2174586126102332\n",
      "Reconstruction loss: 217.21532827309068, KL divergence: 0.044121749343399186\n",
      "Reconstruction loss: 189.13697374359617, KL divergence: 0.07184464405045082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     31\u001b[0m vae \u001b[38;5;241m=\u001b[39m VAE(\n\u001b[1;32m     32\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     33\u001b[0m     encoder_topology\u001b[38;5;241m=\u001b[39mencoder_topology,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     decoder_optimizer\u001b[38;5;241m=\u001b[39mdecoder_optimizer\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#training_data = training_data[:1000]  # Adjust the number of training samples as needed\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust epochs as needed\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Select a test sample\u001b[39;00m\n\u001b[1;32m     49\u001b[0m test_sample, _ \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/sia/tp5/src/core/vae.py:305\u001b[0m, in \u001b[0;36mVAE.fit\u001b[0;34m(self, training_data, epochs, mini_batch_size)\u001b[0m\n\u001b[1;32m    303\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mini_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m--> 305\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_weights_and_biases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m avg_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(mini_batch)\n\u001b[1;32m    307\u001b[0m epoch_avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m n\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/sia/tp5/src/core/vae.py:233\u001b[0m, in \u001b[0;36mVAE.update_weights_and_biases\u001b[0;34m(self, mini_batch)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, _ \u001b[38;5;129;01min\u001b[39;00m mini_batch:\n\u001b[1;32m    222\u001b[0m     (\n\u001b[1;32m    223\u001b[0m         recon_x,\n\u001b[1;32m    224\u001b[0m         mu,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m         decoder_zs,\n\u001b[1;32m    232\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedforward(x)\n\u001b[0;32m--> 233\u001b[0m     loss, recon_loss, kl_div \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecon_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstruction loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecon_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, KL divergence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_div\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/sia/tp5/src/core/vae.py:132\u001b[0m, in \u001b[0;36mVAE.compute_loss\u001b[0;34m(self, x, recon_x, mu, log_var)\u001b[0m\n\u001b[1;32m    127\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m    128\u001b[0m     x \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(recon_x \u001b[38;5;241m+\u001b[39m epsilon) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m recon_x \u001b[38;5;241m+\u001b[39m epsilon)\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# KL divergence\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m kl_div \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Total loss\u001b[39;00m\n\u001b[1;32m    135\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m kl_div\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/sia5/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2389\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[1;32m   2392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/sia5/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:70\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 70\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.activation_function import ActivationFunction\n",
    "from core.optimizer import Optimizer\n",
    "from core.vae import VAE\n",
    "\n",
    "# Define the encoder and decoder topologies\n",
    "latent_dim = 32 # Size of the latent space\n",
    "encoder_topology = [784, 256, 128, 2 * latent_dim]\n",
    "decoder_topology = [latent_dim, 128, 256, 784]\n",
    "\n",
    "\n",
    "# Configure separate optimizers for encoder and decoder\n",
    "encoder_optimizer = Optimizer(\n",
    "    method=\"adam\",\n",
    "    eta=0.1  # Learning rate\n",
    ")\n",
    "\n",
    "decoder_optimizer = Optimizer(\n",
    "    method=\"adam\",\n",
    "    eta=0.1  # Learning rate\n",
    ")\n",
    "\n",
    "# Configure the activation function\n",
    "activation_function = ActivationFunction(\n",
    "    method=\"relu\"\n",
    ")\n",
    "\n",
    "# Create the VAE model\n",
    "vae = VAE(\n",
    "    seed=42,\n",
    "    encoder_topology=encoder_topology,\n",
    "    decoder_topology=decoder_topology,\n",
    "    activation_function=activation_function,\n",
    "    encoder_optimizer=encoder_optimizer,\n",
    "    decoder_optimizer=decoder_optimizer\n",
    ")\n",
    "\n",
    "training_data = training_data[:10000]  # Adjust the number of training samples as needed\n",
    "# Train the model\n",
    "vae.fit(\n",
    "    training_data=training_data,\n",
    "    epochs=20,  # Adjust epochs as needed\n",
    "    mini_batch_size=32,\n",
    ")\n",
    "\n",
    "# Select a test sample\n",
    "test_sample, _ = test_data[0]\n",
    "recon_x, _, _, _, _, _, _, _, _ = vae.feedforward(test_sample)\n",
    "\n",
    "# Visualize the original and reconstructed images\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_sample.reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(recon_x.reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(\"Reconstructed\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAERCAYAAABSGLrIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAS4UlEQVR4nO3da2xU1dfH8TWd1t47Bbm1UIq03AoIXiIJgkGEoCAJJogQlBIUI1EJCFETgq8MBDUaA4gkJggmvBJEVIig8kIJCd4AEaoUSxEohdICpVyEdj8vfJg4/5K9js60tKzvJzGxs07P7M7Mj1NY++wdcs45AXBLS7rZAwDQ8gg6YABBBwwg6IABBB0wgKADBhB0wACCDhhA0AEDCDpa3MyZM6VXr143eximEfRWUFFRIS+88IL07dtXMjIyJCMjQ0pKSuT555+Xffv23ezhwYDkmz2AW93nn38uTzzxhCQnJ8v06dNlyJAhkpSUJGVlZbJx40ZZtWqVVFRUSGFh4c0eKm5hBL0FHT58WKZOnSqFhYXy9ddfS15eXkx92bJl8t5770lSUtv8xaqhoUEyMzNv9jCQAG3zE3aLeOONN6ShoUHWrFnTLOQiIsnJyTJ37lwpKCiIPlZWViaTJ0+Wjh07Slpamtx7772yefPmmO/78MMPJRQKyc6dO+Wll16Szp07S2Zmpjz22GNy+vTpZs+zdetWGTlypGRmZkp2drZMmDBBfv3115hjZs6cKVlZWXL48GEZP368ZGdny/Tp00VE5Ntvv5XHH39cevbsKampqVJQUCDz58+XS5cuNXuuTZs2yaBBgyQtLU0GDRokn3zyyQ1fm4aGBlmwYIEUFBRIamqq9OvXT9566y3hZsoW4tBi8vPzXXFxceDj9+/f7yKRiCspKXHLli1zK1ascA888IALhUJu48aN0ePWrFnjRMTdddddbvTo0W758uVuwYIFLhwOuylTpsScc926dS4UCrmHH37YLV++3C1btsz16tXL5ebmuoqKiuhxpaWlLjU11RUVFbnS0lL3/vvvu3Xr1jnnnHvxxRfd+PHj3ZIlS9zq1avd008/7cLhsJs8eXLMc3355ZcuKSnJDRo0yL399ttu0aJFLhKJuIEDB7rCwsLocU1NTW706NEuFAq5Z555xq1YscJNnDjRiYibN2/ev3iFERRBbyHnzp1zIuImTZrUrFZXV+dOnz4d/e/ixYvOOeceeughN3jwYHf58uXosU1NTW748OGuT58+0ceuB33MmDGuqakp+vj8+fNdOBx2Z8+edc45V19f73Jzc93s2bNjnv/kyZMuEonEPF5aWupExL366qvNxnt9fP+0dOlSFwqFXGVlZfSxoUOHury8vOjzO+fctm3bnIjEBH3Tpk1ORNzrr78ec87Jkye7UCjkysvLmz0f4sOv7i3k/PnzIiKSlZXVrDZq1Cjp3Llz9L+VK1dKbW2tfPPNNzJlyhSpr6+XmpoaqampkTNnzsi4cePk0KFDcvz48ZjzPPvssxIKhaJfjxw5UhobG6WyslJERLZv3y5nz56VadOmRc9XU1Mj4XBYhg0bJjt27Gg2tjlz5jR7LD09Pfr/DQ0NUlNTI8OHDxfnnPz8888iIlJVVSV79uyR0tJSiUQi0ePHjh0rJSUlMefbsmWLhMNhmTt3bszjCxYsEOecbN269cYvKv4z/jGuhWRnZ4uIyIULF5rVVq9eLfX19VJdXS1PPvmkiIiUl5eLc04WL14sixcvvuE5T506Jd27d49+3bNnz5h6hw4dRESkrq5OREQOHTokIiKjR4++4flycnJivk5OTpYePXo0O+7o0aPy2muvyebNm6Pnvu7cuXMiItE/XPr06dPs+/v16yc//fRT9OvKykrJz8+PvkbXDRgwIOZcSByC3kIikYjk5eXJ/v37m9WGDRsmIiJHjhyJPtbU1CQiIgsXLpRx48bd8JzFxcUxX4fD4Rse5/7/H7Sun/Ojjz6Sbt26NTsuOTn27U9NTW3WAWhsbJSxY8dKbW2tvPLKK9K/f3/JzMyU48ePy8yZM6PPgbaNoLegCRMmyAcffCC7d++W++67z3ts7969RUQkJSVFxowZk5DnLyoqEhGRLl26/Odz/vLLL/L777/L2rVrZcaMGdHHt2/fHnPc9XkA13+L+Kfffvut2bFfffWV1NfXx1zVy8rKYs6FxOHv6C3o5ZdfloyMDJk1a5ZUV1c3q7t/tJK6dOkio0aNktWrV0tVVVWzY2/UNtOMGzdOcnJyZMmSJXL16tX/dM7rvzX8c6zOOXn33XdjjsvLy5OhQ4fK2rVro7/Oi/z9B8KBAwdijh0/frw0NjbKihUrYh5/5513JBQKySOPPKL/cPhXuKK3oD59+sj69etl2rRp0q9fv+jMOOecVFRUyPr16yUpKSn69+KVK1fKiBEjZPDgwTJ79mzp3bu3VFdXy65du+TYsWOyd+/ef/X8OTk5smrVKnnqqafk7rvvlqlTp0rnzp3l6NGj8sUXX8j999/fLGz/q3///lJUVCQLFy6U48ePS05OjmzYsKHZ39VFRJYuXSoTJkyQESNGyKxZs6S2tlaWL18uAwcOjPm3iokTJ8qDDz4oixYtkiNHjsiQIUNk27Zt8umnn8q8efOiv4kggW7eP/jbUV5e7ubMmeOKi4tdWlqaS09Pd/3793fPPfec27NnT8yxhw8fdjNmzHDdunVzKSkprnv37u7RRx91H3/8cfSY6+2177//PuZ7d+zY4UTE7dixo9nj48aNc5FIxKWlpbmioiI3c+ZM98MPP0SPKS0tdZmZmTcc/4EDB9yYMWNcVlaW69Spk5s9e7bbu3evExG3Zs2amGM3bNjgBgwY4FJTU11JSYnbuHGjKy0tjWmvOfd362/+/PkuPz/fpaSkuD59+rg333wzpl2IxAk5x1Qk4FbH39EBAwg6YABBBwwg6IABBB0wgKADBhB0wIDAM+P+eTskWpb2Wsc79SHIe8n0ivYjyHvFFR0wgKADBhB0wACCDhhA0AEDCDpgAEEHDDC1wkx76R+39Bjaws+Iv7XWZ5IrOmAAQQcMIOiAAQQdMICgAwYQdMAAgg4YYKqPbqV/3NL3swd5jtYYw63wfrbWz8AVHTCAoAMGEHTAAIIOGEDQAQMIOmAAQQcMIOiAAQnbwKE9TF5IxCYUrbF5QkufIxwOq8fcdtttcY0hKcl/Dfnrr7/UMWjPceXKlbi+P8h72R4+10FwRQcMIOiAAQQdMICgAwYQdMAAgg4YQNABAwL30W+FfmIifoYgPWgfrb8sIpKSkuKtZ2dne+uRSMRb79ixozqGnJwcb/3q1aveekZGhrd+/PhxdQza+1VXV+et19TUeOuXL19Wx6BpL7ngig4YQNABAwg6YABBBwwg6IABBB0wgKADBrSpDRzivX843k0FEnEOrX/cs2dPdQxdu3aN6xxDhw711vPy8tQxJCf7Pxpar762ttZbr6qqUsewd+9eb/3gwYPeuvYznD59Wh2D1mtvbGz01puamtTnaA1c0QEDCDpgAEEHDCDogAEEHTCAoAMGEHTAgDbVR9f65Nq93PH22UX09cwzMzO9da3HrfXIRUQGDx7sraenp3vrvXv39taD9PJzc3O99X379nnr2j3vQdZ11+YDaD3sM2fOeOupqanqGLS149vCOv9BcEUHDCDogAEEHTCAoAMGEHTAAIIOGEDQAQMIOmBAq02YCTIxQJt8oN3Ery00EGQM2sYFnTp18ta1MRYVFalj0Bav0CbdNDQ0eOtBFp7Yvn27t96hQwdv/cSJE956kAkz58+f99a116GgoMBbv3btmjoGbeEJ7f3WnqO1NoDgig4YQNABAwg6YABBBwwg6IABBB0wgKADBrRaHz0R/cJ4N3hIS0tTn0NbWEI7R0lJibeu9fpFRC5duuSta4s6ZGdne+u7du1Sx6D1f3fu3BnXGLQ+vIi+MESXLl289W7dunnrNTU16hhOnTrlrWuLX8T7mU0UruiAAQQdMICgAwYQdMAAgg4YQNABAwg6YEC7uh9dO4e2wUMQ2r3cd9xxh7ceDoe99SCvQ1ZWlreu9XZPnjzprR85ckQdg3Y/ebxrAxQXF6tj0NYGuHDhgreu3fMepIetnUP7zGmvU2vhig4YQNABAwg6YABBBwwg6IABBB0wgKADBrSr+9E1Wo9au3dYRO+DX7161Vuvq6vz1rX73UX0cWrrnVdVVcVVFxG5cuWKt669Dtq94BcvXlTHEIlEvPXq6mpvXZtPcPbsWXUM2uc2yGeqLeCKDhhA0AEDCDpgAEEHDCDogAEEHTCAoAMGEHTAgFabMNMatJv8g2zgoNEW/dc2LgiygYM2qUZb7KBHjx7eepCFJ7RNJIqKirz1vn37euvagg4i+sITQSYf+WiLjIi0nw0aNFzRAQMIOmAAQQcMIOiAAQQdMICgAwYQdMCAdtVHj3eDhyA97IyMDG+9Y8eO3rq2wUNeXp46Bm1zA21xjMrKSm/9nnvuUcdw7do1b33EiBHeurY4RpAFG7T3u76+3lvX+uRBevntpU+u4YoOGEDQAQMIOmAAQQcMIOiAAQQdMICgAwa0qz66RrsfXesNi+h9dO1e8E6dOnnrw4YNU8dw2223eevaxgPaGHr16qWOQTumoKDAW9f66EE2cDh48KC3rvXBtT57ENpnqr302bmiAwYQdMAAgg4YQNABAwg6YABBBwwg6IABt1QfXbvfPCUlRT2H1hfNzc311gsLC9Xn0Jw7d85bP3HihLeurXeelZWljkGbT6C9lvn5+d76H3/8oY5Bez+1Hre2doD2OoroP2eQe9rbAq7ogAEEHTCAoAMGEHTAAIIOGEDQAQMIOmAAQQcMuKUmzGg3+QfZwEGbTNK9e/d/Nab/FWSCRV1dnbd+4cKFuJ4jyOtw5swZb13bqKJHjx7eepCFJ7RFPrT3Svs8aAt8iLSfCTEaruiAAQQdMICgAwYQdMAAgg4YQNABAwg6YEDC+uhtYSF7bZGA1NRU9RyNjY3e+uXLl731Dh06eOsVFRVxj0Fz+vRpbz0nJ0c9h7ZJhNbj1nrU3bp1U8dw+PBhb726utpbP3nypLcepI+uvRfa517TWhs8cEUHDCDogAEEHTCAoAMGEHTAAIIOGEDQAQMS1kdPRD8w3p5kOByO+/zaz9GlSxdvfdeuXd76tWvX1DFo94Jr8wEikYi3vnv3bnUM2uYHAwcO9Na1/nOQOQ3aBgva+6m91kHuNdfGqc2raCu4ogMGEHTAAIIOGEDQAQMIOmAAQQcMIOiAAe1qXXftfnOt95uZmak+h3aO8vJyb/3222/31nNzc9Ux5OXleevauu7auvBBxtCpUydvXZuzoNW/++47dQxlZWXeuvY6aJ+Xq1evqmOId22A1rrfXMMVHTCAoAMGEHTAAIIOGEDQAQMIOmAAQQcMIOiAAW1qwow2uUCrZ2VleetBJopok2rS09O9dW0DB+37g4yhR48e3ro2yUNbPENEpKCgwFvXJhb17NnTW9+zZ486hoaGBm/9zz//9Na1z8vFixfVMWjinRCTiMVQguCKDhhA0AEDCDpgAEEHDCDogAEEHTCAoAMGBO6ja/2+RPT6tMUKtB601n8OsmmAtiB/YWGht15UVOSta312EX0DBm3BBG1jgkmTJqlj2L9/v7eu9bg/++wzb/3HH39Ux3Du3DlvXXuv6uvrvfUgm2lox8T7uW+thSm4ogMGEHTAAIIOGEDQAQMIOmAAQQcMIOiAAYH76K3R72tqavLWk5P9w71y5Yq3HqSHrW3AUFxc7K0PGDDAW7/zzjvVMWj949raWm89Pz/fWz948KA6Bu1e72PHjnnrJ0+e9Na1n0FEpLKy0lvXNqrQ7jcPsjmD9plsL7iiAwYQdMAAgg4YQNABAwg6YABBBwwg6IABCVvXvTXuV9f6y0ePHvXWtfuXRfRevLY2fFKS/8/OmpoadQzafAHtvvwDBw5460H6x1u2bPHWtdehrKzMWw/Snz579qy3rn2mtJ8zyOvQWveLtzSu6IABBB0wgKADBhB0wACCDhhA0AEDCDpgAEEHDEjYhJlETCzQzqHVtQX7g2w6r51D2zyhoqLCW+/atas6Bm3CTJBFG3y0iSgi+uQk7RzaJhIXLlxQxxDvBgzahJjWmAyTiIlkQT63Gq7ogAEEHTCAoAMGEHTAAIIOGEDQAQMIOmBAyAVsJiailxeveMegLQohIhIOh7311NRUbz0lJcVb1xaNENF7q9oYtUUdtP6ziL5Ih9aj1vroQd4LK33yeAV5Dq7ogAEEHTCAoAMGEHTAAIIOGEDQAQMIOmBA4D661vdsDz3LIH34IP3dlqaNUxuj1l8O8jMG2dzAJxGfB+11CLIJRFsX5DMZ7zoNIlzRARMIOmAAQQcMIOiAAQQdMICgAwYQdMCAVrsfPRH9wtboo8d7jkTctx9v37QtrB3QHuZV3CroowMQEYIOmEDQAQMIOmAAQQcMIOiAAQQdMICgAwa0q4UnkBiJmDjUFjBh5m9MmAEgIgQdMIGgAwYQdMAAgg4YQNABAwg6YEDgPjqA9osrOmAAQQcMIOiAAQQdMICgAwYQdMAAgg4YQNABAwg6YMD/Ab0Pn6I3ZFy3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usamos un ejemplo del conjunto de test para feedforward\n",
    "test_sample, _ = test_data[0]\n",
    "\n",
    "# Pasar el test_sample por el modelo VAE\n",
    "recon_x, mu, log_var, z, epsilon, _, _, _, _ = vae.feedforward(test_sample)\n",
    "\n",
    "# Visualizar la imagen original y la reconstruida\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "# Imagen original\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_sample.reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Imagen reconstruida\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(recon_x.reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(\"Reconstruido\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generar una nueva imagen aleatoria desde el espacio latente\n",
    "z_random = np.random.normal(size=(vae.latent_dim, 1))  # Vector aleatorio en el espacio latente\n",
    "generated_x, _, _ = vae.decode(z_random)  # Decodificar desde el vector latente\n",
    "\n",
    "# Visualizar la imagen generada\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(generated_x.reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(\"Generado\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Generar una nueva muestra aleatoria del espacio latente\u001b[39;00m\n\u001b[1;32m      4\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Dimensin del espacio latente = 2\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m new_sample \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(z)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Reconstruir la imagen para visualizarla\u001b[39;00m\n\u001b[1;32m      8\u001b[0m new_sample_image \u001b[38;5;241m=\u001b[39m new_sample\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generar una nueva muestra aleatoria del espacio latente\n",
    "z = np.random.normal(size=(2, 1))  # Dimensin del espacio latente = 2\n",
    "new_sample = vae.generate(z)\n",
    "\n",
    "# Reconstruir la imagen para visualizarla\n",
    "new_sample_image = new_sample.reshape(28, 28)\n",
    "\n",
    "# Mostrar la imagen generada\n",
    "plt.imshow(new_sample_image, cmap='gray')\n",
    "plt.title(\"Imagen generada\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar algunas imgenes del conjunto de prueba\n",
    "test_samples = x_test[:10]\n",
    "\n",
    "# Reconstruir las imgenes\n",
    "reconstructed_samples = [vae.feedforward(x)[0] for x in test_samples]\n",
    "\n",
    "# Mostrar las originales y las reconstruidas\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    # Mostrar la imagen original\n",
    "    axes[0, i].imshow(test_samples[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Mostrar la imagen reconstruida\n",
    "    axes[1, i].imshow(reconstructed_samples[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_title(\"Originales\")\n",
    "axes[1, 0].set_title(\"Reconstruidas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "reconstruction_loss = 0\n",
    "kl_divergence = 0\n",
    "\n",
    "for x in x_test:\n",
    "    recon_x, mu, log_var, _, _ = vae.feedforward(x)\n",
    "    loss, rec_loss, kl_loss = vae.cost_function(x, recon_x, mu, log_var)\n",
    "    total_loss += loss\n",
    "    reconstruction_loss += rec_loss\n",
    "    kl_divergence += kl_loss\n",
    "\n",
    "n = len(x_test)\n",
    "print(f\"Prdida total promedio: {total_loss / n}\")\n",
    "print(f\"Prdida de reconstruccin promedio: {reconstruction_loss / n}\")\n",
    "print(f\"Divergencia KL promedio: {kl_divergence / n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_points = []\n",
    "labels = []\n",
    "\n",
    "# Mapear las imgenes del conjunto de prueba al espacio latente\n",
    "for x in x_test[:1000]:\n",
    "    _, mu, _, _, _ = vae.feedforward(x)\n",
    "    latent_points.append(mu)\n",
    "    labels.append(np.argmax(x))  # Opcional: etiqueta de clase para colorear\n",
    "\n",
    "latent_points = np.array(latent_points)\n",
    "\n",
    "# Graficar el espacio latente\n",
    "plt.scatter(latent_points[:, 0], latent_points[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.title(\"Representacin en el espacio latente\")\n",
    "plt.xlabel(\"Latente 1\")\n",
    "plt.ylabel(\"Latente 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir dos puntos en el espacio latente\n",
    "z1 = np.array([[-1.0], [-1.0]])\n",
    "z2 = np.array([[1.0], [1.0]])\n",
    "\n",
    "# Generar interpolacin lineal\n",
    "interpolations = [z1 + (z2 - z1) * t for t in np.linspace(0, 1, num=10)]\n",
    "\n",
    "# Reconstruir y mostrar las imgenes correspondientes\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 3))\n",
    "\n",
    "for i, z in enumerate(interpolations):\n",
    "    generated_image = vae.generate(z).reshape(28, 28)\n",
    "    axes[i].imshow(generated_image, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.title(\"Interpolacin en el espacio latente\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
